{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tY_jn6VVBTgk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dfb2cec785443b485faca75c72a9a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_348babbbc8214b128f0c0eae974ec5a3",
              "IPY_MODEL_5ddcc4a5d2f743ae8df30324b9369cec",
              "IPY_MODEL_dc9754b6fc994fd3bf2379bc09b47cae"
            ],
            "layout": "IPY_MODEL_17ef6673a59845af9c01f5df2eaec51f"
          }
        },
        "348babbbc8214b128f0c0eae974ec5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc4efb13eacf49ca8d89704c697c3dc5",
            "placeholder": "​",
            "style": "IPY_MODEL_1f019169e103403c91ffada0cff0c43c",
            "value": "100%"
          }
        },
        "5ddcc4a5d2f743ae8df30324b9369cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18938feb5804d908cb0d1a52f87f43e",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cee13b3fa9ac4dd7b77e5acc953a43a1",
            "value": 170498071
          }
        },
        "dc9754b6fc994fd3bf2379bc09b47cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e31ef4512504d169a54170dbbbdadfa",
            "placeholder": "​",
            "style": "IPY_MODEL_f798d67d1da84be9a8c705b0cf9768eb",
            "value": " 170498071/170498071 [00:01&lt;00:00, 113543295.44it/s]"
          }
        },
        "17ef6673a59845af9c01f5df2eaec51f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc4efb13eacf49ca8d89704c697c3dc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f019169e103403c91ffada0cff0c43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b18938feb5804d908cb0d1a52f87f43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee13b3fa9ac4dd7b77e5acc953a43a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e31ef4512504d169a54170dbbbdadfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f798d67d1da84be9a8c705b0cf9768eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexdembele/MI201/blob/main/ProjetMI201Demb%C3%A9l%C3%A9Xie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4zsDWUk5cavn"
      },
      "source": [
        "# Unsupervised Learning Tutorial of Gianni Franchi\n",
        "**PLEASE write your name and first name here:** Xie Antoine Dembélé Alex\n",
        "\n",
        "Welcome to ML project!\n",
        "**In this notebook, you will**:\n",
        "- Learn what is SSL\n",
        "- Learn the difficulty with Overfitting\n",
        "- Learn to implement an Convolutional Neural Network.\n",
        "- Learn to train it when we don't have enough data\n",
        "\n",
        "If you have never used jupyter notebooks, nor Colab notebooks, [here](https://colab.research.google.com/notebooks/welcome.ipynb) is a short intro.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvision\n",
        "\n",
        "Resnet 18\n",
        "\n",
        "tester les modèles en faisant varier les paramètres.\n",
        "\n",
        "Pas utilisé y unabled\n",
        "\n",
        "Renvoyer notebook et rapport réponse question\n",
        "\n",
        "Q5: Fixmatch , inspiré se UDA. On a un set labelisé et un set pas labélisé, pour les labelisé on fait du supervise learning. Sur les non labelisé on fait du self learning: data augmentation (faible puis forte,) puis prédiction, on veur que les différentes dprédictions sient pareil."
      ],
      "metadata": {
        "id": "XGB8_oFGVsZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from numpy import asarray\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import random\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "#import matplotlib.pyplot as plt\n",
        "#from modules import *\n",
        "#import torchvision.models as models_pytorch\n",
        "#import h5py\n",
        "#import torch.optim as optim\n",
        "#import augmentations\n",
        "from torch.nn.functional import kl_div, softmax, log_softmax\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from os.path import exists, join, split\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from PIL import Image, ImageFilter , ImageDraw\n",
        "import PIL\n",
        "import random\n",
        "#import madgrad \n",
        "import matplotlib.pyplot as plt\n",
        "#! pip install madgrad\n",
        "#! pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "RTJ7Zz6TAaJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_class = 10 #number of classes\n",
        "\n",
        "seed=111 #seed for the algorithm\n",
        "batch_size = 32\n",
        "num_train =100 # number of training image by classe\n",
        "cutout=16  # parameter for the cutout\n",
        "num_epochs=50\n",
        "#Validation set size\n",
        "valid_size = 200\n",
        "lr=0.1"
      ],
      "metadata": {
        "id": "kFCoFJzWApYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First let us define a CNN"
      ],
      "metadata": {
        "id": "2Ux1VVt2AtcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bn_momentum = 0.9\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "\n",
        "def conv_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
        "        init.constant_(m.bias, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class WideBasic(nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
        "        super(WideBasic, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=bn_momentum)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        assert ((depth - 4) % 6 == 0), 'Wide-resnet depth should be 6n+4'\n",
        "        n = int((depth - 4) / 6)\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.conv1 = conv3x3(3, nStages[0])\n",
        "        self.layer1 = self._wide_layer(WideBasic, nStages[1], n, dropout_rate, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasic, nStages[2], n, dropout_rate, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasic, nStages[3], n, dropout_rate, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=bn_momentum)\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "        # self.apply(conv_init)\n",
        "\n",
        "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
        "            self.in_planes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        # out = F.avg_pool2d(out, 8)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "YDbhFAHJAx97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now let us define a dataset\n"
      ],
      "metadata": {
        "id": "9Y7RH4rWA9c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_sub_CIFAR(data.Dataset):\n",
        "\n",
        "    def __init__(self, data_feature, data_target,transform,phase='label'):\n",
        "        self.data_feature = data_feature\n",
        "        self.data_target = data_target\n",
        "        self.transform = transform\n",
        "        self.phase=phase\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_feature)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
        "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
        "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
        "        if self.phase=='label':\n",
        "            data_feature = self.transform(Image.fromarray(np.uint8(self.data_feature[index])))\n",
        "            data_target =  self.data_target[index]\n",
        "            return data_feature, data_target\n",
        "\n",
        "        else:\n",
        "            data_feature = self.data_feature[index].float()\n",
        "            return data_feature\n",
        "\n",
        "\n",
        "class CutoutDefault(object):\n",
        "    \"\"\"\n",
        "    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py\n",
        "    \"\"\"\n",
        "    def __init__(self, length):\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if self.length <= 0:\n",
        "            return img\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "\n",
        "        y1 = np.clip(y - self.length // 2, 0, h)\n",
        "        y2 = np.clip(y + self.length // 2, 0, h)\n",
        "        x1 = np.clip(x - self.length // 2, 0, w)\n",
        "        x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "        mask[y1: y2, x1: x2] = 0.\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img *= mask\n",
        "        return img\n",
        "\n",
        "    \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4, padding_mode = 'reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "    CutoutDefault(cutout),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "#Dataset loading\n",
        "CIFAR10_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=None, download=True)\n",
        "CIFAR10_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=None, download=True)\n",
        "np.random.seed(seed=seed)\n",
        "permuation=np.random.permutation(len(CIFAR10_train_dataset.targets))\n",
        "\n",
        "Original_train_data_x = (CIFAR10_train_dataset.data)\n",
        "Original_train_data_y = np.array(CIFAR10_train_dataset.targets)\n",
        "Original_train_data_x = Original_train_data_x[permuation]\n",
        "Original_train_data_y = Original_train_data_y[permuation]\n",
        "\n",
        "Original_test_data_x = CIFAR10_test_dataset.data\n",
        "Original_test_data_y = np.array(CIFAR10_test_dataset.targets)\n",
        "\n",
        "\n",
        "\n",
        "#Selection of 250 labeled images for training and 2000 for validation\n",
        "incr_class = torch.zeros(num_class)\n",
        "train_idx_dico = {} #labeled images index dictionnary\n",
        "\n",
        "for i in range(num_class):\n",
        "    train_idx_dico[str(i)] = []\n",
        "\n",
        "valid_idx = np.zeros(num_class * valid_size, dtype=np.int32) #validation images indexes (2000)\n",
        "incr_t = 0\n",
        "incr_v = 0\n",
        "incrtotal = 0\n",
        "\n",
        "for idx in range(len(Original_train_data_y)):\n",
        "    class_y = Original_train_data_y[idx]\n",
        "    incrtotal += 1\n",
        "\n",
        "    train_idx_dico[str(class_y)].append(idx)\n",
        "    incr_class[class_y] += 1 #count the number of image per class\n",
        "    incr_t += 1\n",
        "\n",
        "\n",
        "train_idx = np.zeros(num_class * num_train, dtype=np.int32) #train labeled images indexes (1000)\n",
        "list_train_id = []\n",
        "list_unalabel_id = []\n",
        "valid_idx = []\n",
        "unlabel_idx_dico = {}\n",
        "for i in range(num_class):\n",
        "    unlabel_idx_dico[str(i)] = []\n",
        "for i in range(num_class):\n",
        "    list_train_id = list_train_id + train_idx_dico[str(i)][0:num_train]\n",
        "    valid_idx =valid_idx + train_idx_dico[str(i)][num_train:num_train+valid_size]\n",
        "    list_unalabel_id = list_unalabel_id + train_idx_dico[str(i)][num_train+valid_size::]\n",
        "    unlabel_idx_dico[str(i)] = train_idx_dico[str(i)][num_train::]\n",
        "\n",
        "#Get labeled and unlabeled data\n",
        "\n",
        "x_train = Original_train_data_x[[int(i) for i in list_train_id]]\n",
        "y_train = Original_train_data_y[[int(i) for i in list_train_id]]\n",
        "\n",
        "x_unlabeled = Original_train_data_x[[int(i) for i in list_unalabel_id]]\n",
        "y_unlabeled = Original_train_data_y[[int(i) for i in list_unalabel_id]]\n",
        "\n",
        "#Get validation set data\n",
        "x_valid = Original_train_data_x[[int(i) for i in valid_idx]]\n",
        "y_valid = Original_train_data_y[[int(i) for i in valid_idx]]\n",
        "\n",
        "# Printing the size of the training, validation and test sets\n",
        "print('Number of training examples: ' + str(x_train.shape[0]))\n",
        "print('Number of unlabeled examples: ' + str(x_unlabeled.shape[0]))\n",
        "print('Number of validation examples: ' + str(x_valid.shape[0]))\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#Dataloader creation\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(Original_test_data_x, Original_test_data_y, transform=transform_test),\n",
        "    batch_size = batch_size,\n",
        "    shuffle=False, num_workers=2)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_train, y_train, transform=transform_train),\n",
        "    batch_size=batch_size,shuffle=True, num_workers=2) #num_workers = 2 ou 1\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_valid, y_valid, transform=transform_test),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "0dfb2cec785443b485faca75c72a9a62",
            "348babbbc8214b128f0c0eae974ec5a3",
            "5ddcc4a5d2f743ae8df30324b9369cec",
            "dc9754b6fc994fd3bf2379bc09b47cae",
            "17ef6673a59845af9c01f5df2eaec51f",
            "cc4efb13eacf49ca8d89704c697c3dc5",
            "1f019169e103403c91ffada0cff0c43c",
            "b18938feb5804d908cb0d1a52f87f43e",
            "cee13b3fa9ac4dd7b77e5acc953a43a1",
            "3e31ef4512504d169a54170dbbbdadfa",
            "f798d67d1da84be9a8c705b0cf9768eb"
          ]
        },
        "id": "JE1_MblIA5V7",
        "outputId": "ec2169db-3cb2-4277-b405-e8744d9adac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dfb2cec785443b485faca75c72a9a62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Number of training examples: 1000\n",
            "Number of unlabeled examples: 47000\n",
            "Number of validation examples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we build the CNN and the optimizer"
      ],
      "metadata": {
        "id": "jxIA7jzZBIjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#Networks creation\n",
        "net = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class)\n",
        "net =net.to(device)\n",
        "net_save = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class) # model where to save the results\n",
        "net_save =net_save.to(device)\n",
        "\n",
        "def learning_rate_scheduler(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 200):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 160):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 80):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.1, optim_factor)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch,net,trainloader,log_interval=15):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate_scheduler(lr, epoch)))\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_scheduler(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        " \n",
        "        inputs, targets = inputs.to(device), targets.to(device) # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)               # Forward Propagation\n",
        "        loss = criterion(outputs, targets)  # Loss\n",
        "        loss.backward()  # Backward Propagation\n",
        "        optimizer.step() # Optimizer update\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (len(trainloader.dataset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "\n",
        "\n",
        "def test(epoch,net,testloader):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        \n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "5HSobiPQBNjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "tY_jn6VVBTgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    train(epoch,net,train_loader)\n",
        "    acc =test(epoch,net,valid_loader)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        net_save.load_state_dict(net.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpnbckfWBWbB",
        "outputId": "8697a89c-72bd-4784-e702-f105c790df85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.1000\n",
            "| Epoch [  0/ 50] Iter[  1/ 32]\t\tLoss: 2.2377 Acc@1: 9.375%\n",
            "| Epoch [  0/ 50] Iter[ 16/ 32]\t\tLoss: 2.4813 Acc@1: 16.992%\n",
            "| Epoch [  0/ 50] Iter[ 31/ 32]\t\tLoss: 2.2276 Acc@1: 18.044%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 1.8544 Acc@1: 19.85%\n",
            "| Saving Best model...\t\t\tTop1 = 19.85%\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/ 50] Iter[  1/ 32]\t\tLoss: 2.4222 Acc@1: 12.500%\n",
            "| Epoch [  1/ 50] Iter[ 16/ 32]\t\tLoss: 2.2226 Acc@1: 16.016%\n",
            "| Epoch [  1/ 50] Iter[ 31/ 32]\t\tLoss: 2.0750 Acc@1: 17.742%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 2.4628 Acc@1: 17.15%\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/ 50] Iter[  1/ 32]\t\tLoss: 2.1419 Acc@1: 21.875%\n",
            "| Epoch [  2/ 50] Iter[ 16/ 32]\t\tLoss: 2.1594 Acc@1: 23.438%\n",
            "| Epoch [  2/ 50] Iter[ 31/ 32]\t\tLoss: 2.1719 Acc@1: 24.093%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 1.4263 Acc@1: 20.80%\n",
            "| Saving Best model...\t\t\tTop1 = 20.80%\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/ 50] Iter[  1/ 32]\t\tLoss: 2.1949 Acc@1: 28.125%\n",
            "| Epoch [  3/ 50] Iter[ 16/ 32]\t\tLoss: 2.0545 Acc@1: 25.000%\n",
            "| Epoch [  3/ 50] Iter[ 31/ 32]\t\tLoss: 1.8936 Acc@1: 24.093%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 2.6291 Acc@1: 26.35%\n",
            "| Saving Best model...\t\t\tTop1 = 26.35%\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/ 50] Iter[  1/ 32]\t\tLoss: 1.7825 Acc@1: 34.375%\n",
            "| Epoch [  4/ 50] Iter[ 16/ 32]\t\tLoss: 1.8468 Acc@1: 27.734%\n",
            "| Epoch [  4/ 50] Iter[ 31/ 32]\t\tLoss: 2.0802 Acc@1: 27.722%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 1.6349 Acc@1: 27.15%\n",
            "| Saving Best model...\t\t\tTop1 = 27.15%\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/ 50] Iter[  1/ 32]\t\tLoss: 2.0801 Acc@1: 25.000%\n",
            "| Epoch [  5/ 50] Iter[ 16/ 32]\t\tLoss: 1.8107 Acc@1: 26.172%\n",
            "| Epoch [  5/ 50] Iter[ 31/ 32]\t\tLoss: 1.9668 Acc@1: 27.419%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 1.6536 Acc@1: 29.70%\n",
            "| Saving Best model...\t\t\tTop1 = 29.70%\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/ 50] Iter[  1/ 32]\t\tLoss: 1.8983 Acc@1: 21.875%\n",
            "| Epoch [  6/ 50] Iter[ 16/ 32]\t\tLoss: 1.7159 Acc@1: 30.859%\n",
            "| Epoch [  6/ 50] Iter[ 31/ 32]\t\tLoss: 1.7489 Acc@1: 30.746%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 2.7364 Acc@1: 30.55%\n",
            "| Saving Best model...\t\t\tTop1 = 30.55%\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/ 50] Iter[  1/ 32]\t\tLoss: 1.8152 Acc@1: 31.250%\n",
            "| Epoch [  7/ 50] Iter[ 16/ 32]\t\tLoss: 1.9313 Acc@1: 29.883%\n",
            "| Epoch [  7/ 50] Iter[ 31/ 32]\t\tLoss: 1.9307 Acc@1: 28.931%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 1.6369 Acc@1: 25.20%\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/ 50] Iter[  1/ 32]\t\tLoss: 1.8991 Acc@1: 15.625%\n",
            "| Epoch [  8/ 50] Iter[ 16/ 32]\t\tLoss: 1.7567 Acc@1: 30.078%\n",
            "| Epoch [  8/ 50] Iter[ 31/ 32]\t\tLoss: 1.8390 Acc@1: 29.234%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 0.7384 Acc@1: 23.70%\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/ 50] Iter[  1/ 32]\t\tLoss: 1.9057 Acc@1: 25.000%\n",
            "| Epoch [  9/ 50] Iter[ 16/ 32]\t\tLoss: 1.9383 Acc@1: 29.492%\n",
            "| Epoch [  9/ 50] Iter[ 31/ 32]\t\tLoss: 2.0170 Acc@1: 32.056%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 2.1490 Acc@1: 32.45%\n",
            "| Saving Best model...\t\t\tTop1 = 32.45%\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/ 50] Iter[  1/ 32]\t\tLoss: 1.9667 Acc@1: 34.375%\n",
            "| Epoch [ 10/ 50] Iter[ 16/ 32]\t\tLoss: 1.9149 Acc@1: 35.547%\n",
            "| Epoch [ 10/ 50] Iter[ 31/ 32]\t\tLoss: 1.9906 Acc@1: 31.956%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 1.8629 Acc@1: 31.80%\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/ 50] Iter[  1/ 32]\t\tLoss: 1.7227 Acc@1: 40.625%\n",
            "| Epoch [ 11/ 50] Iter[ 16/ 32]\t\tLoss: 1.9365 Acc@1: 33.594%\n",
            "| Epoch [ 11/ 50] Iter[ 31/ 32]\t\tLoss: 1.8648 Acc@1: 32.157%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 1.4691 Acc@1: 32.60%\n",
            "| Saving Best model...\t\t\tTop1 = 32.60%\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/ 50] Iter[  1/ 32]\t\tLoss: 1.4854 Acc@1: 46.875%\n",
            "| Epoch [ 12/ 50] Iter[ 16/ 32]\t\tLoss: 1.8090 Acc@1: 32.617%\n",
            "| Epoch [ 12/ 50] Iter[ 31/ 32]\t\tLoss: 1.8462 Acc@1: 33.065%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 2.3980 Acc@1: 31.75%\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/ 50] Iter[  1/ 32]\t\tLoss: 1.8094 Acc@1: 37.500%\n",
            "| Epoch [ 13/ 50] Iter[ 16/ 32]\t\tLoss: 1.9735 Acc@1: 37.500%\n",
            "| Epoch [ 13/ 50] Iter[ 31/ 32]\t\tLoss: 1.7435 Acc@1: 34.375%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 1.8092 Acc@1: 26.50%\n",
            "\n",
            "=> Training Epoch #14, LR=0.1000\n",
            "| Epoch [ 14/ 50] Iter[  1/ 32]\t\tLoss: 1.7276 Acc@1: 34.375%\n",
            "| Epoch [ 14/ 50] Iter[ 16/ 32]\t\tLoss: 1.9534 Acc@1: 35.156%\n",
            "| Epoch [ 14/ 50] Iter[ 31/ 32]\t\tLoss: 1.8602 Acc@1: 34.476%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 1.3997 Acc@1: 36.75%\n",
            "| Saving Best model...\t\t\tTop1 = 36.75%\n",
            "\n",
            "=> Training Epoch #15, LR=0.1000\n",
            "| Epoch [ 15/ 50] Iter[  1/ 32]\t\tLoss: 1.7554 Acc@1: 40.625%\n",
            "| Epoch [ 15/ 50] Iter[ 16/ 32]\t\tLoss: 1.8279 Acc@1: 35.742%\n",
            "| Epoch [ 15/ 50] Iter[ 31/ 32]\t\tLoss: 1.7130 Acc@1: 34.778%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 1.6482 Acc@1: 32.65%\n",
            "\n",
            "=> Training Epoch #16, LR=0.1000\n",
            "| Epoch [ 16/ 50] Iter[  1/ 32]\t\tLoss: 1.4617 Acc@1: 43.750%\n",
            "| Epoch [ 16/ 50] Iter[ 16/ 32]\t\tLoss: 1.6770 Acc@1: 38.867%\n",
            "| Epoch [ 16/ 50] Iter[ 31/ 32]\t\tLoss: 1.4396 Acc@1: 37.399%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 1.6751 Acc@1: 32.65%\n",
            "\n",
            "=> Training Epoch #17, LR=0.1000\n",
            "| Epoch [ 17/ 50] Iter[  1/ 32]\t\tLoss: 1.7298 Acc@1: 31.250%\n",
            "| Epoch [ 17/ 50] Iter[ 16/ 32]\t\tLoss: 1.6979 Acc@1: 40.430%\n",
            "| Epoch [ 17/ 50] Iter[ 31/ 32]\t\tLoss: 1.6438 Acc@1: 38.105%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 0.6771 Acc@1: 34.35%\n",
            "\n",
            "=> Training Epoch #18, LR=0.1000\n",
            "| Epoch [ 18/ 50] Iter[  1/ 32]\t\tLoss: 1.8599 Acc@1: 31.250%\n",
            "| Epoch [ 18/ 50] Iter[ 16/ 32]\t\tLoss: 1.5413 Acc@1: 39.062%\n",
            "| Epoch [ 18/ 50] Iter[ 31/ 32]\t\tLoss: 1.6737 Acc@1: 38.911%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 0.9127 Acc@1: 35.75%\n",
            "\n",
            "=> Training Epoch #19, LR=0.1000\n",
            "| Epoch [ 19/ 50] Iter[  1/ 32]\t\tLoss: 1.6066 Acc@1: 40.625%\n",
            "| Epoch [ 19/ 50] Iter[ 16/ 32]\t\tLoss: 1.4275 Acc@1: 37.109%\n",
            "| Epoch [ 19/ 50] Iter[ 31/ 32]\t\tLoss: 1.5756 Acc@1: 37.198%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 1.4603 Acc@1: 36.60%\n",
            "\n",
            "=> Training Epoch #20, LR=0.1000\n",
            "| Epoch [ 20/ 50] Iter[  1/ 32]\t\tLoss: 1.5670 Acc@1: 37.500%\n",
            "| Epoch [ 20/ 50] Iter[ 16/ 32]\t\tLoss: 1.6312 Acc@1: 35.352%\n",
            "| Epoch [ 20/ 50] Iter[ 31/ 32]\t\tLoss: 1.5984 Acc@1: 37.097%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 1.8332 Acc@1: 36.70%\n",
            "\n",
            "=> Training Epoch #21, LR=0.1000\n",
            "| Epoch [ 21/ 50] Iter[  1/ 32]\t\tLoss: 1.5101 Acc@1: 46.875%\n",
            "| Epoch [ 21/ 50] Iter[ 16/ 32]\t\tLoss: 1.7849 Acc@1: 39.453%\n",
            "| Epoch [ 21/ 50] Iter[ 31/ 32]\t\tLoss: 1.6427 Acc@1: 38.004%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 0.9704 Acc@1: 33.55%\n",
            "\n",
            "=> Training Epoch #22, LR=0.1000\n",
            "| Epoch [ 22/ 50] Iter[  1/ 32]\t\tLoss: 1.4925 Acc@1: 46.875%\n",
            "| Epoch [ 22/ 50] Iter[ 16/ 32]\t\tLoss: 1.5673 Acc@1: 39.453%\n",
            "| Epoch [ 22/ 50] Iter[ 31/ 32]\t\tLoss: 1.6455 Acc@1: 40.423%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 1.6516 Acc@1: 32.45%\n",
            "\n",
            "=> Training Epoch #23, LR=0.1000\n",
            "| Epoch [ 23/ 50] Iter[  1/ 32]\t\tLoss: 1.4333 Acc@1: 28.125%\n",
            "| Epoch [ 23/ 50] Iter[ 16/ 32]\t\tLoss: 1.7325 Acc@1: 41.016%\n",
            "| Epoch [ 23/ 50] Iter[ 31/ 32]\t\tLoss: 2.0206 Acc@1: 40.625%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 1.9770 Acc@1: 30.55%\n",
            "\n",
            "=> Training Epoch #24, LR=0.1000\n",
            "| Epoch [ 24/ 50] Iter[  1/ 32]\t\tLoss: 1.9216 Acc@1: 31.250%\n",
            "| Epoch [ 24/ 50] Iter[ 16/ 32]\t\tLoss: 1.5636 Acc@1: 38.281%\n",
            "| Epoch [ 24/ 50] Iter[ 31/ 32]\t\tLoss: 1.7948 Acc@1: 38.407%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 0.7981 Acc@1: 38.90%\n",
            "| Saving Best model...\t\t\tTop1 = 38.90%\n",
            "\n",
            "=> Training Epoch #25, LR=0.1000\n",
            "| Epoch [ 25/ 50] Iter[  1/ 32]\t\tLoss: 1.6402 Acc@1: 43.750%\n",
            "| Epoch [ 25/ 50] Iter[ 16/ 32]\t\tLoss: 1.5217 Acc@1: 39.453%\n",
            "| Epoch [ 25/ 50] Iter[ 31/ 32]\t\tLoss: 1.5056 Acc@1: 40.827%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 0.4168 Acc@1: 34.30%\n",
            "\n",
            "=> Training Epoch #26, LR=0.1000\n",
            "| Epoch [ 26/ 50] Iter[  1/ 32]\t\tLoss: 1.5569 Acc@1: 46.875%\n",
            "| Epoch [ 26/ 50] Iter[ 16/ 32]\t\tLoss: 1.8657 Acc@1: 41.602%\n",
            "| Epoch [ 26/ 50] Iter[ 31/ 32]\t\tLoss: 1.9605 Acc@1: 41.532%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 1.2814 Acc@1: 41.75%\n",
            "| Saving Best model...\t\t\tTop1 = 41.75%\n",
            "\n",
            "=> Training Epoch #27, LR=0.1000\n",
            "| Epoch [ 27/ 50] Iter[  1/ 32]\t\tLoss: 1.5885 Acc@1: 43.750%\n",
            "| Epoch [ 27/ 50] Iter[ 16/ 32]\t\tLoss: 1.3012 Acc@1: 48.242%\n",
            "| Epoch [ 27/ 50] Iter[ 31/ 32]\t\tLoss: 1.7275 Acc@1: 43.347%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 1.0518 Acc@1: 40.45%\n",
            "\n",
            "=> Training Epoch #28, LR=0.1000\n",
            "| Epoch [ 28/ 50] Iter[  1/ 32]\t\tLoss: 1.6523 Acc@1: 37.500%\n",
            "| Epoch [ 28/ 50] Iter[ 16/ 32]\t\tLoss: 1.8153 Acc@1: 44.336%\n",
            "| Epoch [ 28/ 50] Iter[ 31/ 32]\t\tLoss: 1.9938 Acc@1: 42.036%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 0.8765 Acc@1: 36.25%\n",
            "\n",
            "=> Training Epoch #29, LR=0.1000\n",
            "| Epoch [ 29/ 50] Iter[  1/ 32]\t\tLoss: 1.6574 Acc@1: 31.250%\n",
            "| Epoch [ 29/ 50] Iter[ 16/ 32]\t\tLoss: 1.6599 Acc@1: 42.969%\n",
            "| Epoch [ 29/ 50] Iter[ 31/ 32]\t\tLoss: 1.3112 Acc@1: 44.758%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 2.4835 Acc@1: 39.90%\n",
            "\n",
            "=> Training Epoch #30, LR=0.1000\n",
            "| Epoch [ 30/ 50] Iter[  1/ 32]\t\tLoss: 1.5141 Acc@1: 46.875%\n",
            "| Epoch [ 30/ 50] Iter[ 16/ 32]\t\tLoss: 1.3859 Acc@1: 43.750%\n",
            "| Epoch [ 30/ 50] Iter[ 31/ 32]\t\tLoss: 1.5047 Acc@1: 41.835%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 1.1678 Acc@1: 35.15%\n",
            "\n",
            "=> Training Epoch #31, LR=0.1000\n",
            "| Epoch [ 31/ 50] Iter[  1/ 32]\t\tLoss: 1.5019 Acc@1: 40.625%\n",
            "| Epoch [ 31/ 50] Iter[ 16/ 32]\t\tLoss: 1.6637 Acc@1: 45.312%\n",
            "| Epoch [ 31/ 50] Iter[ 31/ 32]\t\tLoss: 1.6257 Acc@1: 43.347%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 1.7382 Acc@1: 35.85%\n",
            "\n",
            "=> Training Epoch #32, LR=0.1000\n",
            "| Epoch [ 32/ 50] Iter[  1/ 32]\t\tLoss: 1.5140 Acc@1: 46.875%\n",
            "| Epoch [ 32/ 50] Iter[ 16/ 32]\t\tLoss: 1.4892 Acc@1: 48.047%\n",
            "| Epoch [ 32/ 50] Iter[ 31/ 32]\t\tLoss: 1.7161 Acc@1: 47.278%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 1.5648 Acc@1: 38.35%\n",
            "\n",
            "=> Training Epoch #33, LR=0.1000\n",
            "| Epoch [ 33/ 50] Iter[  1/ 32]\t\tLoss: 1.6299 Acc@1: 40.625%\n",
            "| Epoch [ 33/ 50] Iter[ 16/ 32]\t\tLoss: 1.4542 Acc@1: 46.484%\n",
            "| Epoch [ 33/ 50] Iter[ 31/ 32]\t\tLoss: 1.6110 Acc@1: 46.371%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 1.3155 Acc@1: 43.05%\n",
            "| Saving Best model...\t\t\tTop1 = 43.05%\n",
            "\n",
            "=> Training Epoch #34, LR=0.1000\n",
            "| Epoch [ 34/ 50] Iter[  1/ 32]\t\tLoss: 1.9051 Acc@1: 25.000%\n",
            "| Epoch [ 34/ 50] Iter[ 16/ 32]\t\tLoss: 1.1976 Acc@1: 46.289%\n",
            "| Epoch [ 34/ 50] Iter[ 31/ 32]\t\tLoss: 1.6467 Acc@1: 42.238%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 1.6952 Acc@1: 39.70%\n",
            "\n",
            "=> Training Epoch #35, LR=0.1000\n",
            "| Epoch [ 35/ 50] Iter[  1/ 32]\t\tLoss: 1.8533 Acc@1: 31.250%\n",
            "| Epoch [ 35/ 50] Iter[ 16/ 32]\t\tLoss: 1.5217 Acc@1: 45.898%\n",
            "| Epoch [ 35/ 50] Iter[ 31/ 32]\t\tLoss: 1.6499 Acc@1: 46.774%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 0.8540 Acc@1: 44.15%\n",
            "| Saving Best model...\t\t\tTop1 = 44.15%\n",
            "\n",
            "=> Training Epoch #36, LR=0.1000\n",
            "| Epoch [ 36/ 50] Iter[  1/ 32]\t\tLoss: 1.4804 Acc@1: 43.750%\n",
            "| Epoch [ 36/ 50] Iter[ 16/ 32]\t\tLoss: 1.8141 Acc@1: 47.656%\n",
            "| Epoch [ 36/ 50] Iter[ 31/ 32]\t\tLoss: 1.5016 Acc@1: 46.573%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.8461 Acc@1: 42.35%\n",
            "\n",
            "=> Training Epoch #37, LR=0.1000\n",
            "| Epoch [ 37/ 50] Iter[  1/ 32]\t\tLoss: 1.5300 Acc@1: 46.875%\n",
            "| Epoch [ 37/ 50] Iter[ 16/ 32]\t\tLoss: 1.6686 Acc@1: 48.242%\n",
            "| Epoch [ 37/ 50] Iter[ 31/ 32]\t\tLoss: 1.5379 Acc@1: 46.976%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 0.8515 Acc@1: 43.85%\n",
            "\n",
            "=> Training Epoch #38, LR=0.1000\n",
            "| Epoch [ 38/ 50] Iter[  1/ 32]\t\tLoss: 1.3741 Acc@1: 46.875%\n",
            "| Epoch [ 38/ 50] Iter[ 16/ 32]\t\tLoss: 1.3485 Acc@1: 47.266%\n",
            "| Epoch [ 38/ 50] Iter[ 31/ 32]\t\tLoss: 1.1428 Acc@1: 49.294%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 0.9813 Acc@1: 43.45%\n",
            "\n",
            "=> Training Epoch #39, LR=0.1000\n",
            "| Epoch [ 39/ 50] Iter[  1/ 32]\t\tLoss: 1.2209 Acc@1: 53.125%\n",
            "| Epoch [ 39/ 50] Iter[ 16/ 32]\t\tLoss: 1.2766 Acc@1: 48.633%\n",
            "| Epoch [ 39/ 50] Iter[ 31/ 32]\t\tLoss: 1.6735 Acc@1: 46.472%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 1.0573 Acc@1: 37.20%\n",
            "\n",
            "=> Training Epoch #40, LR=0.1000\n",
            "| Epoch [ 40/ 50] Iter[  1/ 32]\t\tLoss: 1.3664 Acc@1: 40.625%\n",
            "| Epoch [ 40/ 50] Iter[ 16/ 32]\t\tLoss: 1.1150 Acc@1: 46.484%\n",
            "| Epoch [ 40/ 50] Iter[ 31/ 32]\t\tLoss: 1.8674 Acc@1: 46.573%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 1.0612 Acc@1: 41.25%\n",
            "\n",
            "=> Training Epoch #41, LR=0.1000\n",
            "| Epoch [ 41/ 50] Iter[  1/ 32]\t\tLoss: 1.4877 Acc@1: 37.500%\n",
            "| Epoch [ 41/ 50] Iter[ 16/ 32]\t\tLoss: 1.8296 Acc@1: 49.219%\n",
            "| Epoch [ 41/ 50] Iter[ 31/ 32]\t\tLoss: 1.4298 Acc@1: 47.278%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 1.0890 Acc@1: 40.15%\n",
            "\n",
            "=> Training Epoch #42, LR=0.1000\n",
            "| Epoch [ 42/ 50] Iter[  1/ 32]\t\tLoss: 1.2124 Acc@1: 65.625%\n",
            "| Epoch [ 42/ 50] Iter[ 16/ 32]\t\tLoss: 1.1913 Acc@1: 50.781%\n",
            "| Epoch [ 42/ 50] Iter[ 31/ 32]\t\tLoss: 1.6574 Acc@1: 47.681%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 1.2456 Acc@1: 45.65%\n",
            "| Saving Best model...\t\t\tTop1 = 45.65%\n",
            "\n",
            "=> Training Epoch #43, LR=0.1000\n",
            "| Epoch [ 43/ 50] Iter[  1/ 32]\t\tLoss: 1.3242 Acc@1: 43.750%\n",
            "| Epoch [ 43/ 50] Iter[ 16/ 32]\t\tLoss: 1.3470 Acc@1: 48.633%\n",
            "| Epoch [ 43/ 50] Iter[ 31/ 32]\t\tLoss: 1.2768 Acc@1: 49.597%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 0.9217 Acc@1: 44.80%\n",
            "\n",
            "=> Training Epoch #44, LR=0.1000\n",
            "| Epoch [ 44/ 50] Iter[  1/ 32]\t\tLoss: 1.6054 Acc@1: 34.375%\n",
            "| Epoch [ 44/ 50] Iter[ 16/ 32]\t\tLoss: 1.5642 Acc@1: 51.367%\n",
            "| Epoch [ 44/ 50] Iter[ 31/ 32]\t\tLoss: 1.5595 Acc@1: 49.597%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 2.0354 Acc@1: 40.00%\n",
            "\n",
            "=> Training Epoch #45, LR=0.1000\n",
            "| Epoch [ 45/ 50] Iter[  1/ 32]\t\tLoss: 1.4729 Acc@1: 50.000%\n",
            "| Epoch [ 45/ 50] Iter[ 16/ 32]\t\tLoss: 1.6864 Acc@1: 50.391%\n",
            "| Epoch [ 45/ 50] Iter[ 31/ 32]\t\tLoss: 1.2677 Acc@1: 50.403%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 1.3883 Acc@1: 46.55%\n",
            "| Saving Best model...\t\t\tTop1 = 46.55%\n",
            "\n",
            "=> Training Epoch #46, LR=0.1000\n",
            "| Epoch [ 46/ 50] Iter[  1/ 32]\t\tLoss: 1.3759 Acc@1: 56.250%\n",
            "| Epoch [ 46/ 50] Iter[ 16/ 32]\t\tLoss: 1.4458 Acc@1: 53.906%\n",
            "| Epoch [ 46/ 50] Iter[ 31/ 32]\t\tLoss: 1.5225 Acc@1: 49.798%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 0.6004 Acc@1: 42.25%\n",
            "\n",
            "=> Training Epoch #47, LR=0.1000\n",
            "| Epoch [ 47/ 50] Iter[  1/ 32]\t\tLoss: 1.7688 Acc@1: 43.750%\n",
            "| Epoch [ 47/ 50] Iter[ 16/ 32]\t\tLoss: 1.2460 Acc@1: 49.023%\n",
            "| Epoch [ 47/ 50] Iter[ 31/ 32]\t\tLoss: 1.5295 Acc@1: 49.798%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 1.3224 Acc@1: 41.05%\n",
            "\n",
            "=> Training Epoch #48, LR=0.1000\n",
            "| Epoch [ 48/ 50] Iter[  1/ 32]\t\tLoss: 1.2629 Acc@1: 53.125%\n",
            "| Epoch [ 48/ 50] Iter[ 16/ 32]\t\tLoss: 1.5381 Acc@1: 53.906%\n",
            "| Epoch [ 48/ 50] Iter[ 31/ 32]\t\tLoss: 1.1807 Acc@1: 52.419%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 1.9163 Acc@1: 48.30%\n",
            "| Saving Best model...\t\t\tTop1 = 48.30%\n",
            "\n",
            "=> Training Epoch #49, LR=0.1000\n",
            "| Epoch [ 49/ 50] Iter[  1/ 32]\t\tLoss: 1.2773 Acc@1: 56.250%\n",
            "| Epoch [ 49/ 50] Iter[ 16/ 32]\t\tLoss: 1.6516 Acc@1: 48.828%\n",
            "| Epoch [ 49/ 50] Iter[ 31/ 32]\t\tLoss: 1.2160 Acc@1: 49.597%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 1.3179 Acc@1: 45.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "weX9mgbZBiNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def test_final(net,testloader):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "            if batch_idx == 0:\n",
        "                predicted_concat = predicted.clone()\n",
        "            else:\n",
        "                predicted_concat = torch.cat((predicted_concat, predicted), 0)\n",
        "\n",
        "        # Save checkpoint when best model\n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| TEST \\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %( loss.item(), acc))\n",
        "    return predicted_concat.cpu().numpy()\n",
        "    \n",
        "\n",
        "predicted_concat = test_final(net,test_loader)\n",
        "\n",
        "\n",
        "id_concat =range(len(predicted_concat))\n",
        "my_submission = pd.DataFrame({'Id': id_concat,'Expected': predicted_concat})\n",
        "\n",
        "# you could use any filename. We choose submission here\n",
        "my_submission.to_csv('submission2.csv', index=False)\n",
        "print('we have saved the submission !! ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDqFxbtmBkAL",
        "outputId": "f31dcd98-4304-44ee-d874-4275977cbb12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST \t\t\tLoss: 1.7705 Acc@1: 43.97%\n",
            "we have saved the submission !! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question for the report\n",
        "I want that you send me a small report with the answer to this question and your notebook.\n",
        "- Q0: Please train wideresnet, and please understand a bit wideresnet.\n",
        "- Q1: Please change DNN with a Resnet 18. Try with one that is pre-trained and one that is not pre-trained. \n",
        "- Q2: Please change DNN with an AlexNet. Try with one that is pre-trained and one that is not pre-trained. (Be careful, you need a bit to play with the learning rate, for questions two, one and zeros I want to see the training loss and training accuracy. What other curb is interesting? Plot it and analyse it.)\n",
        "- Q3: Please try to train an SVM and a random forest.\n",
        "- Q4 After you have trained several models please draw a table and make some conclusions.\n",
        "-  Q5 Read the paper fixmatch (https://amitness.com/2020/03/fixmatch-semi-supervised/) and explain it.\n",
        "- Q6 please try to implement it and try to make it work.\n",
        "- Q7 What can we do to avoid overfitting in Deep learning?\n",
        "\n",
        "\n",
        "*Q0-Q5 = 14 pts*\n",
        "\n",
        "*Q6 = 6 pts*\n",
        "\n",
        "*Q7 = 1 pts*"
      ],
      "metadata": {
        "id": "UrJS4AqABprt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rapport TP "
      ],
      "metadata": {
        "id": "p7KXcYVyaJz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q0 : Wideresnet\n"
      ],
      "metadata": {
        "id": "wBz_TiQJaMwn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivp1144RNvKo"
      },
      "source": [
        "## Q1\n",
        "\n",
        "A faire : \n",
        "- Changer le lr pour avoir de meilleurs résultats sur le pretrained\n",
        "\n",
        "- Changer les images en entrées sur le pretrained pour avoir de meilleurs résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax7iEqbVOHp5"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "\n",
        "net18_untrained = models.resnet18(weights = None)\n",
        "net18_untrained = net18_untrained.to(device)\n",
        "net18_untrained_save = models.resnet18(weights = None) # To save the results\n",
        "net18_untrained_save = net18_untrained_save.to(device)\n",
        "\n",
        "net18_pretrained = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "net18_pretrained = net18_pretrained.to(device)\n",
        "net18_pretrained_save = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) # To save the results\n",
        "net18_pretrained_save = net18_pretrained_save.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH1Wtjs8PTfr",
        "outputId": "d9f224aa-3b20-49e7-ae6f-71bd1804cdbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.1000\n",
            "| Epoch [  0/ 50] Iter[  1/ 32]\t\tLoss: 7.2352 Acc@1: 0.000%\n",
            "| Epoch [  0/ 50] Iter[ 16/ 32]\t\tLoss: 6.6311 Acc@1: 8.203%\n",
            "| Epoch [  0/ 50] Iter[ 31/ 32]\t\tLoss: 6.0291 Acc@1: 9.778%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 12.5333 Acc@1: 6.60%\n",
            "| Saving Best model...\t\t\tTop1 = 6.60%\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/ 50] Iter[  1/ 32]\t\tLoss: 8.0164 Acc@1: 12.500%\n",
            "| Epoch [  1/ 50] Iter[ 16/ 32]\t\tLoss: 3.6267 Acc@1: 14.258%\n",
            "| Epoch [  1/ 50] Iter[ 31/ 32]\t\tLoss: 4.0185 Acc@1: 13.810%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 4.9795 Acc@1: 13.20%\n",
            "| Saving Best model...\t\t\tTop1 = 13.20%\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/ 50] Iter[  1/ 32]\t\tLoss: 3.5349 Acc@1: 21.875%\n",
            "| Epoch [  2/ 50] Iter[ 16/ 32]\t\tLoss: 2.8627 Acc@1: 15.430%\n",
            "| Epoch [  2/ 50] Iter[ 31/ 32]\t\tLoss: 2.3141 Acc@1: 14.617%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 3.4608 Acc@1: 16.40%\n",
            "| Saving Best model...\t\t\tTop1 = 16.40%\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/ 50] Iter[  1/ 32]\t\tLoss: 2.3736 Acc@1: 12.500%\n",
            "| Epoch [  3/ 50] Iter[ 16/ 32]\t\tLoss: 2.6422 Acc@1: 16.211%\n",
            "| Epoch [  3/ 50] Iter[ 31/ 32]\t\tLoss: 3.0567 Acc@1: 15.423%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 1.8858 Acc@1: 15.80%\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/ 50] Iter[  1/ 32]\t\tLoss: 2.6221 Acc@1: 9.375%\n",
            "| Epoch [  4/ 50] Iter[ 16/ 32]\t\tLoss: 2.6643 Acc@1: 15.820%\n",
            "| Epoch [  4/ 50] Iter[ 31/ 32]\t\tLoss: 2.4804 Acc@1: 16.431%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 2.2567 Acc@1: 18.60%\n",
            "| Saving Best model...\t\t\tTop1 = 18.60%\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/ 50] Iter[  1/ 32]\t\tLoss: 2.6160 Acc@1: 18.750%\n",
            "| Epoch [  5/ 50] Iter[ 16/ 32]\t\tLoss: 2.4239 Acc@1: 16.016%\n",
            "| Epoch [  5/ 50] Iter[ 31/ 32]\t\tLoss: 2.3604 Acc@1: 16.734%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 4.6189 Acc@1: 15.55%\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/ 50] Iter[  1/ 32]\t\tLoss: 2.5061 Acc@1: 21.875%\n",
            "| Epoch [  6/ 50] Iter[ 16/ 32]\t\tLoss: 2.1255 Acc@1: 15.039%\n",
            "| Epoch [  6/ 50] Iter[ 31/ 32]\t\tLoss: 2.2549 Acc@1: 14.113%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 2.2473 Acc@1: 19.95%\n",
            "| Saving Best model...\t\t\tTop1 = 19.95%\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/ 50] Iter[  1/ 32]\t\tLoss: 2.4699 Acc@1: 15.625%\n",
            "| Epoch [  7/ 50] Iter[ 16/ 32]\t\tLoss: 2.3417 Acc@1: 18.945%\n",
            "| Epoch [  7/ 50] Iter[ 31/ 32]\t\tLoss: 2.5134 Acc@1: 16.835%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 3.4029 Acc@1: 17.25%\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/ 50] Iter[  1/ 32]\t\tLoss: 2.1172 Acc@1: 34.375%\n",
            "| Epoch [  8/ 50] Iter[ 16/ 32]\t\tLoss: 1.9573 Acc@1: 19.531%\n",
            "| Epoch [  8/ 50] Iter[ 31/ 32]\t\tLoss: 2.2452 Acc@1: 19.758%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 3.2197 Acc@1: 16.15%\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/ 50] Iter[  1/ 32]\t\tLoss: 2.2177 Acc@1: 15.625%\n",
            "| Epoch [  9/ 50] Iter[ 16/ 32]\t\tLoss: 2.1224 Acc@1: 17.969%\n",
            "| Epoch [  9/ 50] Iter[ 31/ 32]\t\tLoss: 2.2827 Acc@1: 18.044%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 2.1971 Acc@1: 15.95%\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/ 50] Iter[  1/ 32]\t\tLoss: 2.2763 Acc@1: 12.500%\n",
            "| Epoch [ 10/ 50] Iter[ 16/ 32]\t\tLoss: 2.2014 Acc@1: 16.797%\n",
            "| Epoch [ 10/ 50] Iter[ 31/ 32]\t\tLoss: 2.1220 Acc@1: 17.540%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 2.4607 Acc@1: 21.10%\n",
            "| Saving Best model...\t\t\tTop1 = 21.10%\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/ 50] Iter[  1/ 32]\t\tLoss: 2.1615 Acc@1: 12.500%\n",
            "| Epoch [ 11/ 50] Iter[ 16/ 32]\t\tLoss: 2.2472 Acc@1: 16.016%\n",
            "| Epoch [ 11/ 50] Iter[ 31/ 32]\t\tLoss: 2.4099 Acc@1: 17.641%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 2.4267 Acc@1: 18.40%\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/ 50] Iter[  1/ 32]\t\tLoss: 1.9898 Acc@1: 28.125%\n",
            "| Epoch [ 12/ 50] Iter[ 16/ 32]\t\tLoss: 2.1288 Acc@1: 19.922%\n",
            "| Epoch [ 12/ 50] Iter[ 31/ 32]\t\tLoss: 2.2029 Acc@1: 20.262%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 2.3156 Acc@1: 21.85%\n",
            "| Saving Best model...\t\t\tTop1 = 21.85%\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/ 50] Iter[  1/ 32]\t\tLoss: 2.3869 Acc@1: 15.625%\n",
            "| Epoch [ 13/ 50] Iter[ 16/ 32]\t\tLoss: 2.3039 Acc@1: 22.070%\n",
            "| Epoch [ 13/ 50] Iter[ 31/ 32]\t\tLoss: 2.1103 Acc@1: 20.060%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 1.9443 Acc@1: 19.60%\n",
            "\n",
            "=> Training Epoch #14, LR=0.1000\n",
            "| Epoch [ 14/ 50] Iter[  1/ 32]\t\tLoss: 2.0669 Acc@1: 18.750%\n",
            "| Epoch [ 14/ 50] Iter[ 16/ 32]\t\tLoss: 2.3821 Acc@1: 19.727%\n",
            "| Epoch [ 14/ 50] Iter[ 31/ 32]\t\tLoss: 2.0911 Acc@1: 21.573%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 2.9074 Acc@1: 23.10%\n",
            "| Saving Best model...\t\t\tTop1 = 23.10%\n",
            "\n",
            "=> Training Epoch #15, LR=0.1000\n",
            "| Epoch [ 15/ 50] Iter[  1/ 32]\t\tLoss: 2.2244 Acc@1: 9.375%\n",
            "| Epoch [ 15/ 50] Iter[ 16/ 32]\t\tLoss: 1.9293 Acc@1: 18.945%\n",
            "| Epoch [ 15/ 50] Iter[ 31/ 32]\t\tLoss: 2.0709 Acc@1: 19.859%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 1.1348 Acc@1: 20.60%\n",
            "\n",
            "=> Training Epoch #16, LR=0.1000\n",
            "| Epoch [ 16/ 50] Iter[  1/ 32]\t\tLoss: 2.5259 Acc@1: 12.500%\n",
            "| Epoch [ 16/ 50] Iter[ 16/ 32]\t\tLoss: 2.0158 Acc@1: 21.094%\n",
            "| Epoch [ 16/ 50] Iter[ 31/ 32]\t\tLoss: 2.0720 Acc@1: 20.766%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 1.9787 Acc@1: 23.35%\n",
            "| Saving Best model...\t\t\tTop1 = 23.35%\n",
            "\n",
            "=> Training Epoch #17, LR=0.1000\n",
            "| Epoch [ 17/ 50] Iter[  1/ 32]\t\tLoss: 2.0121 Acc@1: 25.000%\n",
            "| Epoch [ 17/ 50] Iter[ 16/ 32]\t\tLoss: 2.0049 Acc@1: 23.242%\n",
            "| Epoch [ 17/ 50] Iter[ 31/ 32]\t\tLoss: 2.4754 Acc@1: 22.379%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 1.7887 Acc@1: 26.85%\n",
            "| Saving Best model...\t\t\tTop1 = 26.85%\n",
            "\n",
            "=> Training Epoch #18, LR=0.1000\n",
            "| Epoch [ 18/ 50] Iter[  1/ 32]\t\tLoss: 1.7409 Acc@1: 46.875%\n",
            "| Epoch [ 18/ 50] Iter[ 16/ 32]\t\tLoss: 2.0318 Acc@1: 28.516%\n",
            "| Epoch [ 18/ 50] Iter[ 31/ 32]\t\tLoss: 1.9727 Acc@1: 24.899%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 2.3667 Acc@1: 22.85%\n",
            "\n",
            "=> Training Epoch #19, LR=0.1000\n",
            "| Epoch [ 19/ 50] Iter[  1/ 32]\t\tLoss: 2.1186 Acc@1: 28.125%\n",
            "| Epoch [ 19/ 50] Iter[ 16/ 32]\t\tLoss: 1.9703 Acc@1: 24.609%\n",
            "| Epoch [ 19/ 50] Iter[ 31/ 32]\t\tLoss: 2.0019 Acc@1: 23.387%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 1.7873 Acc@1: 27.05%\n",
            "| Saving Best model...\t\t\tTop1 = 27.05%\n",
            "\n",
            "=> Training Epoch #20, LR=0.1000\n",
            "| Epoch [ 20/ 50] Iter[  1/ 32]\t\tLoss: 2.0191 Acc@1: 25.000%\n",
            "| Epoch [ 20/ 50] Iter[ 16/ 32]\t\tLoss: 1.9298 Acc@1: 27.930%\n",
            "| Epoch [ 20/ 50] Iter[ 31/ 32]\t\tLoss: 2.0425 Acc@1: 27.419%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 1.1965 Acc@1: 26.45%\n",
            "\n",
            "=> Training Epoch #21, LR=0.1000\n",
            "| Epoch [ 21/ 50] Iter[  1/ 32]\t\tLoss: 1.7341 Acc@1: 34.375%\n",
            "| Epoch [ 21/ 50] Iter[ 16/ 32]\t\tLoss: 1.8541 Acc@1: 28.320%\n",
            "| Epoch [ 21/ 50] Iter[ 31/ 32]\t\tLoss: 2.2730 Acc@1: 27.823%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 2.1273 Acc@1: 28.90%\n",
            "| Saving Best model...\t\t\tTop1 = 28.90%\n",
            "\n",
            "=> Training Epoch #22, LR=0.1000\n",
            "| Epoch [ 22/ 50] Iter[  1/ 32]\t\tLoss: 1.8683 Acc@1: 25.000%\n",
            "| Epoch [ 22/ 50] Iter[ 16/ 32]\t\tLoss: 2.0336 Acc@1: 30.469%\n",
            "| Epoch [ 22/ 50] Iter[ 31/ 32]\t\tLoss: 2.3728 Acc@1: 29.032%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 1.3808 Acc@1: 28.65%\n",
            "\n",
            "=> Training Epoch #23, LR=0.1000\n",
            "| Epoch [ 23/ 50] Iter[  1/ 32]\t\tLoss: 2.0587 Acc@1: 18.750%\n",
            "| Epoch [ 23/ 50] Iter[ 16/ 32]\t\tLoss: 1.8691 Acc@1: 26.953%\n",
            "| Epoch [ 23/ 50] Iter[ 31/ 32]\t\tLoss: 1.9797 Acc@1: 27.923%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 1.9748 Acc@1: 26.20%\n",
            "\n",
            "=> Training Epoch #24, LR=0.1000\n",
            "| Epoch [ 24/ 50] Iter[  1/ 32]\t\tLoss: 1.9151 Acc@1: 31.250%\n",
            "| Epoch [ 24/ 50] Iter[ 16/ 32]\t\tLoss: 1.8874 Acc@1: 29.297%\n",
            "| Epoch [ 24/ 50] Iter[ 31/ 32]\t\tLoss: 2.1440 Acc@1: 27.722%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 0.9143 Acc@1: 20.00%\n",
            "\n",
            "=> Training Epoch #25, LR=0.1000\n",
            "| Epoch [ 25/ 50] Iter[  1/ 32]\t\tLoss: 2.0122 Acc@1: 31.250%\n",
            "| Epoch [ 25/ 50] Iter[ 16/ 32]\t\tLoss: 1.9157 Acc@1: 26.562%\n",
            "| Epoch [ 25/ 50] Iter[ 31/ 32]\t\tLoss: 1.6909 Acc@1: 26.310%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 1.6813 Acc@1: 28.75%\n",
            "\n",
            "=> Training Epoch #26, LR=0.1000\n",
            "| Epoch [ 26/ 50] Iter[  1/ 32]\t\tLoss: 2.0450 Acc@1: 25.000%\n",
            "| Epoch [ 26/ 50] Iter[ 16/ 32]\t\tLoss: 1.5856 Acc@1: 28.516%\n",
            "| Epoch [ 26/ 50] Iter[ 31/ 32]\t\tLoss: 2.3046 Acc@1: 28.528%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 1.9608 Acc@1: 30.30%\n",
            "| Saving Best model...\t\t\tTop1 = 30.30%\n",
            "\n",
            "=> Training Epoch #27, LR=0.1000\n",
            "| Epoch [ 27/ 50] Iter[  1/ 32]\t\tLoss: 1.9688 Acc@1: 25.000%\n",
            "| Epoch [ 27/ 50] Iter[ 16/ 32]\t\tLoss: 2.2644 Acc@1: 29.297%\n",
            "| Epoch [ 27/ 50] Iter[ 31/ 32]\t\tLoss: 1.8019 Acc@1: 29.435%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 1.5625 Acc@1: 31.35%\n",
            "| Saving Best model...\t\t\tTop1 = 31.35%\n",
            "\n",
            "=> Training Epoch #28, LR=0.1000\n",
            "| Epoch [ 28/ 50] Iter[  1/ 32]\t\tLoss: 1.8293 Acc@1: 40.625%\n",
            "| Epoch [ 28/ 50] Iter[ 16/ 32]\t\tLoss: 2.1116 Acc@1: 30.859%\n",
            "| Epoch [ 28/ 50] Iter[ 31/ 32]\t\tLoss: 1.7218 Acc@1: 31.048%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 1.2354 Acc@1: 30.80%\n",
            "\n",
            "=> Training Epoch #29, LR=0.1000\n",
            "| Epoch [ 29/ 50] Iter[  1/ 32]\t\tLoss: 2.1412 Acc@1: 21.875%\n",
            "| Epoch [ 29/ 50] Iter[ 16/ 32]\t\tLoss: 1.9584 Acc@1: 30.664%\n",
            "| Epoch [ 29/ 50] Iter[ 31/ 32]\t\tLoss: 1.9430 Acc@1: 31.048%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 1.8414 Acc@1: 29.45%\n",
            "\n",
            "=> Training Epoch #30, LR=0.1000\n",
            "| Epoch [ 30/ 50] Iter[  1/ 32]\t\tLoss: 1.8424 Acc@1: 31.250%\n",
            "| Epoch [ 30/ 50] Iter[ 16/ 32]\t\tLoss: 1.7507 Acc@1: 30.469%\n",
            "| Epoch [ 30/ 50] Iter[ 31/ 32]\t\tLoss: 1.6566 Acc@1: 30.948%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 1.6329 Acc@1: 30.85%\n",
            "\n",
            "=> Training Epoch #31, LR=0.1000\n",
            "| Epoch [ 31/ 50] Iter[  1/ 32]\t\tLoss: 1.9866 Acc@1: 37.500%\n",
            "| Epoch [ 31/ 50] Iter[ 16/ 32]\t\tLoss: 1.9923 Acc@1: 31.250%\n",
            "| Epoch [ 31/ 50] Iter[ 31/ 32]\t\tLoss: 1.8482 Acc@1: 29.536%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 1.6621 Acc@1: 28.65%\n",
            "\n",
            "=> Training Epoch #32, LR=0.1000\n",
            "| Epoch [ 32/ 50] Iter[  1/ 32]\t\tLoss: 1.8878 Acc@1: 28.125%\n",
            "| Epoch [ 32/ 50] Iter[ 16/ 32]\t\tLoss: 1.7341 Acc@1: 31.250%\n",
            "| Epoch [ 32/ 50] Iter[ 31/ 32]\t\tLoss: 1.5358 Acc@1: 31.552%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 3.1291 Acc@1: 30.00%\n",
            "\n",
            "=> Training Epoch #33, LR=0.1000\n",
            "| Epoch [ 33/ 50] Iter[  1/ 32]\t\tLoss: 2.0035 Acc@1: 21.875%\n",
            "| Epoch [ 33/ 50] Iter[ 16/ 32]\t\tLoss: 2.1200 Acc@1: 27.344%\n",
            "| Epoch [ 33/ 50] Iter[ 31/ 32]\t\tLoss: 2.0974 Acc@1: 30.343%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 1.2610 Acc@1: 30.15%\n",
            "\n",
            "=> Training Epoch #34, LR=0.1000\n",
            "| Epoch [ 34/ 50] Iter[  1/ 32]\t\tLoss: 1.6678 Acc@1: 40.625%\n",
            "| Epoch [ 34/ 50] Iter[ 16/ 32]\t\tLoss: 1.6083 Acc@1: 33.594%\n",
            "| Epoch [ 34/ 50] Iter[ 31/ 32]\t\tLoss: 2.0606 Acc@1: 31.653%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 2.1269 Acc@1: 31.80%\n",
            "| Saving Best model...\t\t\tTop1 = 31.80%\n",
            "\n",
            "=> Training Epoch #35, LR=0.1000\n",
            "| Epoch [ 35/ 50] Iter[  1/ 32]\t\tLoss: 1.8521 Acc@1: 25.000%\n",
            "| Epoch [ 35/ 50] Iter[ 16/ 32]\t\tLoss: 1.6941 Acc@1: 30.273%\n",
            "| Epoch [ 35/ 50] Iter[ 31/ 32]\t\tLoss: 2.0617 Acc@1: 31.250%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 1.2203 Acc@1: 27.55%\n",
            "\n",
            "=> Training Epoch #36, LR=0.1000\n",
            "| Epoch [ 36/ 50] Iter[  1/ 32]\t\tLoss: 1.7550 Acc@1: 34.375%\n",
            "| Epoch [ 36/ 50] Iter[ 16/ 32]\t\tLoss: 1.7014 Acc@1: 30.469%\n",
            "| Epoch [ 36/ 50] Iter[ 31/ 32]\t\tLoss: 1.6671 Acc@1: 31.552%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 1.9647 Acc@1: 32.30%\n",
            "| Saving Best model...\t\t\tTop1 = 32.30%\n",
            "\n",
            "=> Training Epoch #37, LR=0.1000\n",
            "| Epoch [ 37/ 50] Iter[  1/ 32]\t\tLoss: 2.0176 Acc@1: 21.875%\n",
            "| Epoch [ 37/ 50] Iter[ 16/ 32]\t\tLoss: 1.6298 Acc@1: 37.305%\n",
            "| Epoch [ 37/ 50] Iter[ 31/ 32]\t\tLoss: 1.8635 Acc@1: 33.569%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 2.2051 Acc@1: 29.55%\n",
            "\n",
            "=> Training Epoch #38, LR=0.1000\n",
            "| Epoch [ 38/ 50] Iter[  1/ 32]\t\tLoss: 1.7154 Acc@1: 40.625%\n",
            "| Epoch [ 38/ 50] Iter[ 16/ 32]\t\tLoss: 1.5124 Acc@1: 36.719%\n",
            "| Epoch [ 38/ 50] Iter[ 31/ 32]\t\tLoss: 1.6853 Acc@1: 34.677%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 1.5796 Acc@1: 35.35%\n",
            "| Saving Best model...\t\t\tTop1 = 35.35%\n",
            "\n",
            "=> Training Epoch #39, LR=0.1000\n",
            "| Epoch [ 39/ 50] Iter[  1/ 32]\t\tLoss: 1.7748 Acc@1: 28.125%\n",
            "| Epoch [ 39/ 50] Iter[ 16/ 32]\t\tLoss: 1.7311 Acc@1: 34.180%\n",
            "| Epoch [ 39/ 50] Iter[ 31/ 32]\t\tLoss: 1.8993 Acc@1: 35.181%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 0.9643 Acc@1: 28.35%\n",
            "\n",
            "=> Training Epoch #40, LR=0.1000\n",
            "| Epoch [ 40/ 50] Iter[  1/ 32]\t\tLoss: 1.8672 Acc@1: 37.500%\n",
            "| Epoch [ 40/ 50] Iter[ 16/ 32]\t\tLoss: 1.7137 Acc@1: 37.305%\n",
            "| Epoch [ 40/ 50] Iter[ 31/ 32]\t\tLoss: 1.8654 Acc@1: 34.778%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 1.7988 Acc@1: 31.05%\n",
            "\n",
            "=> Training Epoch #41, LR=0.1000\n",
            "| Epoch [ 41/ 50] Iter[  1/ 32]\t\tLoss: 1.8959 Acc@1: 21.875%\n",
            "| Epoch [ 41/ 50] Iter[ 16/ 32]\t\tLoss: 1.6707 Acc@1: 34.766%\n",
            "| Epoch [ 41/ 50] Iter[ 31/ 32]\t\tLoss: 1.9320 Acc@1: 35.585%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 2.6698 Acc@1: 28.90%\n",
            "\n",
            "=> Training Epoch #42, LR=0.1000\n",
            "| Epoch [ 42/ 50] Iter[  1/ 32]\t\tLoss: 1.7824 Acc@1: 34.375%\n",
            "| Epoch [ 42/ 50] Iter[ 16/ 32]\t\tLoss: 1.7653 Acc@1: 37.305%\n",
            "| Epoch [ 42/ 50] Iter[ 31/ 32]\t\tLoss: 1.9364 Acc@1: 36.593%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 1.1337 Acc@1: 35.85%\n",
            "| Saving Best model...\t\t\tTop1 = 35.85%\n",
            "\n",
            "=> Training Epoch #43, LR=0.1000\n",
            "| Epoch [ 43/ 50] Iter[  1/ 32]\t\tLoss: 1.5429 Acc@1: 37.500%\n",
            "| Epoch [ 43/ 50] Iter[ 16/ 32]\t\tLoss: 1.8335 Acc@1: 37.305%\n",
            "| Epoch [ 43/ 50] Iter[ 31/ 32]\t\tLoss: 1.4822 Acc@1: 36.593%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 2.1141 Acc@1: 36.30%\n",
            "| Saving Best model...\t\t\tTop1 = 36.30%\n",
            "\n",
            "=> Training Epoch #44, LR=0.1000\n",
            "| Epoch [ 44/ 50] Iter[  1/ 32]\t\tLoss: 1.6613 Acc@1: 31.250%\n",
            "| Epoch [ 44/ 50] Iter[ 16/ 32]\t\tLoss: 1.4448 Acc@1: 38.477%\n",
            "| Epoch [ 44/ 50] Iter[ 31/ 32]\t\tLoss: 1.6630 Acc@1: 36.794%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 1.2623 Acc@1: 32.85%\n",
            "\n",
            "=> Training Epoch #45, LR=0.1000\n",
            "| Epoch [ 45/ 50] Iter[  1/ 32]\t\tLoss: 1.6119 Acc@1: 31.250%\n",
            "| Epoch [ 45/ 50] Iter[ 16/ 32]\t\tLoss: 1.5552 Acc@1: 34.766%\n",
            "| Epoch [ 45/ 50] Iter[ 31/ 32]\t\tLoss: 1.5828 Acc@1: 34.577%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 1.4707 Acc@1: 32.25%\n",
            "\n",
            "=> Training Epoch #46, LR=0.1000\n",
            "| Epoch [ 46/ 50] Iter[  1/ 32]\t\tLoss: 1.7841 Acc@1: 40.625%\n",
            "| Epoch [ 46/ 50] Iter[ 16/ 32]\t\tLoss: 1.3775 Acc@1: 35.742%\n",
            "| Epoch [ 46/ 50] Iter[ 31/ 32]\t\tLoss: 1.4236 Acc@1: 36.089%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 1.2700 Acc@1: 39.50%\n",
            "| Saving Best model...\t\t\tTop1 = 39.50%\n",
            "\n",
            "=> Training Epoch #47, LR=0.1000\n",
            "| Epoch [ 47/ 50] Iter[  1/ 32]\t\tLoss: 1.7487 Acc@1: 21.875%\n",
            "| Epoch [ 47/ 50] Iter[ 16/ 32]\t\tLoss: 1.5091 Acc@1: 38.281%\n",
            "| Epoch [ 47/ 50] Iter[ 31/ 32]\t\tLoss: 1.5194 Acc@1: 38.306%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 1.0351 Acc@1: 36.20%\n",
            "\n",
            "=> Training Epoch #48, LR=0.1000\n",
            "| Epoch [ 48/ 50] Iter[  1/ 32]\t\tLoss: 1.6831 Acc@1: 34.375%\n",
            "| Epoch [ 48/ 50] Iter[ 16/ 32]\t\tLoss: 1.7873 Acc@1: 42.969%\n",
            "| Epoch [ 48/ 50] Iter[ 31/ 32]\t\tLoss: 1.6193 Acc@1: 39.919%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 0.9984 Acc@1: 34.95%\n",
            "\n",
            "=> Training Epoch #49, LR=0.1000\n",
            "| Epoch [ 49/ 50] Iter[  1/ 32]\t\tLoss: 1.7429 Acc@1: 34.375%\n",
            "| Epoch [ 49/ 50] Iter[ 16/ 32]\t\tLoss: 1.5439 Acc@1: 38.867%\n",
            "| Epoch [ 49/ 50] Iter[ 31/ 32]\t\tLoss: 1.5479 Acc@1: 37.097%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 1.5086 Acc@1: 37.10%\n"
          ]
        }
      ],
      "source": [
        "# Training not pretrained model\n",
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    train(epoch,net18_untrained,train_loader)\n",
        "    acc =test(epoch,net18_untrained,valid_loader)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        net18_untrained_save.load_state_dict(net18_untrained.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training pretrained model\n",
        "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "# False if we want to fine the whole model and True if only the last layer\n",
        "feature_extract = True \n",
        "\n",
        "if feature_extract:\n",
        "    for param in net18_pretrained.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Reshaping the last layer so output=num_classes\n",
        "net18_pretrained.fc = nn.Linear(512, num_class).to(device)\n",
        "net18_pretrained_save.fc = nn.Linear(512, num_class).to(device)\n",
        "\n",
        "\n",
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    train(epoch,net18_pretrained,train_loader)\n",
        "    acc =test(epoch,net18_pretrained,valid_loader)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        net18_pretrained_save.load_state_dict(net18_pretrained.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTqxvdeJwxWF",
        "outputId": "98960556-f122-4099-c40d-6a7c39abff94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.1000\n",
            "| Epoch [  0/ 50] Iter[  1/ 32]\t\tLoss: 2.4857 Acc@1: 6.250%\n",
            "| Epoch [  0/ 50] Iter[ 16/ 32]\t\tLoss: 26.5734 Acc@1: 13.672%\n",
            "| Epoch [  0/ 50] Iter[ 31/ 32]\t\tLoss: 37.3026 Acc@1: 16.331%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 13.1509 Acc@1: 19.15%\n",
            "| Saving Best model...\t\t\tTop1 = 19.15%\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/ 50] Iter[  1/ 32]\t\tLoss: 22.5784 Acc@1: 21.875%\n",
            "| Epoch [  1/ 50] Iter[ 16/ 32]\t\tLoss: 13.0909 Acc@1: 24.023%\n",
            "| Epoch [  1/ 50] Iter[ 31/ 32]\t\tLoss: 15.4483 Acc@1: 22.681%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 8.5636 Acc@1: 22.95%\n",
            "| Saving Best model...\t\t\tTop1 = 22.95%\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/ 50] Iter[  1/ 32]\t\tLoss: 12.6856 Acc@1: 28.125%\n",
            "| Epoch [  2/ 50] Iter[ 16/ 32]\t\tLoss: 15.3286 Acc@1: 24.023%\n",
            "| Epoch [  2/ 50] Iter[ 31/ 32]\t\tLoss: 19.1791 Acc@1: 23.286%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 18.5480 Acc@1: 22.50%\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/ 50] Iter[  1/ 32]\t\tLoss: 22.1370 Acc@1: 21.875%\n",
            "| Epoch [  3/ 50] Iter[ 16/ 32]\t\tLoss: 13.5005 Acc@1: 27.539%\n",
            "| Epoch [  3/ 50] Iter[ 31/ 32]\t\tLoss: 24.8559 Acc@1: 26.210%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 63.1818 Acc@1: 20.85%\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/ 50] Iter[  1/ 32]\t\tLoss: 24.9123 Acc@1: 21.875%\n",
            "| Epoch [  4/ 50] Iter[ 16/ 32]\t\tLoss: 18.8972 Acc@1: 26.953%\n",
            "| Epoch [  4/ 50] Iter[ 31/ 32]\t\tLoss: 23.3676 Acc@1: 25.806%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 26.2163 Acc@1: 20.85%\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/ 50] Iter[  1/ 32]\t\tLoss: 15.6860 Acc@1: 37.500%\n",
            "| Epoch [  5/ 50] Iter[ 16/ 32]\t\tLoss: 13.5240 Acc@1: 28.320%\n",
            "| Epoch [  5/ 50] Iter[ 31/ 32]\t\tLoss: 21.0794 Acc@1: 26.210%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 1.2069 Acc@1: 21.80%\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/ 50] Iter[  1/ 32]\t\tLoss: 31.6234 Acc@1: 9.375%\n",
            "| Epoch [  6/ 50] Iter[ 16/ 32]\t\tLoss: 9.1822 Acc@1: 24.023%\n",
            "| Epoch [  6/ 50] Iter[ 31/ 32]\t\tLoss: 25.5756 Acc@1: 25.302%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 28.0980 Acc@1: 18.95%\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/ 50] Iter[  1/ 32]\t\tLoss: 28.3661 Acc@1: 18.750%\n",
            "| Epoch [  7/ 50] Iter[ 16/ 32]\t\tLoss: 21.3865 Acc@1: 23.828%\n",
            "| Epoch [  7/ 50] Iter[ 31/ 32]\t\tLoss: 21.5181 Acc@1: 21.875%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 20.0208 Acc@1: 25.80%\n",
            "| Saving Best model...\t\t\tTop1 = 25.80%\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/ 50] Iter[  1/ 32]\t\tLoss: 24.7202 Acc@1: 18.750%\n",
            "| Epoch [  8/ 50] Iter[ 16/ 32]\t\tLoss: 23.7938 Acc@1: 24.609%\n",
            "| Epoch [  8/ 50] Iter[ 31/ 32]\t\tLoss: 22.0143 Acc@1: 26.008%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 41.5586 Acc@1: 27.70%\n",
            "| Saving Best model...\t\t\tTop1 = 27.70%\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/ 50] Iter[  1/ 32]\t\tLoss: 15.8264 Acc@1: 37.500%\n",
            "| Epoch [  9/ 50] Iter[ 16/ 32]\t\tLoss: 20.9079 Acc@1: 29.883%\n",
            "| Epoch [  9/ 50] Iter[ 31/ 32]\t\tLoss: 20.9847 Acc@1: 27.218%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 35.3962 Acc@1: 25.70%\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/ 50] Iter[  1/ 32]\t\tLoss: 19.5997 Acc@1: 25.000%\n",
            "| Epoch [ 10/ 50] Iter[ 16/ 32]\t\tLoss: 24.4074 Acc@1: 26.367%\n",
            "| Epoch [ 10/ 50] Iter[ 31/ 32]\t\tLoss: 18.6077 Acc@1: 25.806%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 31.2418 Acc@1: 21.75%\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/ 50] Iter[  1/ 32]\t\tLoss: 26.3691 Acc@1: 18.750%\n",
            "| Epoch [ 11/ 50] Iter[ 16/ 32]\t\tLoss: 20.2273 Acc@1: 26.758%\n",
            "| Epoch [ 11/ 50] Iter[ 31/ 32]\t\tLoss: 21.8859 Acc@1: 26.915%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 53.0073 Acc@1: 22.40%\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/ 50] Iter[  1/ 32]\t\tLoss: 19.0957 Acc@1: 28.125%\n",
            "| Epoch [ 12/ 50] Iter[ 16/ 32]\t\tLoss: 19.0361 Acc@1: 27.539%\n",
            "| Epoch [ 12/ 50] Iter[ 31/ 32]\t\tLoss: 21.6763 Acc@1: 27.218%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 56.6225 Acc@1: 23.55%\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/ 50] Iter[  1/ 32]\t\tLoss: 30.9971 Acc@1: 15.625%\n",
            "| Epoch [ 13/ 50] Iter[ 16/ 32]\t\tLoss: 20.0418 Acc@1: 25.391%\n",
            "| Epoch [ 13/ 50] Iter[ 31/ 32]\t\tLoss: 20.1877 Acc@1: 26.411%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 31.2502 Acc@1: 24.95%\n",
            "\n",
            "=> Training Epoch #14, LR=0.1000\n",
            "| Epoch [ 14/ 50] Iter[  1/ 32]\t\tLoss: 30.3941 Acc@1: 9.375%\n",
            "| Epoch [ 14/ 50] Iter[ 16/ 32]\t\tLoss: 24.5867 Acc@1: 24.805%\n",
            "| Epoch [ 14/ 50] Iter[ 31/ 32]\t\tLoss: 25.8680 Acc@1: 25.504%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 42.4844 Acc@1: 21.70%\n",
            "\n",
            "=> Training Epoch #15, LR=0.1000\n",
            "| Epoch [ 15/ 50] Iter[  1/ 32]\t\tLoss: 27.9291 Acc@1: 9.375%\n",
            "| Epoch [ 15/ 50] Iter[ 16/ 32]\t\tLoss: 18.9894 Acc@1: 25.977%\n",
            "| Epoch [ 15/ 50] Iter[ 31/ 32]\t\tLoss: 22.9622 Acc@1: 23.589%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 64.2956 Acc@1: 24.30%\n",
            "\n",
            "=> Training Epoch #16, LR=0.1000\n",
            "| Epoch [ 16/ 50] Iter[  1/ 32]\t\tLoss: 27.2680 Acc@1: 15.625%\n",
            "| Epoch [ 16/ 50] Iter[ 16/ 32]\t\tLoss: 26.0612 Acc@1: 27.734%\n",
            "| Epoch [ 16/ 50] Iter[ 31/ 32]\t\tLoss: 34.2471 Acc@1: 25.806%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 24.6569 Acc@1: 25.35%\n",
            "\n",
            "=> Training Epoch #17, LR=0.1000\n",
            "| Epoch [ 17/ 50] Iter[  1/ 32]\t\tLoss: 24.5737 Acc@1: 28.125%\n",
            "| Epoch [ 17/ 50] Iter[ 16/ 32]\t\tLoss: 14.2547 Acc@1: 26.953%\n",
            "| Epoch [ 17/ 50] Iter[ 31/ 32]\t\tLoss: 22.6545 Acc@1: 25.504%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 19.6481 Acc@1: 24.10%\n",
            "\n",
            "=> Training Epoch #18, LR=0.1000\n",
            "| Epoch [ 18/ 50] Iter[  1/ 32]\t\tLoss: 22.8316 Acc@1: 28.125%\n",
            "| Epoch [ 18/ 50] Iter[ 16/ 32]\t\tLoss: 19.4148 Acc@1: 30.273%\n",
            "| Epoch [ 18/ 50] Iter[ 31/ 32]\t\tLoss: 22.6024 Acc@1: 28.427%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 23.9777 Acc@1: 24.80%\n",
            "\n",
            "=> Training Epoch #19, LR=0.1000\n",
            "| Epoch [ 19/ 50] Iter[  1/ 32]\t\tLoss: 9.1694 Acc@1: 37.500%\n",
            "| Epoch [ 19/ 50] Iter[ 16/ 32]\t\tLoss: 17.5458 Acc@1: 28.516%\n",
            "| Epoch [ 19/ 50] Iter[ 31/ 32]\t\tLoss: 36.8563 Acc@1: 26.210%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 15.2524 Acc@1: 24.40%\n",
            "\n",
            "=> Training Epoch #20, LR=0.1000\n",
            "| Epoch [ 20/ 50] Iter[  1/ 32]\t\tLoss: 25.1215 Acc@1: 6.250%\n",
            "| Epoch [ 20/ 50] Iter[ 16/ 32]\t\tLoss: 18.1454 Acc@1: 25.977%\n",
            "| Epoch [ 20/ 50] Iter[ 31/ 32]\t\tLoss: 22.3346 Acc@1: 26.815%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 35.0911 Acc@1: 28.45%\n",
            "| Saving Best model...\t\t\tTop1 = 28.45%\n",
            "\n",
            "=> Training Epoch #21, LR=0.1000\n",
            "| Epoch [ 21/ 50] Iter[  1/ 32]\t\tLoss: 19.0422 Acc@1: 37.500%\n",
            "| Epoch [ 21/ 50] Iter[ 16/ 32]\t\tLoss: 17.5863 Acc@1: 29.688%\n",
            "| Epoch [ 21/ 50] Iter[ 31/ 32]\t\tLoss: 18.0622 Acc@1: 28.125%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 60.0015 Acc@1: 23.25%\n",
            "\n",
            "=> Training Epoch #22, LR=0.1000\n",
            "| Epoch [ 22/ 50] Iter[  1/ 32]\t\tLoss: 29.2292 Acc@1: 25.000%\n",
            "| Epoch [ 22/ 50] Iter[ 16/ 32]\t\tLoss: 18.0769 Acc@1: 27.148%\n",
            "| Epoch [ 22/ 50] Iter[ 31/ 32]\t\tLoss: 20.1102 Acc@1: 26.210%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 31.9724 Acc@1: 22.90%\n",
            "\n",
            "=> Training Epoch #23, LR=0.1000\n",
            "| Epoch [ 23/ 50] Iter[  1/ 32]\t\tLoss: 15.2748 Acc@1: 31.250%\n",
            "| Epoch [ 23/ 50] Iter[ 16/ 32]\t\tLoss: 19.9565 Acc@1: 28.906%\n",
            "| Epoch [ 23/ 50] Iter[ 31/ 32]\t\tLoss: 23.0100 Acc@1: 27.621%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 31.3884 Acc@1: 25.30%\n",
            "\n",
            "=> Training Epoch #24, LR=0.1000\n",
            "| Epoch [ 24/ 50] Iter[  1/ 32]\t\tLoss: 14.9301 Acc@1: 31.250%\n",
            "| Epoch [ 24/ 50] Iter[ 16/ 32]\t\tLoss: 18.2503 Acc@1: 26.953%\n",
            "| Epoch [ 24/ 50] Iter[ 31/ 32]\t\tLoss: 16.4803 Acc@1: 25.202%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 22.2327 Acc@1: 23.20%\n",
            "\n",
            "=> Training Epoch #25, LR=0.1000\n",
            "| Epoch [ 25/ 50] Iter[  1/ 32]\t\tLoss: 25.4546 Acc@1: 21.875%\n",
            "| Epoch [ 25/ 50] Iter[ 16/ 32]\t\tLoss: 23.6928 Acc@1: 26.953%\n",
            "| Epoch [ 25/ 50] Iter[ 31/ 32]\t\tLoss: 15.1653 Acc@1: 25.000%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 27.0905 Acc@1: 22.80%\n",
            "\n",
            "=> Training Epoch #26, LR=0.1000\n",
            "| Epoch [ 26/ 50] Iter[  1/ 32]\t\tLoss: 21.4621 Acc@1: 28.125%\n",
            "| Epoch [ 26/ 50] Iter[ 16/ 32]\t\tLoss: 27.3017 Acc@1: 25.586%\n",
            "| Epoch [ 26/ 50] Iter[ 31/ 32]\t\tLoss: 26.5217 Acc@1: 26.915%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 32.2106 Acc@1: 24.80%\n",
            "\n",
            "=> Training Epoch #27, LR=0.1000\n",
            "| Epoch [ 27/ 50] Iter[  1/ 32]\t\tLoss: 27.3885 Acc@1: 40.625%\n",
            "| Epoch [ 27/ 50] Iter[ 16/ 32]\t\tLoss: 17.5359 Acc@1: 27.930%\n",
            "| Epoch [ 27/ 50] Iter[ 31/ 32]\t\tLoss: 20.0638 Acc@1: 27.621%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 14.8464 Acc@1: 22.30%\n",
            "\n",
            "=> Training Epoch #28, LR=0.1000\n",
            "| Epoch [ 28/ 50] Iter[  1/ 32]\t\tLoss: 25.5629 Acc@1: 18.750%\n",
            "| Epoch [ 28/ 50] Iter[ 16/ 32]\t\tLoss: 20.4124 Acc@1: 26.953%\n",
            "| Epoch [ 28/ 50] Iter[ 31/ 32]\t\tLoss: 24.2637 Acc@1: 25.302%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 35.5598 Acc@1: 25.55%\n",
            "\n",
            "=> Training Epoch #29, LR=0.1000\n",
            "| Epoch [ 29/ 50] Iter[  1/ 32]\t\tLoss: 26.9811 Acc@1: 21.875%\n",
            "| Epoch [ 29/ 50] Iter[ 16/ 32]\t\tLoss: 20.3090 Acc@1: 26.367%\n",
            "| Epoch [ 29/ 50] Iter[ 31/ 32]\t\tLoss: 20.1543 Acc@1: 25.504%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 52.1152 Acc@1: 26.55%\n",
            "\n",
            "=> Training Epoch #30, LR=0.1000\n",
            "| Epoch [ 30/ 50] Iter[  1/ 32]\t\tLoss: 23.5214 Acc@1: 25.000%\n",
            "| Epoch [ 30/ 50] Iter[ 16/ 32]\t\tLoss: 19.7531 Acc@1: 29.688%\n",
            "| Epoch [ 30/ 50] Iter[ 31/ 32]\t\tLoss: 19.8519 Acc@1: 28.125%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 41.5964 Acc@1: 23.50%\n",
            "\n",
            "=> Training Epoch #31, LR=0.1000\n",
            "| Epoch [ 31/ 50] Iter[  1/ 32]\t\tLoss: 26.1095 Acc@1: 21.875%\n",
            "| Epoch [ 31/ 50] Iter[ 16/ 32]\t\tLoss: 25.2521 Acc@1: 24.805%\n",
            "| Epoch [ 31/ 50] Iter[ 31/ 32]\t\tLoss: 22.3796 Acc@1: 25.706%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 27.2868 Acc@1: 24.15%\n",
            "\n",
            "=> Training Epoch #32, LR=0.1000\n",
            "| Epoch [ 32/ 50] Iter[  1/ 32]\t\tLoss: 30.1781 Acc@1: 25.000%\n",
            "| Epoch [ 32/ 50] Iter[ 16/ 32]\t\tLoss: 18.7097 Acc@1: 29.102%\n",
            "| Epoch [ 32/ 50] Iter[ 31/ 32]\t\tLoss: 24.3461 Acc@1: 26.411%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 54.2442 Acc@1: 25.80%\n",
            "\n",
            "=> Training Epoch #33, LR=0.1000\n",
            "| Epoch [ 33/ 50] Iter[  1/ 32]\t\tLoss: 29.8531 Acc@1: 31.250%\n",
            "| Epoch [ 33/ 50] Iter[ 16/ 32]\t\tLoss: 21.8791 Acc@1: 28.516%\n",
            "| Epoch [ 33/ 50] Iter[ 31/ 32]\t\tLoss: 25.6150 Acc@1: 27.722%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 36.2057 Acc@1: 26.20%\n",
            "\n",
            "=> Training Epoch #34, LR=0.1000\n",
            "| Epoch [ 34/ 50] Iter[  1/ 32]\t\tLoss: 15.2579 Acc@1: 25.000%\n",
            "| Epoch [ 34/ 50] Iter[ 16/ 32]\t\tLoss: 14.4316 Acc@1: 27.930%\n",
            "| Epoch [ 34/ 50] Iter[ 31/ 32]\t\tLoss: 28.0696 Acc@1: 26.815%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 33.7841 Acc@1: 25.10%\n",
            "\n",
            "=> Training Epoch #35, LR=0.1000\n",
            "| Epoch [ 35/ 50] Iter[  1/ 32]\t\tLoss: 18.3167 Acc@1: 40.625%\n",
            "| Epoch [ 35/ 50] Iter[ 16/ 32]\t\tLoss: 15.1931 Acc@1: 30.859%\n",
            "| Epoch [ 35/ 50] Iter[ 31/ 32]\t\tLoss: 16.9006 Acc@1: 29.839%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 39.2992 Acc@1: 25.25%\n",
            "\n",
            "=> Training Epoch #36, LR=0.1000\n",
            "| Epoch [ 36/ 50] Iter[  1/ 32]\t\tLoss: 19.5546 Acc@1: 25.000%\n",
            "| Epoch [ 36/ 50] Iter[ 16/ 32]\t\tLoss: 18.5903 Acc@1: 28.125%\n",
            "| Epoch [ 36/ 50] Iter[ 31/ 32]\t\tLoss: 17.6416 Acc@1: 26.915%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 25.0816 Acc@1: 26.50%\n",
            "\n",
            "=> Training Epoch #37, LR=0.1000\n",
            "| Epoch [ 37/ 50] Iter[  1/ 32]\t\tLoss: 19.5595 Acc@1: 34.375%\n",
            "| Epoch [ 37/ 50] Iter[ 16/ 32]\t\tLoss: 20.0081 Acc@1: 31.055%\n",
            "| Epoch [ 37/ 50] Iter[ 31/ 32]\t\tLoss: 19.6952 Acc@1: 27.923%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 30.6328 Acc@1: 22.80%\n",
            "\n",
            "=> Training Epoch #38, LR=0.1000\n",
            "| Epoch [ 38/ 50] Iter[  1/ 32]\t\tLoss: 28.0996 Acc@1: 25.000%\n",
            "| Epoch [ 38/ 50] Iter[ 16/ 32]\t\tLoss: 18.0023 Acc@1: 30.469%\n",
            "| Epoch [ 38/ 50] Iter[ 31/ 32]\t\tLoss: 22.1321 Acc@1: 31.552%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 14.3210 Acc@1: 22.65%\n",
            "\n",
            "=> Training Epoch #39, LR=0.1000\n",
            "| Epoch [ 39/ 50] Iter[  1/ 32]\t\tLoss: 24.2596 Acc@1: 40.625%\n",
            "| Epoch [ 39/ 50] Iter[ 16/ 32]\t\tLoss: 19.8754 Acc@1: 32.422%\n",
            "| Epoch [ 39/ 50] Iter[ 31/ 32]\t\tLoss: 29.5900 Acc@1: 29.335%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 75.2979 Acc@1: 26.50%\n",
            "\n",
            "=> Training Epoch #40, LR=0.1000\n",
            "| Epoch [ 40/ 50] Iter[  1/ 32]\t\tLoss: 28.9700 Acc@1: 18.750%\n",
            "| Epoch [ 40/ 50] Iter[ 16/ 32]\t\tLoss: 20.7302 Acc@1: 29.883%\n",
            "| Epoch [ 40/ 50] Iter[ 31/ 32]\t\tLoss: 27.2833 Acc@1: 26.512%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 49.5295 Acc@1: 23.60%\n",
            "\n",
            "=> Training Epoch #41, LR=0.1000\n",
            "| Epoch [ 41/ 50] Iter[  1/ 32]\t\tLoss: 18.4123 Acc@1: 28.125%\n",
            "| Epoch [ 41/ 50] Iter[ 16/ 32]\t\tLoss: 20.4101 Acc@1: 26.367%\n",
            "| Epoch [ 41/ 50] Iter[ 31/ 32]\t\tLoss: 23.2327 Acc@1: 26.512%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 39.6423 Acc@1: 25.50%\n",
            "\n",
            "=> Training Epoch #42, LR=0.1000\n",
            "| Epoch [ 42/ 50] Iter[  1/ 32]\t\tLoss: 22.0087 Acc@1: 15.625%\n",
            "| Epoch [ 42/ 50] Iter[ 16/ 32]\t\tLoss: 19.4389 Acc@1: 28.320%\n",
            "| Epoch [ 42/ 50] Iter[ 31/ 32]\t\tLoss: 21.7935 Acc@1: 26.310%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 47.9071 Acc@1: 25.15%\n",
            "\n",
            "=> Training Epoch #43, LR=0.1000\n",
            "| Epoch [ 43/ 50] Iter[  1/ 32]\t\tLoss: 24.5282 Acc@1: 21.875%\n",
            "| Epoch [ 43/ 50] Iter[ 16/ 32]\t\tLoss: 19.9276 Acc@1: 28.516%\n",
            "| Epoch [ 43/ 50] Iter[ 31/ 32]\t\tLoss: 19.6439 Acc@1: 28.125%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 20.1795 Acc@1: 25.40%\n",
            "\n",
            "=> Training Epoch #44, LR=0.1000\n",
            "| Epoch [ 44/ 50] Iter[  1/ 32]\t\tLoss: 23.4915 Acc@1: 25.000%\n",
            "| Epoch [ 44/ 50] Iter[ 16/ 32]\t\tLoss: 24.1664 Acc@1: 28.125%\n",
            "| Epoch [ 44/ 50] Iter[ 31/ 32]\t\tLoss: 15.5918 Acc@1: 27.520%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 44.9465 Acc@1: 25.00%\n",
            "\n",
            "=> Training Epoch #45, LR=0.1000\n",
            "| Epoch [ 45/ 50] Iter[  1/ 32]\t\tLoss: 19.5148 Acc@1: 28.125%\n",
            "| Epoch [ 45/ 50] Iter[ 16/ 32]\t\tLoss: 11.9905 Acc@1: 27.148%\n",
            "| Epoch [ 45/ 50] Iter[ 31/ 32]\t\tLoss: 16.1066 Acc@1: 28.125%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 18.6228 Acc@1: 23.75%\n",
            "\n",
            "=> Training Epoch #46, LR=0.1000\n",
            "| Epoch [ 46/ 50] Iter[  1/ 32]\t\tLoss: 19.7876 Acc@1: 34.375%\n",
            "| Epoch [ 46/ 50] Iter[ 16/ 32]\t\tLoss: 30.1638 Acc@1: 28.125%\n",
            "| Epoch [ 46/ 50] Iter[ 31/ 32]\t\tLoss: 20.0841 Acc@1: 27.016%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 40.7757 Acc@1: 23.90%\n",
            "\n",
            "=> Training Epoch #47, LR=0.1000\n",
            "| Epoch [ 47/ 50] Iter[  1/ 32]\t\tLoss: 25.4941 Acc@1: 25.000%\n",
            "| Epoch [ 47/ 50] Iter[ 16/ 32]\t\tLoss: 32.1819 Acc@1: 29.492%\n",
            "| Epoch [ 47/ 50] Iter[ 31/ 32]\t\tLoss: 15.3062 Acc@1: 26.310%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 20.9323 Acc@1: 25.60%\n",
            "\n",
            "=> Training Epoch #48, LR=0.1000\n",
            "| Epoch [ 48/ 50] Iter[  1/ 32]\t\tLoss: 15.5108 Acc@1: 34.375%\n",
            "| Epoch [ 48/ 50] Iter[ 16/ 32]\t\tLoss: 21.6350 Acc@1: 30.078%\n",
            "| Epoch [ 48/ 50] Iter[ 31/ 32]\t\tLoss: 24.7466 Acc@1: 28.931%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 17.1570 Acc@1: 22.30%\n",
            "\n",
            "=> Training Epoch #49, LR=0.1000\n",
            "| Epoch [ 49/ 50] Iter[  1/ 32]\t\tLoss: 26.8376 Acc@1: 28.125%\n",
            "| Epoch [ 49/ 50] Iter[ 16/ 32]\t\tLoss: 21.4978 Acc@1: 26.367%\n",
            "| Epoch [ 49/ 50] Iter[ 31/ 32]\t\tLoss: 15.8168 Acc@1: 26.512%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 31.4262 Acc@1: 26.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0fQoGYhKvA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a01b10-9070-4c9e-8580-6ffcf2ec9539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST \t\t\tLoss: 24.9964 Acc@1: 25.61%\n",
            "\n",
            "| TEST \t\t\tLoss: 1.5040 Acc@1: 36.30%\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "\n",
        "# Pretrained\n",
        "predicted_concat = test_final(net18_pretrained,test_loader)\n",
        "\n",
        "# Not pretrained\n",
        "predicted_concat = test_final(net18_untrained,test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWkxGbIukXEc"
      },
      "source": [
        "## Q2 : AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFgwUgZmkqp1"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "\n",
        "alexnet_untrained = models.alexnet(weights = None)\n",
        "alexnet_untrained = alexnet_untrained.to(device)\n",
        "alexnet_untrained_save = models.alexnet(weights = None) # To save the results\n",
        "alexnet_untrained_save = alexnet_untrained_save.to(device)\n",
        "\n",
        "alexnet_pretrained = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1)\n",
        "alexnet_pretrained = alexnet_pretrained.to(device)\n",
        "alexnet_pretrained_save = models.alexnet(weights = None) # To save the results\n",
        "alexnet_pretrained_save = alexnet_pretrained_save.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset has to be resized\n",
        "\n",
        "# Training images are 256x256\n",
        "resized_shape = 256\n",
        "\n",
        "transform_train_alexnet = transforms.Compose([\n",
        "    transforms.Resize(resized_shape),\n",
        "    transforms.RandomCrop(resized_shape, padding=4, padding_mode = 'reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "    CutoutDefault(resized_shape//2),\n",
        "])\n",
        "\n",
        "transform_test_alexnet = transforms.Compose([\n",
        "    transforms.Resize(resized_shape),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "test_loader_alexnet = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(Original_test_data_x, Original_test_data_y, transform=transform_test_alexnet),\n",
        "    batch_size = batch_size,\n",
        "    shuffle=False, num_workers=2)\n",
        "\n",
        "train_loader_alexnet = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_train, y_train, transform=transform_train_alexnet),\n",
        "    batch_size=batch_size,shuffle=True, num_workers=2) #num_workers = 2 ou 1\n",
        "\n",
        "valid_loader_alexnet = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_valid, y_valid, transform=transform_test_alexnet),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "PsrhnrjSzfO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJHbu1shkqp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a504ffe2-2c06-40d3-fd39-d59c5c028bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.1000\n",
            "| Epoch [  0/ 50] Iter[  1/ 32]\t\tLoss: 6.9043 Acc@1: 0.000%\n",
            "| Epoch [  0/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 12.305%\n",
            "| Epoch [  0/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.685%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: nan Acc@1: 10.00%\n",
            "| Saving Best model...\t\t\tTop1 = 10.00%\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [  1/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [  1/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [  2/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.742%\n",
            "| Epoch [  2/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [  3/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [  3/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [  4/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [  4/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [  5/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.547%\n",
            "| Epoch [  5/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [  6/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [  6/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [  7/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 11.914%\n",
            "| Epoch [  7/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 0.000%\n",
            "| Epoch [  8/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.570%\n",
            "| Epoch [  8/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [  9/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.961%\n",
            "| Epoch [  9/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.778%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 10/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.547%\n",
            "| Epoch [ 10/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 11/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 8.984%\n",
            "| Epoch [ 11/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.778%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [ 12/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 12/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 13/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.961%\n",
            "| Epoch [ 13/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #14, LR=0.1000\n",
            "| Epoch [ 14/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 14/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [ 14/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #15, LR=0.1000\n",
            "| Epoch [ 15/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 0.000%\n",
            "| Epoch [ 15/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [ 15/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #16, LR=0.1000\n",
            "| Epoch [ 16/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 16/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 7.617%\n",
            "| Epoch [ 16/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #17, LR=0.1000\n",
            "| Epoch [ 17/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [ 17/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 17/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #18, LR=0.1000\n",
            "| Epoch [ 18/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 18/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.742%\n",
            "| Epoch [ 18/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #19, LR=0.1000\n",
            "| Epoch [ 19/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 19/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.180%\n",
            "| Epoch [ 19/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #20, LR=0.1000\n",
            "| Epoch [ 20/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [ 20/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.742%\n",
            "| Epoch [ 20/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #21, LR=0.1000\n",
            "| Epoch [ 21/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [ 21/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 11.523%\n",
            "| Epoch [ 21/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #22, LR=0.1000\n",
            "| Epoch [ 22/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 22/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 8.008%\n",
            "| Epoch [ 22/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #23, LR=0.1000\n",
            "| Epoch [ 23/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 23/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 12.109%\n",
            "| Epoch [ 23/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #24, LR=0.1000\n",
            "| Epoch [ 24/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 24/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.547%\n",
            "| Epoch [ 24/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #25, LR=0.1000\n",
            "| Epoch [ 25/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 18.750%\n",
            "| Epoch [ 25/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 25/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #26, LR=0.1000\n",
            "| Epoch [ 26/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 26/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [ 26/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #27, LR=0.1000\n",
            "| Epoch [ 27/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 21.875%\n",
            "| Epoch [ 27/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.570%\n",
            "| Epoch [ 27/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.778%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #28, LR=0.1000\n",
            "| Epoch [ 28/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [ 28/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 28/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #29, LR=0.1000\n",
            "| Epoch [ 29/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 29/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.766%\n",
            "| Epoch [ 29/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #30, LR=0.1000\n",
            "| Epoch [ 30/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 21.875%\n",
            "| Epoch [ 30/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 30/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #31, LR=0.1000\n",
            "| Epoch [ 31/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 31/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 8.203%\n",
            "| Epoch [ 31/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #32, LR=0.1000\n",
            "| Epoch [ 32/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 32/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.156%\n",
            "| Epoch [ 32/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #33, LR=0.1000\n",
            "| Epoch [ 33/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 18.750%\n",
            "| Epoch [ 33/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 11.328%\n",
            "| Epoch [ 33/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #34, LR=0.1000\n",
            "| Epoch [ 34/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [ 34/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.961%\n",
            "| Epoch [ 34/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #35, LR=0.1000\n",
            "| Epoch [ 35/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 35/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.742%\n",
            "| Epoch [ 35/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #36, LR=0.1000\n",
            "| Epoch [ 36/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 36/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 36/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #37, LR=0.1000\n",
            "| Epoch [ 37/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [ 37/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [ 37/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #38, LR=0.1000\n",
            "| Epoch [ 38/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 18.750%\n",
            "| Epoch [ 38/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 11.328%\n",
            "| Epoch [ 38/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #39, LR=0.1000\n",
            "| Epoch [ 39/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [ 39/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.766%\n",
            "| Epoch [ 39/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #40, LR=0.1000\n",
            "| Epoch [ 40/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 40/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 11.523%\n",
            "| Epoch [ 40/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #41, LR=0.1000\n",
            "| Epoch [ 41/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 12.500%\n",
            "| Epoch [ 41/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.961%\n",
            "| Epoch [ 41/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #42, LR=0.1000\n",
            "| Epoch [ 42/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 42/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.938%\n",
            "| Epoch [ 42/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #43, LR=0.1000\n",
            "| Epoch [ 43/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [ 43/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.570%\n",
            "| Epoch [ 43/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #44, LR=0.1000\n",
            "| Epoch [ 44/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 44/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.766%\n",
            "| Epoch [ 44/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #45, LR=0.1000\n",
            "| Epoch [ 45/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 6.250%\n",
            "| Epoch [ 45/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 9.961%\n",
            "| Epoch [ 45/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.879%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #46, LR=0.1000\n",
            "| Epoch [ 46/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 0.000%\n",
            "| Epoch [ 46/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.352%\n",
            "| Epoch [ 46/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #47, LR=0.1000\n",
            "| Epoch [ 47/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 3.125%\n",
            "| Epoch [ 47/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 8.984%\n",
            "| Epoch [ 47/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #48, LR=0.1000\n",
            "| Epoch [ 48/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 15.625%\n",
            "| Epoch [ 48/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 11.523%\n",
            "| Epoch [ 48/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 10.081%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: nan Acc@1: 10.00%\n",
            "\n",
            "=> Training Epoch #49, LR=0.1000\n",
            "| Epoch [ 49/ 50] Iter[  1/ 32]\t\tLoss: nan Acc@1: 9.375%\n",
            "| Epoch [ 49/ 50] Iter[ 16/ 32]\t\tLoss: nan Acc@1: 10.156%\n",
            "| Epoch [ 49/ 50] Iter[ 31/ 32]\t\tLoss: nan Acc@1: 9.980%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: nan Acc@1: 10.00%\n"
          ]
        }
      ],
      "source": [
        "# Training not pretrained model\n",
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    train(epoch,alexnet_untrained,train_loader_alexnet)\n",
        "    acc =test(epoch,alexnet_untrained,valid_loader_alexnet)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        alexnet_untrained_save.load_state_dict(alexnet_untrained.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training pretrained model\n",
        "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "# False if we want to fine the whole model and True if only the last layer\n",
        "feature_extract = True \n",
        "\n",
        "if feature_extract:\n",
        "    for param in alexnet_pretrained.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Reshaping the last layer so output=num_classes\n",
        "alexnet_pretrained.classifier[6] = nn.Linear(4096,num_class).to(device)\n",
        "alexnet_pretrained_save.classifier[6] = nn.Linear(4096,num_class).to(device)\n",
        "\n",
        "\n",
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    train(epoch,alexnet_pretrained,train_loader_alexnet)\n",
        "    acc =test(epoch,alexnet_pretrained,valid_loader_alexnet)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        alexnet_pretrained_save.load_state_dict(alexnet_pretrained.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3b4232-3569-4287-bf8f-9efea44c5870",
        "id": "8CQF-pwGTafK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.1000\n",
            "| Epoch [  0/ 50] Iter[  1/ 32]\t\tLoss: 2.3654 Acc@1: 6.250%\n",
            "| Epoch [  0/ 50] Iter[ 16/ 32]\t\tLoss: 37.6024 Acc@1: 20.312%\n",
            "| Epoch [  0/ 50] Iter[ 31/ 32]\t\tLoss: 39.1715 Acc@1: 29.940%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 38.5825 Acc@1: 48.55%\n",
            "| Saving Best model...\t\t\tTop1 = 48.55%\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/ 50] Iter[  1/ 32]\t\tLoss: 25.2555 Acc@1: 56.250%\n",
            "| Epoch [  1/ 50] Iter[ 16/ 32]\t\tLoss: 30.9274 Acc@1: 48.438%\n",
            "| Epoch [  1/ 50] Iter[ 31/ 32]\t\tLoss: 22.1905 Acc@1: 42.944%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 0.0000 Acc@1: 54.35%\n",
            "| Saving Best model...\t\t\tTop1 = 54.35%\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/ 50] Iter[  1/ 32]\t\tLoss: 16.6916 Acc@1: 59.375%\n",
            "| Epoch [  2/ 50] Iter[ 16/ 32]\t\tLoss: 12.2249 Acc@1: 54.492%\n",
            "| Epoch [  2/ 50] Iter[ 31/ 32]\t\tLoss: 15.2179 Acc@1: 53.427%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 0.0000 Acc@1: 57.20%\n",
            "| Saving Best model...\t\t\tTop1 = 57.20%\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/ 50] Iter[  1/ 32]\t\tLoss: 14.3710 Acc@1: 62.500%\n",
            "| Epoch [  3/ 50] Iter[ 16/ 32]\t\tLoss: 22.2824 Acc@1: 54.297%\n",
            "| Epoch [  3/ 50] Iter[ 31/ 32]\t\tLoss: 21.5376 Acc@1: 50.706%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 37.0862 Acc@1: 53.25%\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/ 50] Iter[  1/ 32]\t\tLoss: 30.9213 Acc@1: 53.125%\n",
            "| Epoch [  4/ 50] Iter[ 16/ 32]\t\tLoss: 23.1389 Acc@1: 46.094%\n",
            "| Epoch [  4/ 50] Iter[ 31/ 32]\t\tLoss: 32.7603 Acc@1: 46.069%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 18.2542 Acc@1: 53.75%\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/ 50] Iter[  1/ 32]\t\tLoss: 33.8299 Acc@1: 40.625%\n",
            "| Epoch [  5/ 50] Iter[ 16/ 32]\t\tLoss: 21.5100 Acc@1: 52.344%\n",
            "| Epoch [  5/ 50] Iter[ 31/ 32]\t\tLoss: 31.7527 Acc@1: 52.117%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 7.7595 Acc@1: 57.90%\n",
            "| Saving Best model...\t\t\tTop1 = 57.90%\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/ 50] Iter[  1/ 32]\t\tLoss: 19.5894 Acc@1: 50.000%\n",
            "| Epoch [  6/ 50] Iter[ 16/ 32]\t\tLoss: 18.0008 Acc@1: 53.711%\n",
            "| Epoch [  6/ 50] Iter[ 31/ 32]\t\tLoss: 24.3885 Acc@1: 52.218%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 56.5950 Acc@1: 53.05%\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/ 50] Iter[  1/ 32]\t\tLoss: 36.1184 Acc@1: 31.250%\n",
            "| Epoch [  7/ 50] Iter[ 16/ 32]\t\tLoss: 27.4529 Acc@1: 54.492%\n",
            "| Epoch [  7/ 50] Iter[ 31/ 32]\t\tLoss: 19.3021 Acc@1: 54.940%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 4.0342 Acc@1: 62.60%\n",
            "| Saving Best model...\t\t\tTop1 = 62.60%\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/ 50] Iter[  1/ 32]\t\tLoss: 10.8939 Acc@1: 68.750%\n",
            "| Epoch [  8/ 50] Iter[ 16/ 32]\t\tLoss: 16.3071 Acc@1: 58.008%\n",
            "| Epoch [  8/ 50] Iter[ 31/ 32]\t\tLoss: 14.9148 Acc@1: 57.560%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 0.0001 Acc@1: 57.60%\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/ 50] Iter[  1/ 32]\t\tLoss: 27.4085 Acc@1: 43.750%\n",
            "| Epoch [  9/ 50] Iter[ 16/ 32]\t\tLoss: 11.5855 Acc@1: 52.930%\n",
            "| Epoch [  9/ 50] Iter[ 31/ 32]\t\tLoss: 14.3011 Acc@1: 55.746%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 44.0965 Acc@1: 58.25%\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/ 50] Iter[  1/ 32]\t\tLoss: 12.5048 Acc@1: 71.875%\n",
            "| Epoch [ 10/ 50] Iter[ 16/ 32]\t\tLoss: 14.2796 Acc@1: 58.594%\n",
            "| Epoch [ 10/ 50] Iter[ 31/ 32]\t\tLoss: 23.2925 Acc@1: 57.863%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 8.2557 Acc@1: 57.70%\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/ 50] Iter[  1/ 32]\t\tLoss: 20.7307 Acc@1: 43.750%\n",
            "| Epoch [ 11/ 50] Iter[ 16/ 32]\t\tLoss: 33.8085 Acc@1: 56.641%\n",
            "| Epoch [ 11/ 50] Iter[ 31/ 32]\t\tLoss: 18.3500 Acc@1: 54.839%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 11.4175 Acc@1: 60.30%\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/ 50] Iter[  1/ 32]\t\tLoss: 14.1451 Acc@1: 68.750%\n",
            "| Epoch [ 12/ 50] Iter[ 16/ 32]\t\tLoss: 19.9783 Acc@1: 59.375%\n",
            "| Epoch [ 12/ 50] Iter[ 31/ 32]\t\tLoss: 21.7127 Acc@1: 57.661%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 0.0076 Acc@1: 53.25%\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/ 50] Iter[  1/ 32]\t\tLoss: 22.5796 Acc@1: 59.375%\n",
            "| Epoch [ 13/ 50] Iter[ 16/ 32]\t\tLoss: 17.1428 Acc@1: 58.008%\n",
            "| Epoch [ 13/ 50] Iter[ 31/ 32]\t\tLoss: 29.9066 Acc@1: 58.569%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 50.9575 Acc@1: 53.95%\n",
            "\n",
            "=> Training Epoch #14, LR=0.1000\n",
            "| Epoch [ 14/ 50] Iter[  1/ 32]\t\tLoss: 26.6641 Acc@1: 53.125%\n",
            "| Epoch [ 14/ 50] Iter[ 16/ 32]\t\tLoss: 13.7754 Acc@1: 61.719%\n",
            "| Epoch [ 14/ 50] Iter[ 31/ 32]\t\tLoss: 13.9921 Acc@1: 57.762%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 28.7246 Acc@1: 61.50%\n",
            "\n",
            "=> Training Epoch #15, LR=0.1000\n",
            "| Epoch [ 15/ 50] Iter[  1/ 32]\t\tLoss: 21.6818 Acc@1: 56.250%\n",
            "| Epoch [ 15/ 50] Iter[ 16/ 32]\t\tLoss: 8.0756 Acc@1: 59.570%\n",
            "| Epoch [ 15/ 50] Iter[ 31/ 32]\t\tLoss: 30.0435 Acc@1: 57.661%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 2.2843 Acc@1: 54.30%\n",
            "\n",
            "=> Training Epoch #16, LR=0.1000\n",
            "| Epoch [ 16/ 50] Iter[  1/ 32]\t\tLoss: 31.7174 Acc@1: 46.875%\n",
            "| Epoch [ 16/ 50] Iter[ 16/ 32]\t\tLoss: 13.4052 Acc@1: 59.961%\n",
            "| Epoch [ 16/ 50] Iter[ 31/ 32]\t\tLoss: 18.3360 Acc@1: 58.669%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 3.5710 Acc@1: 61.40%\n",
            "\n",
            "=> Training Epoch #17, LR=0.1000\n",
            "| Epoch [ 17/ 50] Iter[  1/ 32]\t\tLoss: 12.2621 Acc@1: 75.000%\n",
            "| Epoch [ 17/ 50] Iter[ 16/ 32]\t\tLoss: 17.0017 Acc@1: 64.648%\n",
            "| Epoch [ 17/ 50] Iter[ 31/ 32]\t\tLoss: 15.0889 Acc@1: 61.996%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 4.8619 Acc@1: 64.20%\n",
            "| Saving Best model...\t\t\tTop1 = 64.20%\n",
            "\n",
            "=> Training Epoch #18, LR=0.1000\n",
            "| Epoch [ 18/ 50] Iter[  1/ 32]\t\tLoss: 8.8582 Acc@1: 71.875%\n",
            "| Epoch [ 18/ 50] Iter[ 16/ 32]\t\tLoss: 22.2301 Acc@1: 66.992%\n",
            "| Epoch [ 18/ 50] Iter[ 31/ 32]\t\tLoss: 8.2662 Acc@1: 64.819%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 1.8249 Acc@1: 64.85%\n",
            "| Saving Best model...\t\t\tTop1 = 64.85%\n",
            "\n",
            "=> Training Epoch #19, LR=0.1000\n",
            "| Epoch [ 19/ 50] Iter[  1/ 32]\t\tLoss: 23.3343 Acc@1: 50.000%\n",
            "| Epoch [ 19/ 50] Iter[ 16/ 32]\t\tLoss: 14.8731 Acc@1: 60.156%\n",
            "| Epoch [ 19/ 50] Iter[ 31/ 32]\t\tLoss: 33.1709 Acc@1: 58.770%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 22.4010 Acc@1: 50.60%\n",
            "\n",
            "=> Training Epoch #20, LR=0.1000\n",
            "| Epoch [ 20/ 50] Iter[  1/ 32]\t\tLoss: 25.3180 Acc@1: 65.625%\n",
            "| Epoch [ 20/ 50] Iter[ 16/ 32]\t\tLoss: 14.0935 Acc@1: 61.328%\n",
            "| Epoch [ 20/ 50] Iter[ 31/ 32]\t\tLoss: 5.8760 Acc@1: 63.609%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 0.0000 Acc@1: 56.70%\n",
            "\n",
            "=> Training Epoch #21, LR=0.1000\n",
            "| Epoch [ 21/ 50] Iter[  1/ 32]\t\tLoss: 16.5658 Acc@1: 59.375%\n",
            "| Epoch [ 21/ 50] Iter[ 16/ 32]\t\tLoss: 20.4693 Acc@1: 60.156%\n",
            "| Epoch [ 21/ 50] Iter[ 31/ 32]\t\tLoss: 20.7800 Acc@1: 59.577%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 10.7706 Acc@1: 64.20%\n",
            "\n",
            "=> Training Epoch #22, LR=0.1000\n",
            "| Epoch [ 22/ 50] Iter[  1/ 32]\t\tLoss: 28.9070 Acc@1: 50.000%\n",
            "| Epoch [ 22/ 50] Iter[ 16/ 32]\t\tLoss: 11.7900 Acc@1: 58.398%\n",
            "| Epoch [ 22/ 50] Iter[ 31/ 32]\t\tLoss: 14.2185 Acc@1: 60.383%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 21.2984 Acc@1: 61.95%\n",
            "\n",
            "=> Training Epoch #23, LR=0.1000\n",
            "| Epoch [ 23/ 50] Iter[  1/ 32]\t\tLoss: 21.7431 Acc@1: 50.000%\n",
            "| Epoch [ 23/ 50] Iter[ 16/ 32]\t\tLoss: 20.4289 Acc@1: 57.812%\n",
            "| Epoch [ 23/ 50] Iter[ 31/ 32]\t\tLoss: 19.7168 Acc@1: 59.677%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 24.1079 Acc@1: 57.80%\n",
            "\n",
            "=> Training Epoch #24, LR=0.1000\n",
            "| Epoch [ 24/ 50] Iter[  1/ 32]\t\tLoss: 23.2044 Acc@1: 56.250%\n",
            "| Epoch [ 24/ 50] Iter[ 16/ 32]\t\tLoss: 19.3296 Acc@1: 59.180%\n",
            "| Epoch [ 24/ 50] Iter[ 31/ 32]\t\tLoss: 20.6683 Acc@1: 57.964%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 72.2816 Acc@1: 57.50%\n",
            "\n",
            "=> Training Epoch #25, LR=0.1000\n",
            "| Epoch [ 25/ 50] Iter[  1/ 32]\t\tLoss: 16.7145 Acc@1: 62.500%\n",
            "| Epoch [ 25/ 50] Iter[ 16/ 32]\t\tLoss: 22.5772 Acc@1: 60.547%\n",
            "| Epoch [ 25/ 50] Iter[ 31/ 32]\t\tLoss: 12.1193 Acc@1: 59.980%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 6.7760 Acc@1: 54.00%\n",
            "\n",
            "=> Training Epoch #26, LR=0.1000\n",
            "| Epoch [ 26/ 50] Iter[  1/ 32]\t\tLoss: 13.0286 Acc@1: 59.375%\n",
            "| Epoch [ 26/ 50] Iter[ 16/ 32]\t\tLoss: 19.1140 Acc@1: 60.547%\n",
            "| Epoch [ 26/ 50] Iter[ 31/ 32]\t\tLoss: 13.9137 Acc@1: 60.484%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 10.3346 Acc@1: 59.15%\n",
            "\n",
            "=> Training Epoch #27, LR=0.1000\n",
            "| Epoch [ 27/ 50] Iter[  1/ 32]\t\tLoss: 6.9046 Acc@1: 62.500%\n",
            "| Epoch [ 27/ 50] Iter[ 16/ 32]\t\tLoss: 15.8434 Acc@1: 63.281%\n",
            "| Epoch [ 27/ 50] Iter[ 31/ 32]\t\tLoss: 17.0535 Acc@1: 62.198%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 28.1708 Acc@1: 62.55%\n",
            "\n",
            "=> Training Epoch #28, LR=0.1000\n",
            "| Epoch [ 28/ 50] Iter[  1/ 32]\t\tLoss: 12.5318 Acc@1: 65.625%\n",
            "| Epoch [ 28/ 50] Iter[ 16/ 32]\t\tLoss: 13.7184 Acc@1: 62.695%\n",
            "| Epoch [ 28/ 50] Iter[ 31/ 32]\t\tLoss: 15.6993 Acc@1: 61.593%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 12.6547 Acc@1: 63.00%\n",
            "\n",
            "=> Training Epoch #29, LR=0.1000\n",
            "| Epoch [ 29/ 50] Iter[  1/ 32]\t\tLoss: 16.7125 Acc@1: 59.375%\n",
            "| Epoch [ 29/ 50] Iter[ 16/ 32]\t\tLoss: 16.6428 Acc@1: 66.016%\n",
            "| Epoch [ 29/ 50] Iter[ 31/ 32]\t\tLoss: 19.0214 Acc@1: 63.206%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 6.4367 Acc@1: 65.20%\n",
            "| Saving Best model...\t\t\tTop1 = 65.20%\n",
            "\n",
            "=> Training Epoch #30, LR=0.1000\n",
            "| Epoch [ 30/ 50] Iter[  1/ 32]\t\tLoss: 14.9532 Acc@1: 65.625%\n",
            "| Epoch [ 30/ 50] Iter[ 16/ 32]\t\tLoss: 24.3249 Acc@1: 64.062%\n",
            "| Epoch [ 30/ 50] Iter[ 31/ 32]\t\tLoss: 19.3408 Acc@1: 63.508%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 16.4822 Acc@1: 65.40%\n",
            "| Saving Best model...\t\t\tTop1 = 65.40%\n",
            "\n",
            "=> Training Epoch #31, LR=0.1000\n",
            "| Epoch [ 31/ 50] Iter[  1/ 32]\t\tLoss: 7.3488 Acc@1: 68.750%\n",
            "| Epoch [ 31/ 50] Iter[ 16/ 32]\t\tLoss: 6.1551 Acc@1: 66.016%\n",
            "| Epoch [ 31/ 50] Iter[ 31/ 32]\t\tLoss: 10.8195 Acc@1: 65.524%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 27.1127 Acc@1: 54.95%\n",
            "\n",
            "=> Training Epoch #32, LR=0.1000\n",
            "| Epoch [ 32/ 50] Iter[  1/ 32]\t\tLoss: 15.6554 Acc@1: 65.625%\n",
            "| Epoch [ 32/ 50] Iter[ 16/ 32]\t\tLoss: 9.9559 Acc@1: 68.555%\n",
            "| Epoch [ 32/ 50] Iter[ 31/ 32]\t\tLoss: 12.0314 Acc@1: 65.726%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 5.4853 Acc@1: 60.80%\n",
            "\n",
            "=> Training Epoch #33, LR=0.1000\n",
            "| Epoch [ 33/ 50] Iter[  1/ 32]\t\tLoss: 4.7522 Acc@1: 81.250%\n",
            "| Epoch [ 33/ 50] Iter[ 16/ 32]\t\tLoss: 14.4599 Acc@1: 69.141%\n",
            "| Epoch [ 33/ 50] Iter[ 31/ 32]\t\tLoss: 17.7689 Acc@1: 64.919%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 15.8745 Acc@1: 62.20%\n",
            "\n",
            "=> Training Epoch #34, LR=0.1000\n",
            "| Epoch [ 34/ 50] Iter[  1/ 32]\t\tLoss: 11.5881 Acc@1: 56.250%\n",
            "| Epoch [ 34/ 50] Iter[ 16/ 32]\t\tLoss: 15.5931 Acc@1: 68.555%\n",
            "| Epoch [ 34/ 50] Iter[ 31/ 32]\t\tLoss: 22.1459 Acc@1: 63.609%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 4.9920 Acc@1: 56.40%\n",
            "\n",
            "=> Training Epoch #35, LR=0.1000\n",
            "| Epoch [ 35/ 50] Iter[  1/ 32]\t\tLoss: 21.0409 Acc@1: 56.250%\n",
            "| Epoch [ 35/ 50] Iter[ 16/ 32]\t\tLoss: 19.1947 Acc@1: 59.961%\n",
            "| Epoch [ 35/ 50] Iter[ 31/ 32]\t\tLoss: 13.1095 Acc@1: 58.165%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 7.5769 Acc@1: 54.50%\n",
            "\n",
            "=> Training Epoch #36, LR=0.1000\n",
            "| Epoch [ 36/ 50] Iter[  1/ 32]\t\tLoss: 12.7398 Acc@1: 62.500%\n",
            "| Epoch [ 36/ 50] Iter[ 16/ 32]\t\tLoss: 16.2807 Acc@1: 61.719%\n",
            "| Epoch [ 36/ 50] Iter[ 31/ 32]\t\tLoss: 14.0008 Acc@1: 58.468%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.9558 Acc@1: 58.65%\n",
            "\n",
            "=> Training Epoch #37, LR=0.1000\n",
            "| Epoch [ 37/ 50] Iter[  1/ 32]\t\tLoss: 22.3717 Acc@1: 53.125%\n",
            "| Epoch [ 37/ 50] Iter[ 16/ 32]\t\tLoss: 18.4196 Acc@1: 66.211%\n",
            "| Epoch [ 37/ 50] Iter[ 31/ 32]\t\tLoss: 23.1332 Acc@1: 64.113%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 0.0000 Acc@1: 53.05%\n",
            "\n",
            "=> Training Epoch #38, LR=0.1000\n",
            "| Epoch [ 38/ 50] Iter[  1/ 32]\t\tLoss: 21.7449 Acc@1: 50.000%\n",
            "| Epoch [ 38/ 50] Iter[ 16/ 32]\t\tLoss: 11.4955 Acc@1: 64.258%\n",
            "| Epoch [ 38/ 50] Iter[ 31/ 32]\t\tLoss: 24.6805 Acc@1: 63.407%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 30.0785 Acc@1: 53.75%\n",
            "\n",
            "=> Training Epoch #39, LR=0.1000\n",
            "| Epoch [ 39/ 50] Iter[  1/ 32]\t\tLoss: 19.0850 Acc@1: 65.625%\n",
            "| Epoch [ 39/ 50] Iter[ 16/ 32]\t\tLoss: 10.6262 Acc@1: 63.086%\n",
            "| Epoch [ 39/ 50] Iter[ 31/ 32]\t\tLoss: 16.1214 Acc@1: 61.794%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 44.4687 Acc@1: 58.00%\n",
            "\n",
            "=> Training Epoch #40, LR=0.1000\n",
            "| Epoch [ 40/ 50] Iter[  1/ 32]\t\tLoss: 22.9791 Acc@1: 59.375%\n",
            "| Epoch [ 40/ 50] Iter[ 16/ 32]\t\tLoss: 15.9660 Acc@1: 67.578%\n",
            "| Epoch [ 40/ 50] Iter[ 31/ 32]\t\tLoss: 17.6846 Acc@1: 66.734%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 6.5424 Acc@1: 58.30%\n",
            "\n",
            "=> Training Epoch #41, LR=0.1000\n",
            "| Epoch [ 41/ 50] Iter[  1/ 32]\t\tLoss: 16.7613 Acc@1: 59.375%\n",
            "| Epoch [ 41/ 50] Iter[ 16/ 32]\t\tLoss: 14.1861 Acc@1: 66.602%\n",
            "| Epoch [ 41/ 50] Iter[ 31/ 32]\t\tLoss: 26.5438 Acc@1: 64.315%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 3.6848 Acc@1: 57.90%\n",
            "\n",
            "=> Training Epoch #42, LR=0.1000\n",
            "| Epoch [ 42/ 50] Iter[  1/ 32]\t\tLoss: 11.0783 Acc@1: 68.750%\n",
            "| Epoch [ 42/ 50] Iter[ 16/ 32]\t\tLoss: 4.8396 Acc@1: 70.117%\n",
            "| Epoch [ 42/ 50] Iter[ 31/ 32]\t\tLoss: 19.8605 Acc@1: 66.230%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 6.1012 Acc@1: 60.20%\n",
            "\n",
            "=> Training Epoch #43, LR=0.1000\n",
            "| Epoch [ 43/ 50] Iter[  1/ 32]\t\tLoss: 11.6584 Acc@1: 81.250%\n",
            "| Epoch [ 43/ 50] Iter[ 16/ 32]\t\tLoss: 2.6552 Acc@1: 67.383%\n",
            "| Epoch [ 43/ 50] Iter[ 31/ 32]\t\tLoss: 15.9351 Acc@1: 65.927%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 25.1062 Acc@1: 57.50%\n",
            "\n",
            "=> Training Epoch #44, LR=0.1000\n",
            "| Epoch [ 44/ 50] Iter[  1/ 32]\t\tLoss: 18.2542 Acc@1: 59.375%\n",
            "| Epoch [ 44/ 50] Iter[ 16/ 32]\t\tLoss: 8.9498 Acc@1: 62.109%\n",
            "| Epoch [ 44/ 50] Iter[ 31/ 32]\t\tLoss: 13.5500 Acc@1: 65.020%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 0.0000 Acc@1: 59.05%\n",
            "\n",
            "=> Training Epoch #45, LR=0.1000\n",
            "| Epoch [ 45/ 50] Iter[  1/ 32]\t\tLoss: 18.6734 Acc@1: 62.500%\n",
            "| Epoch [ 45/ 50] Iter[ 16/ 32]\t\tLoss: 13.5104 Acc@1: 57.422%\n",
            "| Epoch [ 45/ 50] Iter[ 31/ 32]\t\tLoss: 26.7677 Acc@1: 58.871%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 11.3448 Acc@1: 62.30%\n",
            "\n",
            "=> Training Epoch #46, LR=0.1000\n",
            "| Epoch [ 46/ 50] Iter[  1/ 32]\t\tLoss: 8.8172 Acc@1: 68.750%\n",
            "| Epoch [ 46/ 50] Iter[ 16/ 32]\t\tLoss: 18.7458 Acc@1: 62.695%\n",
            "| Epoch [ 46/ 50] Iter[ 31/ 32]\t\tLoss: 15.4271 Acc@1: 63.004%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 6.1254 Acc@1: 63.75%\n",
            "\n",
            "=> Training Epoch #47, LR=0.1000\n",
            "| Epoch [ 47/ 50] Iter[  1/ 32]\t\tLoss: 12.2685 Acc@1: 71.875%\n",
            "| Epoch [ 47/ 50] Iter[ 16/ 32]\t\tLoss: 18.5589 Acc@1: 66.016%\n",
            "| Epoch [ 47/ 50] Iter[ 31/ 32]\t\tLoss: 20.8603 Acc@1: 64.214%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 13.5525 Acc@1: 61.75%\n",
            "\n",
            "=> Training Epoch #48, LR=0.1000\n",
            "| Epoch [ 48/ 50] Iter[  1/ 32]\t\tLoss: 10.7549 Acc@1: 62.500%\n",
            "| Epoch [ 48/ 50] Iter[ 16/ 32]\t\tLoss: 10.0990 Acc@1: 68.164%\n",
            "| Epoch [ 48/ 50] Iter[ 31/ 32]\t\tLoss: 20.1284 Acc@1: 65.524%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 21.6226 Acc@1: 57.90%\n",
            "\n",
            "=> Training Epoch #49, LR=0.1000\n",
            "| Epoch [ 49/ 50] Iter[  1/ 32]\t\tLoss: 18.7786 Acc@1: 59.375%\n",
            "| Epoch [ 49/ 50] Iter[ 16/ 32]\t\tLoss: 20.1265 Acc@1: 62.891%\n",
            "| Epoch [ 49/ 50] Iter[ 31/ 32]\t\tLoss: 15.9824 Acc@1: 63.407%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 5.9127 Acc@1: 61.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EnS0ihhkqp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d15790b-07fe-456d-d6b5-64da5ca13a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST \t\t\tLoss: 27.0689 Acc@1: 61.58%\n",
            "\n",
            "| TEST \t\t\tLoss: nan Acc@1: 10.00%\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "\n",
        "# Pretrained\n",
        "# Mauvais resultats car les catégories du resnet18 entraine sont differents de ceux qu'on a\n",
        "predicted_concat = test_final(alexnet_pretrained,test_loader_alexnet)\n",
        "\n",
        "# Not pretrained\n",
        "predicted_concat = test_final(alexnet_untrained,test_loader_alexnet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBQ8y3ZaZcMJ"
      },
      "source": [
        "## Q3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set the database in 2D\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "for data in train_loader.dataset :\n",
        "    X_train.append(data[0])\n",
        "    Y_train.append(data[1])\n",
        "\n",
        "for data in test_loader.dataset :\n",
        "    X_test.append(data[0])\n",
        "    Y_test.append(data[1])\n",
        "\n",
        "X_train = torch.cat(X_train)\n",
        "X_train = X_train.reshape(len(train_loader.dataset),-1)\n",
        "\n",
        "X_test = torch.cat(X_test)\n",
        "X_test = X_test.reshape(len(test_loader.dataset),-1)"
      ],
      "metadata": {
        "id": "vYXvlqUVeSdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMctJFVKZjv6"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipk_HGv7ZZRC"
      },
      "outputs": [],
      "source": [
        "# SVM (Default OneVersusRest)\n",
        "from sklearn import svm\n",
        "\n",
        "parameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              'C': np.logspace(-2,1,5),\n",
        "              'degree': [k for k in range(3,6)],\n",
        "              'decision_function_shape': ['ovo', 'ovr'],\n",
        "              }\n",
        "\n",
        "svc = svm.SVC(random_state=seed)\n",
        "svc = GridSearchCV(svc, parameters)\n",
        "\n",
        "svc.fit(X_train, Y_train)\n",
        "\n",
        "acc = svc.score(X_test, Y_test)\n",
        "\n",
        "print(\"\\n| TEST on SVM \\t\\t\\tAccuracy: %.2f%%\" %(acc*100))\n",
        "print(\"Parameters are\")\n",
        "print(svc.get_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnfK1RH1Zlre"
      },
      "source": [
        "### RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvcPq-KXZpxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e928f1-5f93-4ed1-b065-45b90ec5c6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST on a single tree \t\t\tAccuracy: 18.43%\n"
          ]
        }
      ],
      "source": [
        "# Single tree\n",
        "clf = tree.DecisionTreeClassifier(criterion = 'gini', random_state = seed) # Arbre de décision\n",
        "clf = clf.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "# # To see the tree\n",
        "# print(\"L'arbre de décision est\")\n",
        "# plt.figure(figsize=(25,20))\n",
        "# tree.plot_tree(clf,\n",
        "#                filled=True, rounded=True,\n",
        "#                class_names = classes)\n",
        "# print(\" \")\n",
        "\n",
        "\n",
        "# # To cleary see the tree with all nodes :\n",
        "# import graphviz\n",
        "\n",
        "# dot_data = tree.export_graphviz(clf, out_file=None) \n",
        "# graph = graphviz.Source(dot_data) \n",
        "\n",
        "# print(\"L'arbre de décision est\")\n",
        "# dot_data = tree.export_graphviz(clf, out_file=None, \n",
        "#                     #  feature_names=X_train.columns.values,  \n",
        "#                      class_names=classes,  \n",
        "#                      filled=True, rounded=True,  \n",
        "#                      special_characters=True)  \n",
        "# graph = graphviz.Source(dot_data)  \n",
        "# display(graph)\n",
        "\n",
        "# Evaluation\n",
        "acc = clf.score(X_test,Y_test)\n",
        "print(\"\\n| TEST on a single tree \\t\\t\\tAccuracy: %.2f%%\" %(acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On a randomforest\n",
        "clf = ensemble.RandomForestClassifier(criterion = 'gini', random_state = seed)\n",
        "clf = clf.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluation\n",
        "acc = clf.score(X_test,Y_test)\n",
        "print(\"\\n| TEST on a random forest \\t\\t\\tAccuracy: %.2f%%\" %(acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Os1JExbf-PT",
        "outputId": "e47ae303-8f6e-4906-ee1f-9871c9345b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST on a random forest \t\t\tAccuracy: 30.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5:\n",
        "\n",
        "ChatGPT:\n",
        "FixMatch est un algorithme de deep learning qui a été présenté par Google Research en 2020. Il est utilisé pour l'apprentissage supervisé avec des données étiquetées de faible qualité. Il combine deux techniques d'apprentissage supervisé : l'apprentissage semi-supervisé et l'apprentissage supervisé fortement étiqueté.\n",
        "\n",
        "L'algorithme utilise d'abord un modèle pré-entraîné pour générer des pseudo-étiquettes pour des exemples non étiquetés, qui sont ensuite utilisés pour entraîner un modèle supervisé avec une régularisation supplémentaire pour améliorer l'apprentissage.\n",
        "\n",
        "Il utilise également des \"données de contrôle\" qui sont des exemples fortement étiquetés qui ont une forte confiance pour les étiquettes. Ces données de contrôle sont utilisées pour fournir un ajustement supplémentaire pour le modèle supervisé.\n",
        "\n",
        "En utilisant ces deux techniques, FixMatch permet d'utiliser efficacement des données de faible qualité pour entraîner des modèles de reconnaissance d'images avec des performances élevées. Il a été montré pour être très efficace pour les tâches de classification d'image en utilisant peu de données étiquetées, avec des résultats comparables à ceux obtenus avec des jeux de données étiquetées beaucoup plus importants.\n"
      ],
      "metadata": {
        "id": "r9sbQIGk2tae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 : Explications de FixMatch\n",
        "\n",
        "FixMatch est une méthode d'apprentissage qui a été pensé par Google Reasearch en 2020. Il a été pensé car en DL il est nécessaire d'avoir une grande base de données pour pouvoir effectuer l'apprentissage or créer cette base de données (et surtout poser des labels sur chaque élément) est très coûteux.\n",
        "\n",
        "FixMatch a été originellement pensé sur la classification d'images à partir de réseaux de neurones tel que ResNet. Imaginons qu'on possède des images avec un label et d'autres non. Le réseau de neurones traditionnel ne peut apprendre que sur les images avec un label or FixMatch est une méthode qui vise également à apprendre sur les données sans label d'où le fait que c'est une méthode d'apprentissage semi-supervisé car il apprend également sur des données non annotées. \n",
        "\n",
        "FixMatch utilise la data augmentation, technique utilisée pour augmenter la taille d'une base de données en utilisant des propriétés sur les objets sur lesquels on travaille. Par exemple si on souhaite classifier des images d'animaux, alors effectuer une symétrie horizontale, une faible rotation ou une faible translation ne change que très peu l'image et le réseau de neurones doit être capable de toujours donner le même animal en sortie peu importe le changement sur l'image. On distingue alors deux types de data augmentation :\n",
        "- **Weak augmentation :** On retrouve des des techniques telle que la symétrie horizontale, la translation, le zoom/dezoom. Ces techniques sont catégorisées comme weak car on ne change qu'assez peu l'image et qu'un bon réseau de neurones entraîné serait capable de classifier l'image avec une accuracy qui varierait assez peu. Dans FixMatch, ils ont choisi de n'utiliser que la symétrie horizontale et des translations (qui ne vont que jusqu'à 12.5% de la taille initiale)\n",
        "- **Strong augmentation :** Les techniques utilisées effectuent des changements plus drastiques sur l'image et il est bien plus compliqué de classifier l'image, parmi ces techniques on retrouve des changements sur la couleur, le contraste, la luminosité, etc... Parmi les techniques de strong augmentation, FixMatch utilise le CutOut qui consiste à enlever un carré (de taille raisonnable) de l'image. FixMatch effectue également un AutoAugment qui consiste à choisir un certain nombre de transformations parmi un set prédéfini, de choisir une magnitude qui définit la \"puissance\" de la transformation et d'effectuer ces transformations sur l'image. FixMatch utilise le RandAugment ou le CTAugment pour effectuer l'AutoAugment\n",
        "\n",
        "L'apprentissage de FixMatch s'effectue en plusieurs étapes :\n",
        "- Il faut tout d'abord créer la base de données avec des données avec label et des données sans label avec μ fois plus de données sans label que données avec label (il a été prouvé que plus μ est grand, meilleurs sont les résultats). Il faut également choisir une architecture pour les réseaux de neurones.\n",
        "- On commence à travailler uniquement sur les données ayant un label, la fonction de perte à minimiser est *lₛ* qu'on calcule à l'aide de l'entropie croisée\n",
        "- Après avoir fait cette étape, on va effectuer une weak augmentation sur les données sans label. Et on va demander au réseau de neurones de classifier les images. Si la classification est donnée avec une certaine confiance supérieuse à un seuil (pour éviter d'avoir des pseudo label aberrant), on va poser un pseudo-label sur l'image en fonction de ce qu'a donné le réseau de neurones.\n",
        "- On va maintenant effectuer une strong augmentation sur les données avec un pseudo-label et demander au réseau de neurones de classifier les images qui ont subi cette augmentation. A partir du pseudo label et de la sortie, on détermine l'entropie croisée et la fonction de perte *lᵤ*  uniquement sur les données ayant un pseudo label\n",
        "- On calcule la fonction de perte totale *lₛ + λᵤlᵤ* avec *λᵤ* un paramètre de régularisation (qui sera fixé contrairement à d'autres algorithmes de SSL). A partir de cette fonction, on va remettre à jour les poids\n",
        "\n",
        "Le fait d'introduire un seuil pour créer le pseudo label fait qu'au début le réseau de neurones ne pourra mettre aucun pseudo label sur les images. Donc au début l'entraînement s'effectuera comme si on entrainait traditionnellement un réseau de neurones sur des des données avec label, puis petit à petit il va être capable de créer quelques pseudo label et ainsi les données sans label vont commencer à avoir de plus en plus d'impact sur les poids du réseau\n"
      ],
      "metadata": {
        "id": "qL_PbGDf6i7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6 : Implémentation de FixMatch\n",
        "\n",
        "On choisit un réseau de neurones WideResNet 28-2 comme dans le papier originel (https://arxiv.org/ftp/arxiv/papers/2001/2001.07685.pdf)\n",
        "\n",
        "On commence par définir tous les hyperparamètres qu'on devra utiliser (les valeurs seront prises à l'identique au papier originel)\n",
        "\n",
        "On va utiliser 100 images avec label par classe (on peut choisir une autre valeur mais il faudra reconstruire les données avec label qu'on a fait au tout début). On possède donc 1000 images avec label et 47000 images sans label"
      ],
      "metadata": {
        "id": "6FlCd27K8pQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "num_class = 10 #number of classes\n",
        "seed=111 #seed for the algorithm\n",
        "batch_size = 32\n",
        "num_train = 100 # number of labeled image by classe\n",
        "cutout = 16  # parameter for the cutout\n",
        "num_epochs = 100\n",
        "#Validation set size\n",
        "valid_size = 200\n",
        "\n",
        "threshold = 0.95 # Threshold to set pseudo label = tau\n",
        "lambda_u = 1 # scalar that decides how much both the unlabeled image loss contribute relative to the labeled loss\n",
        "mu = 7 # Relative size between labeled and unlabeled data\n",
        "lr = 0.03 # Learning rate\n",
        "bn_momentum = 0.9 # Used in the BatchNorm2D layer in WideResNet\n",
        "momentum = 0.9 # In SGD optimizer\n",
        "weight_decay = 5e-4 # In SGD optimizer\n",
        "nesterov = True # In SGD optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "torch.manual_seed(seed) # Set the seed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5sg0BWn99yd",
        "outputId": "8bb2b977-42ba-48b9-ec3c-8a026fab5952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd9d0fa4670>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On définit les différentes transformations qu'on fera subir à nos données (weak, strong augmentation), on construit les dataloader ainsi qu'une fonction **unlabeled_loader** qui permettra de créer 2 dataloader à partir dun set d'images: un pour le weak augmentation et l'autre pour le strong augmentation (les images sont dans un ordre aléatoire mais dans le même ordre pour les 2 dataloader ce qui nous permettra d'itérer sur chacun des dataloader à chaque étape de l'algorithme)"
      ],
      "metadata": {
        "id": "OLl3-zBrdPQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create shuffled dataloader with weak and strong augmentation\n",
        "def unlabeled_loader(x_unlabeled, y_unlabeled, seed = None) :\n",
        "    if seed==None :\n",
        "        permutation = np.random.permutation(len(x_unlabeled))\n",
        "    else :\n",
        "        permutation = np.random.RandomState(seed=seed).permutation(len(x_unlabeled))\n",
        "    x_unlabeled = x_unlabeled[permutation]\n",
        "    x_unlabeled = x_unlabeled[:len(x_train)*mu]\n",
        "    y_unlabeled = y_unlabeled[permutation]\n",
        "    y_unlabeled = y_unlabeled[:len(x_train)*mu]\n",
        "    unlabeled_loader_weak = torch.utils.data.DataLoader(\n",
        "        Dataset_sub_CIFAR(x_unlabeled, y_unlabeled, transform=transform_weak),\n",
        "        batch_size=batch_size*mu,shuffle=False, num_workers=2) \n",
        "    \n",
        "    unlabeled_loader_strong = torch.utils.data.DataLoader(\n",
        "        Dataset_sub_CIFAR(x_unlabeled, y_unlabeled, transform=transform_strong),\n",
        "        batch_size=batch_size*mu,shuffle=False, num_workers=2) \n",
        "    return unlabeled_loader_weak, unlabeled_loader_strong\n",
        "\n",
        "\n",
        "#Dataset loading\n",
        "CIFAR10_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=None, download=True)\n",
        "CIFAR10_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=None, download=True)\n",
        "np.random.seed(seed=seed)\n",
        "permuation=np.random.permutation(len(CIFAR10_train_dataset.targets))\n",
        "\n",
        "Original_train_data_x = (CIFAR10_train_dataset.data)\n",
        "Original_train_data_y = np.array(CIFAR10_train_dataset.targets)\n",
        "Original_train_data_x = Original_train_data_x[permuation]\n",
        "Original_train_data_y = Original_train_data_y[permuation]\n",
        "\n",
        "Original_test_data_x = CIFAR10_test_dataset.data\n",
        "Original_test_data_y = np.array(CIFAR10_test_dataset.targets)\n",
        "\n",
        "\n",
        "\n",
        "#Selection of 250 labeled images for training and 2000 for validation\n",
        "incr_class = torch.zeros(num_class)\n",
        "train_idx_dico = {} #labeled images index dictionnary\n",
        "\n",
        "for i in range(num_class):\n",
        "    train_idx_dico[str(i)] = []\n",
        "\n",
        "valid_idx = np.zeros(num_class * valid_size, dtype=np.int32) #validation images indexes (2000)\n",
        "incr_t = 0\n",
        "incr_v = 0\n",
        "incrtotal = 0\n",
        "\n",
        "for idx in range(len(Original_train_data_y)):\n",
        "    class_y = Original_train_data_y[idx]\n",
        "    incrtotal += 1\n",
        "\n",
        "    train_idx_dico[str(class_y)].append(idx)\n",
        "    incr_class[class_y] += 1 #count the number of image per class\n",
        "    incr_t += 1\n",
        "\n",
        "\n",
        "train_idx = np.zeros(num_class * num_train, dtype=np.int32) #train labeled images indexes (1000)\n",
        "list_train_id = []\n",
        "list_unalabel_id = []\n",
        "valid_idx = []\n",
        "unlabel_idx_dico = {}\n",
        "for i in range(num_class):\n",
        "    unlabel_idx_dico[str(i)] = []\n",
        "for i in range(num_class):\n",
        "    list_train_id = list_train_id + train_idx_dico[str(i)][0:num_train]\n",
        "    valid_idx =valid_idx + train_idx_dico[str(i)][num_train:num_train+valid_size]\n",
        "    list_unalabel_id = list_unalabel_id + train_idx_dico[str(i)][num_train+valid_size::]\n",
        "    unlabel_idx_dico[str(i)] = train_idx_dico[str(i)][num_train::]\n",
        "\n",
        "#Get labeled and unlabeled data\n",
        "\n",
        "x_train = Original_train_data_x[[int(i) for i in list_train_id]]\n",
        "y_train = Original_train_data_y[[int(i) for i in list_train_id]]\n",
        "\n",
        "x_unlabeled = Original_train_data_x[[int(i) for i in list_unalabel_id]]\n",
        "y_unlabeled = Original_train_data_y[[int(i) for i in list_unalabel_id]]\n",
        "\n",
        "#Get validation set data\n",
        "x_valid = Original_train_data_x[[int(i) for i in valid_idx]]\n",
        "y_valid = Original_train_data_y[[int(i) for i in valid_idx]]\n",
        "\n",
        "# Printing the size of the training, validation and test sets\n",
        "print('Number of training examples: ' + str(x_train.shape[0]))\n",
        "print('Number of unlabeled examples: ' + str(x_unlabeled.shape[0]))\n",
        "print('Number of validation examples: ' + str(x_valid.shape[0]))\n",
        "\n",
        "transform_weak = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4, padding_mode = 'reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# We choose to use RandAugment\n",
        "transform_strong = transforms.Compose([\n",
        "    transforms.RandAugment(),\n",
        "    transforms.ToTensor(),\n",
        "    CutoutDefault(cutout),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "transform_normalize = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "# Dataloader creation\n",
        "\n",
        "labeled_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_train, y_train, transform=transform_normalize),\n",
        "    batch_size=batch_size,shuffle=True, num_workers=2) #num_workers = 2 ou 1\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(Original_test_data_x, Original_test_data_y, transform=transform_normalize),\n",
        "    batch_size = batch_size,\n",
        "    shuffle=False, num_workers=2)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_valid, y_valid, transform= transform_normalize),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "67V160pEVMdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a249228f-21e0-4ae4-ecd2-334c00b925f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Number of training examples: 1000\n",
            "Number of unlabeled examples: 47000\n",
            "Number of validation examples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée le réseau de neurones qui est un WideResNet 28-2\n",
        "\n",
        "On crée également la fonction d'entraînement. Cette dernière affichera en plus des performances et de la fonction de perte, des informations sur le nombre de pseudo label posé sur les images sans label (plus ce nombre est grand plus le réseau est \"confiant\" dans ses prédictions), le nombre de pseudo labels correct (information qu'on ne devrait pas avoir mais qu'on dispose ici et qui permet de voir la performance du réseau de neurones sur des données) ainsi que le nombre de fois où le réseau de neurones donne la même sortie pour une image avec une strong augmentation que son pseudo label (cela mesure sa consistance)"
      ],
      "metadata": {
        "id": "LnuNeNbSdvZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Networks creation\n",
        "net = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class)\n",
        "net = net.to(device)\n",
        "net_save = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class) # model where to save the results\n",
        "net_save = net_save.to(device)\n",
        "\n",
        "# Training\n",
        "def train_fixmatch(epoch,net,trainloader,log_interval=15):\n",
        "    net.train()\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    total_pseudo_label = 0 # nb of unlabeled images\n",
        "    nb_pseudo_label = 0 # nb of pseudo label (higher than threshold)\n",
        "    same_pseudo_label = 0 # nb of same output between weak and strong augment\n",
        "    correct_pseudo_label = 0 # correct pseudo label\n",
        "\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate_scheduler(lr, epoch)))\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_scheduler(lr, epoch), momentum=momentum, weight_decay=weight_decay, nesterov = nesterov)\n",
        "\n",
        "    unlabeled_loader_weak, unlabeled_loader_strong = unlabeled_loader(x_unlabeled, y_unlabeled, seed=epoch)\n",
        "    iterator_strong = iter(unlabeled_loader_strong)\n",
        "    iterator_weak = iter(unlabeled_loader_weak)\n",
        "    for batch_idx, (inputs_train, targets_train) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        # Labeled inputs\n",
        "        inputs_train, targets_train = inputs_train.to(device), targets_train.to(device) # GPU settings\n",
        "        inputs_train, targets_train = Variable(inputs_train), Variable(targets_train)\n",
        "        outputs_train = net(inputs_train)               # Forward Propagation\n",
        "\n",
        "        # Get pseudo label\n",
        "        inputs_weak, targets_weak = next(iterator_weak)\n",
        "        inputs_weak = inputs_weak.to(device)\n",
        "        targets_weak = targets_weak.to(device)\n",
        "        outputs_weak = net(inputs_weak)\n",
        "        outputs_weak = nn.Softmax(dim=1)(outputs_weak) # Softmax = Normalization -> To get probabilities so it can be compared to threshold\n",
        "        outputs_weak_max, pseudo_label = outputs_weak.max(dim=1) # Get max and pseudo label\n",
        "\n",
        "        mask_targets = outputs_weak_max>threshold # Used to know if we take the pseudo label in the loss, mask on targets\n",
        "        mask_inputs = mask_targets.view(-1,1) # Mask on inputs/outputs\n",
        "\n",
        "        # Strong augmentation (only on pseudo label higher than the threshold)\n",
        "        inputs_strong, _ = next(iterator_strong)\n",
        "        inputs_strong = inputs_strong.to(device)\n",
        "        inputs_strong = Variable(inputs_strong)\n",
        "        pseudo_label = Variable(pseudo_label)\n",
        "        outputs_strong = net(inputs_strong)\n",
        "\n",
        "\n",
        "        # Loss and backpropagation\n",
        "        outputs_strong_masked = (torch.masked_select(outputs_strong, mask_inputs)).view(-1, num_class)\n",
        "        pseudo_label_masked = torch.masked_select(pseudo_label, mask_targets)\n",
        "        targets_weak_masked = torch.masked_select(targets_weak, mask_targets)\n",
        "        # loss_u = loss on unlabeled\n",
        "        if outputs_strong_masked.numel() > 0 :\n",
        "            loss_u = criterion(outputs_strong_masked, pseudo_label_masked)\n",
        "        else :\n",
        "            loss_u = 0\n",
        "        loss = criterion(outputs_train, targets_train) + lambda_u * loss_u # Loss\n",
        "        loss.backward()  # Backward Propagation\n",
        "        optimizer.step() # Optimizer update\n",
        "\n",
        "\n",
        "        _, predicted_train = torch.max(outputs_train.data, 1)\n",
        "        correct_train += predicted_train.eq(targets_train.data).cpu().sum()\n",
        "        total_train += targets_train.size(0)\n",
        "\n",
        "        total_pseudo_label += mask_inputs.size(0)\n",
        "        nb_pseudo_label += mask_inputs.cpu().sum()\n",
        "        _, predicted_strong = torch.max(outputs_strong_masked.data, 1)\n",
        "        correct_pseudo_label += pseudo_label_masked.eq(targets_weak_masked.data).cpu().sum()\n",
        "        same_pseudo_label += predicted_strong.eq(pseudo_label_masked.data).cpu().sum()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%% \\nPseudo labeled: %.3f%% (%d/%d)    \\tCorrect pseudo label: %s (%d/%d)    \\tSame output weak and strong augmentation: %s (%d/%d)'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (len(trainloader.dataset)//batch_size)+1, loss.item(), 100.*correct_train/total_train,\n",
        "                    100.*nb_pseudo_label/total_pseudo_label, nb_pseudo_label, total_pseudo_label,\n",
        "                  ('{:.2f}%'.format(100.*correct_pseudo_label/nb_pseudo_label) if nb_pseudo_label!=0 else 'Undefined'), correct_pseudo_label, nb_pseudo_label,\n",
        "                  ('{:.2f}%'.format(100.*same_pseudo_label/nb_pseudo_label) if nb_pseudo_label!=0 else 'Undefined'), same_pseudo_label, nb_pseudo_label))"
      ],
      "metadata": {
        "id": "0lBnTl0XUaOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entraînement et évaluation"
      ],
      "metadata": {
        "id": "z2TfCScTasSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_fixmatch(epoch,net,labeled_loader)\n",
        "    acc =test(epoch,net,valid_loader)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        net_save.load_state_dict(net.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNKOfZHs9KcV",
        "outputId": "e140b5aa-bd36-4c5c-d028-e0ebeb26b4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.0300\n",
            "| Epoch [  0/100] Iter[  1/ 32]\t\tLoss: 2.3109 Acc@1: 9.375% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  0/100] Iter[ 16/ 32]\t\tLoss: 2.1104 Acc@1: 17.969% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  0/100] Iter[ 31/ 32]\t\tLoss: 1.7244 Acc@1: 21.976% \n",
            "Pseudo labeled: 0.000% (0/6944)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 2.1608 Acc@1: 22.70%\n",
            "| Saving Best model...\t\t\tTop1 = 22.70%\n",
            "\n",
            "=> Training Epoch #1, LR=0.0300\n",
            "| Epoch [  1/100] Iter[  1/ 32]\t\tLoss: 2.1796 Acc@1: 21.875% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  1/100] Iter[ 16/ 32]\t\tLoss: 1.8590 Acc@1: 25.781% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  1/100] Iter[ 31/ 32]\t\tLoss: 1.7217 Acc@1: 27.621% \n",
            "Pseudo labeled: 0.000% (0/6944)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 2.6429 Acc@1: 29.15%\n",
            "| Saving Best model...\t\t\tTop1 = 29.15%\n",
            "\n",
            "=> Training Epoch #2, LR=0.0300\n",
            "| Epoch [  2/100] Iter[  1/ 32]\t\tLoss: 1.9760 Acc@1: 25.000% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  2/100] Iter[ 16/ 32]\t\tLoss: 1.9438 Acc@1: 31.250% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  2/100] Iter[ 31/ 32]\t\tLoss: 1.9391 Acc@1: 32.460% \n",
            "Pseudo labeled: 0.000% (0/6944)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 2.3037 Acc@1: 27.20%\n",
            "\n",
            "=> Training Epoch #3, LR=0.0300\n",
            "| Epoch [  3/100] Iter[  1/ 32]\t\tLoss: 1.7047 Acc@1: 34.375% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  3/100] Iter[ 16/ 32]\t\tLoss: 1.3130 Acc@1: 37.500% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  3/100] Iter[ 31/ 32]\t\tLoss: 1.8187 Acc@1: 37.097% \n",
            "Pseudo labeled: 0.000% (0/6944)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 2.2976 Acc@1: 23.35%\n",
            "\n",
            "=> Training Epoch #4, LR=0.0300\n",
            "| Epoch [  4/100] Iter[  1/ 32]\t\tLoss: 1.7668 Acc@1: 31.250% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  4/100] Iter[ 16/ 32]\t\tLoss: 1.5631 Acc@1: 39.258% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  4/100] Iter[ 31/ 32]\t\tLoss: 2.5066 Acc@1: 35.685% \n",
            "Pseudo labeled: 2.463% (171/6944)    \tCorrect pseudo label: 30.41% (52/171)    \tSame output weak and strong augmentation: 93.57% (160/171)\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 3.4777 Acc@1: 29.40%\n",
            "| Saving Best model...\t\t\tTop1 = 29.40%\n",
            "\n",
            "=> Training Epoch #5, LR=0.0300\n",
            "| Epoch [  5/100] Iter[  1/ 32]\t\tLoss: 2.9021 Acc@1: 25.000% \n",
            "Pseudo labeled: 8.482% (19/224)    \tCorrect pseudo label: 42.11% (8/19)    \tSame output weak and strong augmentation: 94.74% (18/19)\n",
            "| Epoch [  5/100] Iter[ 16/ 32]\t\tLoss: 1.9783 Acc@1: 32.031% \n",
            "Pseudo labeled: 2.985% (107/3584)    \tCorrect pseudo label: 28.04% (30/107)    \tSame output weak and strong augmentation: 97.20% (104/107)\n",
            "| Epoch [  5/100] Iter[ 31/ 32]\t\tLoss: 1.4627 Acc@1: 32.863% \n",
            "Pseudo labeled: 1.541% (107/6944)    \tCorrect pseudo label: 28.04% (30/107)    \tSame output weak and strong augmentation: 97.20% (104/107)\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 2.5675 Acc@1: 29.65%\n",
            "| Saving Best model...\t\t\tTop1 = 29.65%\n",
            "\n",
            "=> Training Epoch #6, LR=0.0300\n",
            "| Epoch [  6/100] Iter[  1/ 32]\t\tLoss: 1.8189 Acc@1: 37.500% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  6/100] Iter[ 16/ 32]\t\tLoss: 1.3214 Acc@1: 39.648% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  6/100] Iter[ 31/ 32]\t\tLoss: 2.4530 Acc@1: 38.105% \n",
            "Pseudo labeled: 0.677% (47/6944)    \tCorrect pseudo label: 23.40% (11/47)    \tSame output weak and strong augmentation: 93.62% (44/47)\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 4.2510 Acc@1: 25.05%\n",
            "\n",
            "=> Training Epoch #7, LR=0.0300\n",
            "| Epoch [  7/100] Iter[  1/ 32]\t\tLoss: 2.5094 Acc@1: 28.125% \n",
            "Pseudo labeled: 8.036% (18/224)    \tCorrect pseudo label: 22.22% (4/18)    \tSame output weak and strong augmentation: 100.00% (18/18)\n",
            "| Epoch [  7/100] Iter[ 16/ 32]\t\tLoss: 2.7568 Acc@1: 31.445% \n",
            "Pseudo labeled: 2.651% (95/3584)    \tCorrect pseudo label: 20.00% (19/95)    \tSame output weak and strong augmentation: 94.74% (90/95)\n",
            "| Epoch [  7/100] Iter[ 31/ 32]\t\tLoss: 1.6117 Acc@1: 31.552% \n",
            "Pseudo labeled: 1.397% (97/6944)    \tCorrect pseudo label: 20.62% (20/97)    \tSame output weak and strong augmentation: 94.85% (92/97)\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 2.6490 Acc@1: 25.50%\n",
            "\n",
            "=> Training Epoch #8, LR=0.0300\n",
            "| Epoch [  8/100] Iter[  1/ 32]\t\tLoss: 1.8038 Acc@1: 31.250% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  8/100] Iter[ 16/ 32]\t\tLoss: 1.4864 Acc@1: 34.570% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  8/100] Iter[ 31/ 32]\t\tLoss: 1.6435 Acc@1: 40.121% \n",
            "Pseudo labeled: 0.000% (0/6944)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 2.4512 Acc@1: 31.70%\n",
            "| Saving Best model...\t\t\tTop1 = 31.70%\n",
            "\n",
            "=> Training Epoch #9, LR=0.0300\n",
            "| Epoch [  9/100] Iter[  1/ 32]\t\tLoss: 1.7976 Acc@1: 31.250% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  9/100] Iter[ 16/ 32]\t\tLoss: 1.7166 Acc@1: 42.578% \n",
            "Pseudo labeled: 0.000% (0/3584)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [  9/100] Iter[ 31/ 32]\t\tLoss: 1.6555 Acc@1: 38.407% \n",
            "Pseudo labeled: 1.915% (133/6944)    \tCorrect pseudo label: 43.61% (58/133)    \tSame output weak and strong augmentation: 95.49% (127/133)\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 1.8215 Acc@1: 35.05%\n",
            "| Saving Best model...\t\t\tTop1 = 35.05%\n",
            "\n",
            "=> Training Epoch #10, LR=0.0300\n",
            "| Epoch [ 10/100] Iter[  1/ 32]\t\tLoss: 1.8126 Acc@1: 53.125% \n",
            "Pseudo labeled: 1.339% (3/224)    \tCorrect pseudo label: 33.33% (1/3)    \tSame output weak and strong augmentation: 100.00% (3/3)\n",
            "| Epoch [ 10/100] Iter[ 16/ 32]\t\tLoss: 2.5167 Acc@1: 40.039% \n",
            "Pseudo labeled: 2.846% (102/3584)    \tCorrect pseudo label: 38.24% (39/102)    \tSame output weak and strong augmentation: 92.16% (94/102)\n",
            "| Epoch [ 10/100] Iter[ 31/ 32]\t\tLoss: 2.2334 Acc@1: 34.476% \n",
            "Pseudo labeled: 2.074% (144/6944)    \tCorrect pseudo label: 27.08% (39/144)    \tSame output weak and strong augmentation: 93.75% (135/144)\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 2.3545 Acc@1: 33.75%\n",
            "\n",
            "=> Training Epoch #11, LR=0.0300\n",
            "| Epoch [ 11/100] Iter[  1/ 32]\t\tLoss: 1.6941 Acc@1: 46.875% \n",
            "Pseudo labeled: 1.339% (3/224)    \tCorrect pseudo label: 66.67% (2/3)    \tSame output weak and strong augmentation: 100.00% (3/3)\n",
            "| Epoch [ 11/100] Iter[ 16/ 32]\t\tLoss: 1.4317 Acc@1: 39.062% \n",
            "Pseudo labeled: 3.348% (120/3584)    \tCorrect pseudo label: 40.83% (49/120)    \tSame output weak and strong augmentation: 98.33% (118/120)\n",
            "| Epoch [ 11/100] Iter[ 31/ 32]\t\tLoss: 1.4938 Acc@1: 40.323% \n",
            "Pseudo labeled: 1.728% (120/6944)    \tCorrect pseudo label: 40.83% (49/120)    \tSame output weak and strong augmentation: 98.33% (118/120)\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 3.3933 Acc@1: 32.75%\n",
            "\n",
            "=> Training Epoch #12, LR=0.0300\n",
            "| Epoch [ 12/100] Iter[  1/ 32]\t\tLoss: 1.6521 Acc@1: 43.750% \n",
            "Pseudo labeled: 0.000% (0/224)    \tCorrect pseudo label: Undefined (0/0)    \tSame output weak and strong augmentation: Undefined (0/0)\n",
            "| Epoch [ 12/100] Iter[ 16/ 32]\t\tLoss: 1.8903 Acc@1: 45.312% \n",
            "Pseudo labeled: 1.869% (67/3584)    \tCorrect pseudo label: 40.30% (27/67)    \tSame output weak and strong augmentation: 85.07% (57/67)\n",
            "| Epoch [ 12/100] Iter[ 31/ 32]\t\tLoss: 2.1678 Acc@1: 35.181% \n",
            "Pseudo labeled: 1.800% (125/6944)    \tCorrect pseudo label: 38.40% (48/125)    \tSame output weak and strong augmentation: 88.00% (110/125)\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 1.3992 Acc@1: 27.35%\n",
            "\n",
            "=> Training Epoch #13, LR=0.0300\n",
            "| Epoch [ 13/100] Iter[  1/ 32]\t\tLoss: 2.1529 Acc@1: 25.000% \n",
            "Pseudo labeled: 1.786% (4/224)    \tCorrect pseudo label: 50.00% (2/4)    \tSame output weak and strong augmentation: 100.00% (4/4)\n",
            "| Epoch [ 13/100] Iter[ 16/ 32]\t\tLoss: 1.8251 Acc@1: 33.984% \n",
            "Pseudo labeled: 1.730% (62/3584)    \tCorrect pseudo label: 37.10% (23/62)    \tSame output weak and strong augmentation: 93.55% (58/62)\n",
            "| Epoch [ 13/100] Iter[ 31/ 32]\t\tLoss: 1.8094 Acc@1: 33.266% \n",
            "Pseudo labeled: 2.779% (193/6944)    \tCorrect pseudo label: 44.04% (85/193)    \tSame output weak and strong augmentation: 92.75% (179/193)\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 3.2373 Acc@1: 32.45%\n",
            "\n",
            "=> Training Epoch #14, LR=0.0300\n",
            "| Epoch [ 14/100] Iter[  1/ 32]\t\tLoss: 2.0931 Acc@1: 34.375% \n",
            "Pseudo labeled: 2.679% (6/224)    \tCorrect pseudo label: 50.00% (3/6)    \tSame output weak and strong augmentation: 100.00% (6/6)\n",
            "| Epoch [ 14/100] Iter[ 16/ 32]\t\tLoss: 2.2053 Acc@1: 38.867% \n",
            "Pseudo labeled: 3.906% (140/3584)    \tCorrect pseudo label: 65.00% (91/140)    \tSame output weak and strong augmentation: 96.43% (135/140)\n",
            "| Epoch [ 14/100] Iter[ 31/ 32]\t\tLoss: 2.0073 Acc@1: 37.802% \n",
            "Pseudo labeled: 3.384% (235/6944)    \tCorrect pseudo label: 62.55% (147/235)    \tSame output weak and strong augmentation: 95.74% (225/235)\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 2.4981 Acc@1: 35.00%\n",
            "\n",
            "=> Training Epoch #15, LR=0.0300\n",
            "| Epoch [ 15/100] Iter[  1/ 32]\t\tLoss: 1.5375 Acc@1: 46.875% \n",
            "Pseudo labeled: 4.018% (9/224)    \tCorrect pseudo label: 88.89% (8/9)    \tSame output weak and strong augmentation: 100.00% (9/9)\n",
            "| Epoch [ 15/100] Iter[ 16/ 32]\t\tLoss: 2.4474 Acc@1: 45.117% \n",
            "Pseudo labeled: 3.739% (134/3584)    \tCorrect pseudo label: 54.48% (73/134)    \tSame output weak and strong augmentation: 96.27% (129/134)\n",
            "| Epoch [ 15/100] Iter[ 31/ 32]\t\tLoss: 1.3824 Acc@1: 43.448% \n",
            "Pseudo labeled: 3.255% (226/6944)    \tCorrect pseudo label: 58.41% (132/226)    \tSame output weak and strong augmentation: 94.69% (214/226)\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 1.3117 Acc@1: 41.95%\n",
            "| Saving Best model...\t\t\tTop1 = 41.95%\n",
            "\n",
            "=> Training Epoch #16, LR=0.0300\n",
            "| Epoch [ 16/100] Iter[  1/ 32]\t\tLoss: 1.5048 Acc@1: 62.500% \n",
            "Pseudo labeled: 0.446% (1/224)    \tCorrect pseudo label: 100.00% (1/1)    \tSame output weak and strong augmentation: 100.00% (1/1)\n",
            "| Epoch [ 16/100] Iter[ 16/ 32]\t\tLoss: 1.7543 Acc@1: 43.164% \n",
            "Pseudo labeled: 4.353% (156/3584)    \tCorrect pseudo label: 67.95% (106/156)    \tSame output weak and strong augmentation: 96.15% (150/156)\n",
            "| Epoch [ 16/100] Iter[ 31/ 32]\t\tLoss: 2.1029 Acc@1: 43.952% \n",
            "Pseudo labeled: 4.983% (346/6944)    \tCorrect pseudo label: 61.27% (212/346)    \tSame output weak and strong augmentation: 96.24% (333/346)\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 2.7789 Acc@1: 34.50%\n",
            "\n",
            "=> Training Epoch #17, LR=0.0300\n",
            "| Epoch [ 17/100] Iter[  1/ 32]\t\tLoss: 1.9630 Acc@1: 43.750% \n",
            "Pseudo labeled: 10.268% (23/224)    \tCorrect pseudo label: 60.87% (14/23)    \tSame output weak and strong augmentation: 100.00% (23/23)\n",
            "| Epoch [ 17/100] Iter[ 16/ 32]\t\tLoss: 1.7117 Acc@1: 49.023% \n",
            "Pseudo labeled: 5.329% (191/3584)    \tCorrect pseudo label: 70.16% (134/191)    \tSame output weak and strong augmentation: 93.19% (178/191)\n",
            "| Epoch [ 17/100] Iter[ 31/ 32]\t\tLoss: 1.3537 Acc@1: 46.875% \n",
            "Pseudo labeled: 5.487% (381/6944)    \tCorrect pseudo label: 68.24% (260/381)    \tSame output weak and strong augmentation: 92.39% (352/381)\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 1.4285 Acc@1: 40.75%\n",
            "\n",
            "=> Training Epoch #18, LR=0.0300\n",
            "| Epoch [ 18/100] Iter[  1/ 32]\t\tLoss: 1.9148 Acc@1: 37.500% \n",
            "Pseudo labeled: 2.679% (6/224)    \tCorrect pseudo label: 50.00% (3/6)    \tSame output weak and strong augmentation: 100.00% (6/6)\n",
            "| Epoch [ 18/100] Iter[ 16/ 32]\t\tLoss: 1.7465 Acc@1: 49.414% \n",
            "Pseudo labeled: 6.417% (230/3584)    \tCorrect pseudo label: 69.57% (160/230)    \tSame output weak and strong augmentation: 93.91% (216/230)\n",
            "| Epoch [ 18/100] Iter[ 31/ 32]\t\tLoss: 2.2135 Acc@1: 50.403% \n",
            "Pseudo labeled: 6.855% (476/6944)    \tCorrect pseudo label: 70.80% (337/476)    \tSame output weak and strong augmentation: 91.39% (435/476)\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 3.0178 Acc@1: 33.25%\n",
            "\n",
            "=> Training Epoch #19, LR=0.0300\n",
            "| Epoch [ 19/100] Iter[  1/ 32]\t\tLoss: 1.4033 Acc@1: 56.250% \n",
            "Pseudo labeled: 9.375% (21/224)    \tCorrect pseudo label: 76.19% (16/21)    \tSame output weak and strong augmentation: 95.24% (20/21)\n",
            "| Epoch [ 19/100] Iter[ 16/ 32]\t\tLoss: 1.9629 Acc@1: 50.977% \n",
            "Pseudo labeled: 5.943% (213/3584)    \tCorrect pseudo label: 70.89% (151/213)    \tSame output weak and strong augmentation: 95.31% (203/213)\n",
            "| Epoch [ 19/100] Iter[ 31/ 32]\t\tLoss: 1.7361 Acc@1: 50.101% \n",
            "Pseudo labeled: 6.020% (418/6944)    \tCorrect pseudo label: 72.97% (305/418)    \tSame output weak and strong augmentation: 95.22% (398/418)\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 2.3800 Acc@1: 41.25%\n",
            "\n",
            "=> Training Epoch #20, LR=0.0300\n",
            "| Epoch [ 20/100] Iter[  1/ 32]\t\tLoss: 1.9527 Acc@1: 50.000% \n",
            "Pseudo labeled: 3.571% (8/224)    \tCorrect pseudo label: 75.00% (6/8)    \tSame output weak and strong augmentation: 87.50% (7/8)\n",
            "| Epoch [ 20/100] Iter[ 16/ 32]\t\tLoss: 1.2693 Acc@1: 54.883% \n",
            "Pseudo labeled: 6.278% (225/3584)    \tCorrect pseudo label: 70.22% (158/225)    \tSame output weak and strong augmentation: 94.22% (212/225)\n",
            "| Epoch [ 20/100] Iter[ 31/ 32]\t\tLoss: 2.1992 Acc@1: 52.823% \n",
            "Pseudo labeled: 7.388% (513/6944)    \tCorrect pseudo label: 63.16% (324/513)    \tSame output weak and strong augmentation: 92.40% (474/513)\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 1.1490 Acc@1: 42.45%\n",
            "| Saving Best model...\t\t\tTop1 = 42.45%\n",
            "\n",
            "=> Training Epoch #21, LR=0.0300\n",
            "| Epoch [ 21/100] Iter[  1/ 32]\t\tLoss: 1.5819 Acc@1: 59.375% \n",
            "Pseudo labeled: 4.018% (9/224)    \tCorrect pseudo label: 44.44% (4/9)    \tSame output weak and strong augmentation: 100.00% (9/9)\n",
            "| Epoch [ 21/100] Iter[ 16/ 32]\t\tLoss: 1.6696 Acc@1: 55.469% \n",
            "Pseudo labeled: 5.999% (215/3584)    \tCorrect pseudo label: 64.65% (139/215)    \tSame output weak and strong augmentation: 93.02% (200/215)\n",
            "| Epoch [ 21/100] Iter[ 31/ 32]\t\tLoss: 1.5244 Acc@1: 54.536% \n",
            "Pseudo labeled: 6.207% (431/6944)    \tCorrect pseudo label: 71.46% (308/431)    \tSame output weak and strong augmentation: 90.49% (390/431)\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 1.4541 Acc@1: 37.40%\n",
            "\n",
            "=> Training Epoch #22, LR=0.0300\n",
            "| Epoch [ 22/100] Iter[  1/ 32]\t\tLoss: 1.3373 Acc@1: 62.500% \n",
            "Pseudo labeled: 3.125% (7/224)    \tCorrect pseudo label: 57.14% (4/7)    \tSame output weak and strong augmentation: 85.71% (6/7)\n",
            "| Epoch [ 22/100] Iter[ 16/ 32]\t\tLoss: 1.4386 Acc@1: 56.055% \n",
            "Pseudo labeled: 8.203% (294/3584)    \tCorrect pseudo label: 75.85% (223/294)    \tSame output weak and strong augmentation: 93.54% (275/294)\n",
            "| Epoch [ 22/100] Iter[ 31/ 32]\t\tLoss: 2.3102 Acc@1: 55.343% \n",
            "Pseudo labeled: 8.511% (591/6944)    \tCorrect pseudo label: 79.53% (470/591)    \tSame output weak and strong augmentation: 90.86% (537/591)\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 2.1284 Acc@1: 38.20%\n",
            "\n",
            "=> Training Epoch #23, LR=0.0300\n",
            "| Epoch [ 23/100] Iter[  1/ 32]\t\tLoss: 1.1879 Acc@1: 71.875% \n",
            "Pseudo labeled: 5.804% (13/224)    \tCorrect pseudo label: 92.31% (12/13)    \tSame output weak and strong augmentation: 92.31% (12/13)\n",
            "| Epoch [ 23/100] Iter[ 16/ 32]\t\tLoss: 1.5133 Acc@1: 59.375% \n",
            "Pseudo labeled: 8.259% (296/3584)    \tCorrect pseudo label: 83.11% (246/296)    \tSame output weak and strong augmentation: 89.19% (264/296)\n",
            "| Epoch [ 23/100] Iter[ 31/ 32]\t\tLoss: 1.5580 Acc@1: 58.871% \n",
            "Pseudo labeled: 8.252% (573/6944)    \tCorrect pseudo label: 81.33% (466/573)    \tSame output weak and strong augmentation: 89.01% (510/573)\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 1.0281 Acc@1: 45.60%\n",
            "| Saving Best model...\t\t\tTop1 = 45.60%\n",
            "\n",
            "=> Training Epoch #24, LR=0.0300\n",
            "| Epoch [ 24/100] Iter[  1/ 32]\t\tLoss: 1.5428 Acc@1: 71.875% \n",
            "Pseudo labeled: 9.821% (22/224)    \tCorrect pseudo label: 68.18% (15/22)    \tSame output weak and strong augmentation: 77.27% (17/22)\n",
            "| Epoch [ 24/100] Iter[ 16/ 32]\t\tLoss: 1.2956 Acc@1: 66.016% \n",
            "Pseudo labeled: 12.667% (454/3584)    \tCorrect pseudo label: 84.14% (382/454)    \tSame output weak and strong augmentation: 88.77% (403/454)\n",
            "| Epoch [ 24/100] Iter[ 31/ 32]\t\tLoss: 2.1188 Acc@1: 64.315% \n",
            "Pseudo labeled: 11.780% (818/6944)    \tCorrect pseudo label: 84.35% (690/818)    \tSame output weak and strong augmentation: 88.63% (725/818)\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 1.1846 Acc@1: 43.10%\n",
            "\n",
            "=> Training Epoch #25, LR=0.0300\n",
            "| Epoch [ 25/100] Iter[  1/ 32]\t\tLoss: 1.1042 Acc@1: 68.750% \n",
            "Pseudo labeled: 12.500% (28/224)    \tCorrect pseudo label: 82.14% (23/28)    \tSame output weak and strong augmentation: 85.71% (24/28)\n",
            "| Epoch [ 25/100] Iter[ 16/ 32]\t\tLoss: 1.2876 Acc@1: 65.430% \n",
            "Pseudo labeled: 13.644% (489/3584)    \tCorrect pseudo label: 80.57% (394/489)    \tSame output weak and strong augmentation: 89.37% (437/489)\n",
            "| Epoch [ 25/100] Iter[ 31/ 32]\t\tLoss: 1.7768 Acc@1: 66.028% \n",
            "Pseudo labeled: 13.710% (952/6944)    \tCorrect pseudo label: 80.78% (769/952)    \tSame output weak and strong augmentation: 88.13% (839/952)\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 2.7624 Acc@1: 43.55%\n",
            "\n",
            "=> Training Epoch #26, LR=0.0300\n",
            "| Epoch [ 26/100] Iter[  1/ 32]\t\tLoss: 1.0774 Acc@1: 59.375% \n",
            "Pseudo labeled: 10.268% (23/224)    \tCorrect pseudo label: 86.96% (20/23)    \tSame output weak and strong augmentation: 91.30% (21/23)\n",
            "| Epoch [ 26/100] Iter[ 16/ 32]\t\tLoss: 1.1334 Acc@1: 67.383% \n",
            "Pseudo labeled: 13.142% (471/3584)    \tCorrect pseudo label: 80.47% (379/471)    \tSame output weak and strong augmentation: 87.05% (410/471)\n",
            "| Epoch [ 26/100] Iter[ 31/ 32]\t\tLoss: 1.3820 Acc@1: 66.935% \n",
            "Pseudo labeled: 14.070% (977/6944)    \tCorrect pseudo label: 79.43% (776/977)    \tSame output weak and strong augmentation: 87.10% (851/977)\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 3.2391 Acc@1: 41.15%\n",
            "\n",
            "=> Training Epoch #27, LR=0.0300\n",
            "| Epoch [ 27/100] Iter[  1/ 32]\t\tLoss: 1.2719 Acc@1: 75.000% \n",
            "Pseudo labeled: 11.607% (26/224)    \tCorrect pseudo label: 69.23% (18/26)    \tSame output weak and strong augmentation: 76.92% (20/26)\n",
            "| Epoch [ 27/100] Iter[ 16/ 32]\t\tLoss: 1.2152 Acc@1: 71.289% \n",
            "Pseudo labeled: 12.863% (461/3584)    \tCorrect pseudo label: 85.90% (396/461)    \tSame output weak and strong augmentation: 87.42% (403/461)\n",
            "| Epoch [ 27/100] Iter[ 31/ 32]\t\tLoss: 1.1434 Acc@1: 67.742% \n",
            "Pseudo labeled: 14.372% (998/6944)    \tCorrect pseudo label: 83.17% (830/998)    \tSame output weak and strong augmentation: 87.17% (870/998)\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 0.9888 Acc@1: 49.10%\n",
            "| Saving Best model...\t\t\tTop1 = 49.10%\n",
            "\n",
            "=> Training Epoch #28, LR=0.0300\n",
            "| Epoch [ 28/100] Iter[  1/ 32]\t\tLoss: 0.8523 Acc@1: 81.250% \n",
            "Pseudo labeled: 13.839% (31/224)    \tCorrect pseudo label: 87.10% (27/31)    \tSame output weak and strong augmentation: 90.32% (28/31)\n",
            "| Epoch [ 28/100] Iter[ 16/ 32]\t\tLoss: 1.3001 Acc@1: 73.438% \n",
            "Pseudo labeled: 16.211% (581/3584)    \tCorrect pseudo label: 82.62% (480/581)    \tSame output weak and strong augmentation: 85.54% (497/581)\n",
            "| Epoch [ 28/100] Iter[ 31/ 32]\t\tLoss: 1.1923 Acc@1: 70.262% \n",
            "Pseudo labeled: 15.783% (1096/6944)    \tCorrect pseudo label: 81.39% (892/1096)    \tSame output weak and strong augmentation: 85.49% (937/1096)\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 1.0213 Acc@1: 45.95%\n",
            "\n",
            "=> Training Epoch #29, LR=0.0300\n",
            "| Epoch [ 29/100] Iter[  1/ 32]\t\tLoss: 1.5300 Acc@1: 56.250% \n",
            "Pseudo labeled: 16.071% (36/224)    \tCorrect pseudo label: 66.67% (24/36)    \tSame output weak and strong augmentation: 88.89% (32/36)\n",
            "| Epoch [ 29/100] Iter[ 16/ 32]\t\tLoss: 1.4244 Acc@1: 73.242% \n",
            "Pseudo labeled: 13.588% (487/3584)    \tCorrect pseudo label: 84.80% (413/487)    \tSame output weak and strong augmentation: 85.42% (416/487)\n",
            "| Epoch [ 29/100] Iter[ 31/ 32]\t\tLoss: 1.0615 Acc@1: 74.294% \n",
            "Pseudo labeled: 15.697% (1090/6944)    \tCorrect pseudo label: 83.39% (909/1090)    \tSame output weak and strong augmentation: 86.24% (940/1090)\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 1.9487 Acc@1: 41.35%\n",
            "\n",
            "=> Training Epoch #30, LR=0.0300\n",
            "| Epoch [ 30/100] Iter[  1/ 32]\t\tLoss: 1.3664 Acc@1: 62.500% \n",
            "Pseudo labeled: 17.857% (40/224)    \tCorrect pseudo label: 80.00% (32/40)    \tSame output weak and strong augmentation: 87.50% (35/40)\n",
            "| Epoch [ 30/100] Iter[ 16/ 32]\t\tLoss: 1.1173 Acc@1: 78.906% \n",
            "Pseudo labeled: 18.610% (667/3584)    \tCorrect pseudo label: 82.01% (547/667)    \tSame output weak and strong augmentation: 83.21% (555/667)\n",
            "| Epoch [ 30/100] Iter[ 31/ 32]\t\tLoss: 1.2131 Acc@1: 76.109% \n",
            "Pseudo labeled: 18.548% (1288/6944)    \tCorrect pseudo label: 82.76% (1066/1288)    \tSame output weak and strong augmentation: 83.23% (1072/1288)\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 0.3296 Acc@1: 49.50%\n",
            "| Saving Best model...\t\t\tTop1 = 49.50%\n",
            "\n",
            "=> Training Epoch #31, LR=0.0300\n",
            "| Epoch [ 31/100] Iter[  1/ 32]\t\tLoss: 1.4417 Acc@1: 65.625% \n",
            "Pseudo labeled: 20.089% (45/224)    \tCorrect pseudo label: 82.22% (37/45)    \tSame output weak and strong augmentation: 80.00% (36/45)\n",
            "| Epoch [ 31/100] Iter[ 16/ 32]\t\tLoss: 0.8731 Acc@1: 82.422% \n",
            "Pseudo labeled: 19.950% (715/3584)    \tCorrect pseudo label: 82.38% (589/715)    \tSame output weak and strong augmentation: 85.73% (613/715)\n",
            "| Epoch [ 31/100] Iter[ 31/ 32]\t\tLoss: 1.0045 Acc@1: 79.839% \n",
            "Pseudo labeled: 19.052% (1323/6944)    \tCorrect pseudo label: 81.03% (1072/1323)    \tSame output weak and strong augmentation: 85.03% (1125/1323)\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 1.0986 Acc@1: 50.40%\n",
            "| Saving Best model...\t\t\tTop1 = 50.40%\n",
            "\n",
            "=> Training Epoch #32, LR=0.0300\n",
            "| Epoch [ 32/100] Iter[  1/ 32]\t\tLoss: 1.0893 Acc@1: 90.625% \n",
            "Pseudo labeled: 19.643% (44/224)    \tCorrect pseudo label: 81.82% (36/44)    \tSame output weak and strong augmentation: 84.09% (37/44)\n",
            "| Epoch [ 32/100] Iter[ 16/ 32]\t\tLoss: 1.4605 Acc@1: 80.859% \n",
            "Pseudo labeled: 19.336% (693/3584)    \tCorrect pseudo label: 81.53% (565/693)    \tSame output weak and strong augmentation: 86.00% (596/693)\n",
            "| Epoch [ 32/100] Iter[ 31/ 32]\t\tLoss: 1.1881 Acc@1: 79.536% \n",
            "Pseudo labeled: 19.643% (1364/6944)    \tCorrect pseudo label: 82.84% (1130/1364)    \tSame output weak and strong augmentation: 85.63% (1168/1364)\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 0.7131 Acc@1: 48.80%\n",
            "\n",
            "=> Training Epoch #33, LR=0.0300\n",
            "| Epoch [ 33/100] Iter[  1/ 32]\t\tLoss: 0.8185 Acc@1: 81.250% \n",
            "Pseudo labeled: 18.750% (42/224)    \tCorrect pseudo label: 69.05% (29/42)    \tSame output weak and strong augmentation: 85.71% (36/42)\n",
            "| Epoch [ 33/100] Iter[ 16/ 32]\t\tLoss: 0.5114 Acc@1: 83.203% \n",
            "Pseudo labeled: 20.647% (740/3584)    \tCorrect pseudo label: 83.24% (616/740)    \tSame output weak and strong augmentation: 86.62% (641/740)\n",
            "| Epoch [ 33/100] Iter[ 31/ 32]\t\tLoss: 1.1210 Acc@1: 82.560% \n",
            "Pseudo labeled: 19.888% (1381/6944)    \tCorrect pseudo label: 82.55% (1140/1381)    \tSame output weak and strong augmentation: 85.08% (1175/1381)\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 1.2842 Acc@1: 48.15%\n",
            "\n",
            "=> Training Epoch #34, LR=0.0300\n",
            "| Epoch [ 34/100] Iter[  1/ 32]\t\tLoss: 0.8382 Acc@1: 87.500% \n",
            "Pseudo labeled: 16.964% (38/224)    \tCorrect pseudo label: 63.16% (24/38)    \tSame output weak and strong augmentation: 84.21% (32/38)\n",
            "| Epoch [ 34/100] Iter[ 16/ 32]\t\tLoss: 0.6395 Acc@1: 88.086% \n",
            "Pseudo labeled: 22.266% (798/3584)    \tCorrect pseudo label: 80.08% (639/798)    \tSame output weak and strong augmentation: 84.09% (671/798)\n",
            "| Epoch [ 34/100] Iter[ 31/ 32]\t\tLoss: 1.0834 Acc@1: 85.081% \n",
            "Pseudo labeled: 23.041% (1600/6944)    \tCorrect pseudo label: 81.31% (1301/1600)    \tSame output weak and strong augmentation: 84.81% (1357/1600)\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 0.1326 Acc@1: 54.90%\n",
            "| Saving Best model...\t\t\tTop1 = 54.90%\n",
            "\n",
            "=> Training Epoch #35, LR=0.0300\n",
            "| Epoch [ 35/100] Iter[  1/ 32]\t\tLoss: 0.6716 Acc@1: 96.875% \n",
            "Pseudo labeled: 20.982% (47/224)    \tCorrect pseudo label: 80.85% (38/47)    \tSame output weak and strong augmentation: 87.23% (41/47)\n",
            "| Epoch [ 35/100] Iter[ 16/ 32]\t\tLoss: 0.9882 Acc@1: 89.062% \n",
            "Pseudo labeled: 24.665% (884/3584)    \tCorrect pseudo label: 82.35% (728/884)    \tSame output weak and strong augmentation: 85.41% (755/884)\n",
            "| Epoch [ 35/100] Iter[ 31/ 32]\t\tLoss: 1.4551 Acc@1: 87.298% \n",
            "Pseudo labeled: 24.352% (1691/6944)    \tCorrect pseudo label: 81.90% (1385/1691)    \tSame output weak and strong augmentation: 83.68% (1415/1691)\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 1.9653 Acc@1: 49.80%\n",
            "\n",
            "=> Training Epoch #36, LR=0.0300\n",
            "| Epoch [ 36/100] Iter[  1/ 32]\t\tLoss: 1.4965 Acc@1: 68.750% \n",
            "Pseudo labeled: 22.321% (50/224)    \tCorrect pseudo label: 78.00% (39/50)    \tSame output weak and strong augmentation: 80.00% (40/50)\n",
            "| Epoch [ 36/100] Iter[ 16/ 32]\t\tLoss: 0.9309 Acc@1: 85.352% \n",
            "Pseudo labeled: 23.661% (848/3584)    \tCorrect pseudo label: 80.42% (682/848)    \tSame output weak and strong augmentation: 83.02% (704/848)\n",
            "| Epoch [ 36/100] Iter[ 31/ 32]\t\tLoss: 1.0183 Acc@1: 85.988% \n",
            "Pseudo labeled: 24.093% (1673/6944)    \tCorrect pseudo label: 80.16% (1341/1673)    \tSame output weak and strong augmentation: 82.67% (1383/1673)\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.3410 Acc@1: 53.20%\n",
            "\n",
            "=> Training Epoch #37, LR=0.0300\n",
            "| Epoch [ 37/100] Iter[  1/ 32]\t\tLoss: 1.0676 Acc@1: 84.375% \n",
            "Pseudo labeled: 23.214% (52/224)    \tCorrect pseudo label: 78.85% (41/52)    \tSame output weak and strong augmentation: 78.85% (41/52)\n",
            "| Epoch [ 37/100] Iter[ 16/ 32]\t\tLoss: 0.7067 Acc@1: 90.039% \n",
            "Pseudo labeled: 22.824% (818/3584)    \tCorrect pseudo label: 81.30% (665/818)    \tSame output weak and strong augmentation: 83.74% (685/818)\n",
            "| Epoch [ 37/100] Iter[ 31/ 32]\t\tLoss: 1.1991 Acc@1: 87.298% \n",
            "Pseudo labeled: 24.525% (1703/6944)    \tCorrect pseudo label: 80.15% (1365/1703)    \tSame output weak and strong augmentation: 84.32% (1436/1703)\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 1.0924 Acc@1: 44.00%\n",
            "\n",
            "=> Training Epoch #38, LR=0.0300\n",
            "| Epoch [ 38/100] Iter[  1/ 32]\t\tLoss: 1.0044 Acc@1: 84.375% \n",
            "Pseudo labeled: 30.357% (68/224)    \tCorrect pseudo label: 85.29% (58/68)    \tSame output weak and strong augmentation: 79.41% (54/68)\n",
            "| Epoch [ 38/100] Iter[ 16/ 32]\t\tLoss: 1.0698 Acc@1: 91.016% \n",
            "Pseudo labeled: 26.507% (950/3584)    \tCorrect pseudo label: 80.42% (764/950)    \tSame output weak and strong augmentation: 82.74% (786/950)\n",
            "| Epoch [ 38/100] Iter[ 31/ 32]\t\tLoss: 0.7107 Acc@1: 92.540% \n",
            "Pseudo labeled: 26.671% (1852/6944)    \tCorrect pseudo label: 79.97% (1481/1852)    \tSame output weak and strong augmentation: 83.05% (1538/1852)\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 0.5774 Acc@1: 50.90%\n",
            "\n",
            "=> Training Epoch #39, LR=0.0300\n",
            "| Epoch [ 39/100] Iter[  1/ 32]\t\tLoss: 0.8917 Acc@1: 84.375% \n",
            "Pseudo labeled: 28.571% (64/224)    \tCorrect pseudo label: 81.25% (52/64)    \tSame output weak and strong augmentation: 84.38% (54/64)\n",
            "| Epoch [ 39/100] Iter[ 16/ 32]\t\tLoss: 1.1312 Acc@1: 93.750% \n",
            "Pseudo labeled: 27.818% (997/3584)    \tCorrect pseudo label: 76.13% (759/997)    \tSame output weak and strong augmentation: 82.15% (819/997)\n",
            "| Epoch [ 39/100] Iter[ 31/ 32]\t\tLoss: 0.9210 Acc@1: 91.230% \n",
            "Pseudo labeled: 27.578% (1915/6944)    \tCorrect pseudo label: 77.86% (1491/1915)    \tSame output weak and strong augmentation: 81.98% (1570/1915)\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 0.6559 Acc@1: 54.10%\n",
            "\n",
            "=> Training Epoch #40, LR=0.0300\n",
            "| Epoch [ 40/100] Iter[  1/ 32]\t\tLoss: 0.5640 Acc@1: 87.500% \n",
            "Pseudo labeled: 29.911% (67/224)    \tCorrect pseudo label: 74.63% (50/67)    \tSame output weak and strong augmentation: 89.55% (60/67)\n",
            "| Epoch [ 40/100] Iter[ 16/ 32]\t\tLoss: 0.5769 Acc@1: 93.750% \n",
            "Pseudo labeled: 28.181% (1010/3584)    \tCorrect pseudo label: 80.99% (818/1010)    \tSame output weak and strong augmentation: 86.93% (878/1010)\n",
            "| Epoch [ 40/100] Iter[ 31/ 32]\t\tLoss: 0.8382 Acc@1: 93.044% \n",
            "Pseudo labeled: 28.125% (1953/6944)    \tCorrect pseudo label: 80.44% (1571/1953)    \tSame output weak and strong augmentation: 84.54% (1651/1953)\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 2.7712 Acc@1: 51.15%\n",
            "\n",
            "=> Training Epoch #41, LR=0.0300\n",
            "| Epoch [ 41/100] Iter[  1/ 32]\t\tLoss: 1.0195 Acc@1: 96.875% \n",
            "Pseudo labeled: 27.232% (61/224)    \tCorrect pseudo label: 73.77% (45/61)    \tSame output weak and strong augmentation: 72.13% (44/61)\n",
            "| Epoch [ 41/100] Iter[ 16/ 32]\t\tLoss: 1.3650 Acc@1: 93.359% \n",
            "Pseudo labeled: 31.920% (1144/3584)    \tCorrect pseudo label: 77.71% (889/1144)    \tSame output weak and strong augmentation: 83.48% (955/1144)\n",
            "| Epoch [ 41/100] Iter[ 31/ 32]\t\tLoss: 0.7316 Acc@1: 92.137% \n",
            "Pseudo labeled: 31.351% (2177/6944)    \tCorrect pseudo label: 78.69% (1713/2177)    \tSame output weak and strong augmentation: 83.37% (1815/2177)\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 0.4980 Acc@1: 50.40%\n",
            "\n",
            "=> Training Epoch #42, LR=0.0300\n",
            "| Epoch [ 42/100] Iter[  1/ 32]\t\tLoss: 0.9651 Acc@1: 84.375% \n",
            "Pseudo labeled: 19.196% (43/224)    \tCorrect pseudo label: 81.40% (35/43)    \tSame output weak and strong augmentation: 81.40% (35/43)\n",
            "| Epoch [ 42/100] Iter[ 16/ 32]\t\tLoss: 1.4145 Acc@1: 92.969% \n",
            "Pseudo labeled: 28.181% (1010/3584)    \tCorrect pseudo label: 77.72% (785/1010)    \tSame output weak and strong augmentation: 84.46% (853/1010)\n",
            "| Epoch [ 42/100] Iter[ 31/ 32]\t\tLoss: 1.2160 Acc@1: 91.129% \n",
            "Pseudo labeled: 27.938% (1940/6944)    \tCorrect pseudo label: 76.80% (1490/1940)    \tSame output weak and strong augmentation: 82.89% (1608/1940)\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 0.0902 Acc@1: 55.70%\n",
            "| Saving Best model...\t\t\tTop1 = 55.70%\n",
            "\n",
            "=> Training Epoch #43, LR=0.0300\n",
            "| Epoch [ 43/100] Iter[  1/ 32]\t\tLoss: 0.8713 Acc@1: 84.375% \n",
            "Pseudo labeled: 27.232% (61/224)    \tCorrect pseudo label: 72.13% (44/61)    \tSame output weak and strong augmentation: 88.52% (54/61)\n",
            "| Epoch [ 43/100] Iter[ 16/ 32]\t\tLoss: 0.5256 Acc@1: 94.727% \n",
            "Pseudo labeled: 28.544% (1023/3584)    \tCorrect pseudo label: 78.79% (806/1023)    \tSame output weak and strong augmentation: 84.95% (869/1023)\n",
            "| Epoch [ 43/100] Iter[ 31/ 32]\t\tLoss: 0.7552 Acc@1: 93.548% \n",
            "Pseudo labeled: 28.687% (1992/6944)    \tCorrect pseudo label: 78.77% (1569/1992)    \tSame output weak and strong augmentation: 83.23% (1658/1992)\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 0.7189 Acc@1: 53.70%\n",
            "\n",
            "=> Training Epoch #44, LR=0.0300\n",
            "| Epoch [ 44/100] Iter[  1/ 32]\t\tLoss: 0.7507 Acc@1: 96.875% \n",
            "Pseudo labeled: 28.125% (63/224)    \tCorrect pseudo label: 74.60% (47/63)    \tSame output weak and strong augmentation: 82.54% (52/63)\n",
            "| Epoch [ 44/100] Iter[ 16/ 32]\t\tLoss: 0.9466 Acc@1: 96.094% \n",
            "Pseudo labeled: 30.190% (1082/3584)    \tCorrect pseudo label: 80.13% (867/1082)    \tSame output weak and strong augmentation: 82.53% (893/1082)\n",
            "| Epoch [ 44/100] Iter[ 31/ 32]\t\tLoss: 0.7126 Acc@1: 94.456% \n",
            "Pseudo labeled: 30.991% (2152/6944)    \tCorrect pseudo label: 76.81% (1653/2152)    \tSame output weak and strong augmentation: 81.51% (1754/2152)\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 0.4303 Acc@1: 58.45%\n",
            "| Saving Best model...\t\t\tTop1 = 58.45%\n",
            "\n",
            "=> Training Epoch #45, LR=0.0300\n",
            "| Epoch [ 45/100] Iter[  1/ 32]\t\tLoss: 0.7370 Acc@1: 96.875% \n",
            "Pseudo labeled: 34.375% (77/224)    \tCorrect pseudo label: 81.82% (63/77)    \tSame output weak and strong augmentation: 75.32% (58/77)\n",
            "| Epoch [ 45/100] Iter[ 16/ 32]\t\tLoss: 0.8304 Acc@1: 96.680% \n",
            "Pseudo labeled: 30.525% (1094/3584)    \tCorrect pseudo label: 79.34% (868/1094)    \tSame output weak and strong augmentation: 80.90% (885/1094)\n",
            "| Epoch [ 45/100] Iter[ 31/ 32]\t\tLoss: 0.6163 Acc@1: 95.464% \n",
            "Pseudo labeled: 29.522% (2050/6944)    \tCorrect pseudo label: 78.73% (1614/2050)    \tSame output weak and strong augmentation: 81.76% (1676/2050)\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 0.2580 Acc@1: 55.90%\n",
            "\n",
            "=> Training Epoch #46, LR=0.0300\n",
            "| Epoch [ 46/100] Iter[  1/ 32]\t\tLoss: 0.8055 Acc@1: 93.750% \n",
            "Pseudo labeled: 32.589% (73/224)    \tCorrect pseudo label: 69.86% (51/73)    \tSame output weak and strong augmentation: 79.45% (58/73)\n",
            "| Epoch [ 46/100] Iter[ 16/ 32]\t\tLoss: 0.4971 Acc@1: 96.680% \n",
            "Pseudo labeled: 32.840% (1177/3584)    \tCorrect pseudo label: 78.16% (920/1177)    \tSame output weak and strong augmentation: 82.50% (971/1177)\n",
            "| Epoch [ 46/100] Iter[ 31/ 32]\t\tLoss: 0.8814 Acc@1: 95.262% \n",
            "Pseudo labeled: 31.984% (2221/6944)    \tCorrect pseudo label: 79.15% (1758/2221)    \tSame output weak and strong augmentation: 83.39% (1852/2221)\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 0.1794 Acc@1: 57.60%\n",
            "\n",
            "=> Training Epoch #47, LR=0.0300\n",
            "| Epoch [ 47/100] Iter[  1/ 32]\t\tLoss: 0.6393 Acc@1: 96.875% \n",
            "Pseudo labeled: 33.482% (75/224)    \tCorrect pseudo label: 80.00% (60/75)    \tSame output weak and strong augmentation: 82.67% (62/75)\n",
            "| Epoch [ 47/100] Iter[ 16/ 32]\t\tLoss: 0.6126 Acc@1: 96.289% \n",
            "Pseudo labeled: 33.147% (1188/3584)    \tCorrect pseudo label: 81.73% (971/1188)    \tSame output weak and strong augmentation: 84.26% (1001/1188)\n",
            "| Epoch [ 47/100] Iter[ 31/ 32]\t\tLoss: 0.6095 Acc@1: 94.657% \n",
            "Pseudo labeled: 33.324% (2314/6944)    \tCorrect pseudo label: 80.12% (1854/2314)    \tSame output weak and strong augmentation: 83.66% (1936/2314)\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 0.0160 Acc@1: 53.50%\n",
            "\n",
            "=> Training Epoch #48, LR=0.0300\n",
            "| Epoch [ 48/100] Iter[  1/ 32]\t\tLoss: 0.4784 Acc@1: 93.750% \n",
            "Pseudo labeled: 29.018% (65/224)    \tCorrect pseudo label: 73.85% (48/65)    \tSame output weak and strong augmentation: 92.31% (60/65)\n",
            "| Epoch [ 48/100] Iter[ 16/ 32]\t\tLoss: 0.7271 Acc@1: 96.875% \n",
            "Pseudo labeled: 32.896% (1179/3584)    \tCorrect pseudo label: 80.75% (952/1179)    \tSame output weak and strong augmentation: 84.48% (996/1179)\n",
            "| Epoch [ 48/100] Iter[ 31/ 32]\t\tLoss: 0.6951 Acc@1: 96.169% \n",
            "Pseudo labeled: 32.013% (2223/6944)    \tCorrect pseudo label: 80.93% (1799/2223)    \tSame output weak and strong augmentation: 83.90% (1865/2223)\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 0.0752 Acc@1: 56.90%\n",
            "\n",
            "=> Training Epoch #49, LR=0.0300\n",
            "| Epoch [ 49/100] Iter[  1/ 32]\t\tLoss: 0.7545 Acc@1: 93.750% \n",
            "Pseudo labeled: 33.036% (74/224)    \tCorrect pseudo label: 74.32% (55/74)    \tSame output weak and strong augmentation: 83.78% (62/74)\n",
            "| Epoch [ 49/100] Iter[ 16/ 32]\t\tLoss: 0.5952 Acc@1: 95.898% \n",
            "Pseudo labeled: 34.180% (1225/3584)    \tCorrect pseudo label: 80.73% (989/1225)    \tSame output weak and strong augmentation: 84.57% (1036/1225)\n",
            "| Epoch [ 49/100] Iter[ 31/ 32]\t\tLoss: 0.7122 Acc@1: 93.649% \n",
            "Pseudo labeled: 33.482% (2325/6944)    \tCorrect pseudo label: 78.62% (1828/2325)    \tSame output weak and strong augmentation: 83.57% (1943/2325)\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 0.4232 Acc@1: 52.40%\n",
            "\n",
            "=> Training Epoch #50, LR=0.0300\n",
            "| Epoch [ 50/100] Iter[  1/ 32]\t\tLoss: 0.9386 Acc@1: 90.625% \n",
            "Pseudo labeled: 26.339% (59/224)    \tCorrect pseudo label: 81.36% (48/59)    \tSame output weak and strong augmentation: 76.27% (45/59)\n",
            "| Epoch [ 50/100] Iter[ 16/ 32]\t\tLoss: 0.6530 Acc@1: 95.508% \n",
            "Pseudo labeled: 31.445% (1127/3584)    \tCorrect pseudo label: 80.12% (903/1127)    \tSame output weak and strong augmentation: 83.41% (940/1127)\n",
            "| Epoch [ 50/100] Iter[ 31/ 32]\t\tLoss: 0.5792 Acc@1: 96.573% \n",
            "Pseudo labeled: 32.733% (2273/6944)    \tCorrect pseudo label: 81.79% (1859/2273)    \tSame output weak and strong augmentation: 84.87% (1929/2273)\n",
            "\n",
            "| Validation Epoch #50\t\t\tLoss: 0.7160 Acc@1: 56.50%\n",
            "\n",
            "=> Training Epoch #51, LR=0.0300\n",
            "| Epoch [ 51/100] Iter[  1/ 32]\t\tLoss: 0.3093 Acc@1: 96.875% \n",
            "Pseudo labeled: 33.036% (74/224)    \tCorrect pseudo label: 79.73% (59/74)    \tSame output weak and strong augmentation: 94.59% (70/74)\n",
            "| Epoch [ 51/100] Iter[ 16/ 32]\t\tLoss: 0.7784 Acc@1: 97.461% \n",
            "Pseudo labeled: 35.686% (1279/3584)    \tCorrect pseudo label: 79.05% (1011/1279)    \tSame output weak and strong augmentation: 85.77% (1097/1279)\n",
            "| Epoch [ 51/100] Iter[ 31/ 32]\t\tLoss: 0.5474 Acc@1: 95.665% \n",
            "Pseudo labeled: 34.764% (2414/6944)    \tCorrect pseudo label: 77.63% (1874/2414)    \tSame output weak and strong augmentation: 85.09% (2054/2414)\n",
            "\n",
            "| Validation Epoch #51\t\t\tLoss: 2.3434 Acc@1: 45.90%\n",
            "\n",
            "=> Training Epoch #52, LR=0.0300\n",
            "| Epoch [ 52/100] Iter[  1/ 32]\t\tLoss: 0.6045 Acc@1: 90.625% \n",
            "Pseudo labeled: 35.268% (79/224)    \tCorrect pseudo label: 79.75% (63/79)    \tSame output weak and strong augmentation: 86.08% (68/79)\n",
            "| Epoch [ 52/100] Iter[ 16/ 32]\t\tLoss: 0.5134 Acc@1: 97.461% \n",
            "Pseudo labeled: 36.300% (1301/3584)    \tCorrect pseudo label: 81.86% (1065/1301)    \tSame output weak and strong augmentation: 84.09% (1094/1301)\n",
            "| Epoch [ 52/100] Iter[ 31/ 32]\t\tLoss: 0.6987 Acc@1: 97.177% \n",
            "Pseudo labeled: 35.397% (2458/6944)    \tCorrect pseudo label: 81.53% (2004/2458)    \tSame output weak and strong augmentation: 84.30% (2072/2458)\n",
            "\n",
            "| Validation Epoch #52\t\t\tLoss: 0.5821 Acc@1: 58.65%\n",
            "| Saving Best model...\t\t\tTop1 = 58.65%\n",
            "\n",
            "=> Training Epoch #53, LR=0.0300\n",
            "| Epoch [ 53/100] Iter[  1/ 32]\t\tLoss: 0.6138 Acc@1: 100.000% \n",
            "Pseudo labeled: 34.375% (77/224)    \tCorrect pseudo label: 77.92% (60/77)    \tSame output weak and strong augmentation: 80.52% (62/77)\n",
            "| Epoch [ 53/100] Iter[ 16/ 32]\t\tLoss: 0.6627 Acc@1: 98.047% \n",
            "Pseudo labeled: 34.403% (1233/3584)    \tCorrect pseudo label: 81.18% (1001/1233)    \tSame output weak and strong augmentation: 84.27% (1039/1233)\n",
            "| Epoch [ 53/100] Iter[ 31/ 32]\t\tLoss: 0.6802 Acc@1: 97.480% \n",
            "Pseudo labeled: 34.015% (2362/6944)    \tCorrect pseudo label: 80.44% (1900/2362)    \tSame output weak and strong augmentation: 84.29% (1991/2362)\n",
            "\n",
            "| Validation Epoch #53\t\t\tLoss: 0.5260 Acc@1: 56.60%\n",
            "\n",
            "=> Training Epoch #54, LR=0.0300\n",
            "| Epoch [ 54/100] Iter[  1/ 32]\t\tLoss: 0.4203 Acc@1: 96.875% \n",
            "Pseudo labeled: 37.500% (84/224)    \tCorrect pseudo label: 84.52% (71/84)    \tSame output weak and strong augmentation: 88.10% (74/84)\n",
            "| Epoch [ 54/100] Iter[ 16/ 32]\t\tLoss: 0.6465 Acc@1: 97.656% \n",
            "Pseudo labeled: 35.045% (1256/3584)    \tCorrect pseudo label: 81.61% (1025/1256)    \tSame output weak and strong augmentation: 84.63% (1063/1256)\n",
            "| Epoch [ 54/100] Iter[ 31/ 32]\t\tLoss: 0.5533 Acc@1: 97.077% \n",
            "Pseudo labeled: 34.317% (2383/6944)    \tCorrect pseudo label: 81.37% (1939/2383)    \tSame output weak and strong augmentation: 84.52% (2014/2383)\n",
            "\n",
            "| Validation Epoch #54\t\t\tLoss: 0.0254 Acc@1: 60.15%\n",
            "| Saving Best model...\t\t\tTop1 = 60.15%\n",
            "\n",
            "=> Training Epoch #55, LR=0.0300\n",
            "| Epoch [ 55/100] Iter[  1/ 32]\t\tLoss: 0.5732 Acc@1: 100.000% \n",
            "Pseudo labeled: 30.804% (69/224)    \tCorrect pseudo label: 76.81% (53/69)    \tSame output weak and strong augmentation: 86.96% (60/69)\n",
            "| Epoch [ 55/100] Iter[ 16/ 32]\t\tLoss: 0.7097 Acc@1: 97.461% \n",
            "Pseudo labeled: 35.491% (1272/3584)    \tCorrect pseudo label: 80.42% (1023/1272)    \tSame output weak and strong augmentation: 83.81% (1066/1272)\n",
            "| Epoch [ 55/100] Iter[ 31/ 32]\t\tLoss: 1.0830 Acc@1: 96.774% \n",
            "Pseudo labeled: 34.749% (2413/6944)    \tCorrect pseudo label: 80.02% (1931/2413)    \tSame output weak and strong augmentation: 83.05% (2004/2413)\n",
            "\n",
            "| Validation Epoch #55\t\t\tLoss: 0.4377 Acc@1: 56.80%\n",
            "\n",
            "=> Training Epoch #56, LR=0.0300\n",
            "| Epoch [ 56/100] Iter[  1/ 32]\t\tLoss: 0.6421 Acc@1: 96.875% \n",
            "Pseudo labeled: 36.607% (82/224)    \tCorrect pseudo label: 79.27% (65/82)    \tSame output weak and strong augmentation: 79.27% (65/82)\n",
            "| Epoch [ 56/100] Iter[ 16/ 32]\t\tLoss: 0.7120 Acc@1: 98.047% \n",
            "Pseudo labeled: 35.296% (1265/3584)    \tCorrect pseudo label: 80.55% (1019/1265)    \tSame output weak and strong augmentation: 84.27% (1066/1265)\n",
            "| Epoch [ 56/100] Iter[ 31/ 32]\t\tLoss: 0.5748 Acc@1: 97.681% \n",
            "Pseudo labeled: 34.778% (2415/6944)    \tCorrect pseudo label: 79.63% (1923/2415)    \tSame output weak and strong augmentation: 83.69% (2021/2415)\n",
            "\n",
            "| Validation Epoch #56\t\t\tLoss: 0.2259 Acc@1: 53.50%\n",
            "\n",
            "=> Training Epoch #57, LR=0.0300\n",
            "| Epoch [ 57/100] Iter[  1/ 32]\t\tLoss: 0.4030 Acc@1: 96.875% \n",
            "Pseudo labeled: 37.054% (83/224)    \tCorrect pseudo label: 80.72% (67/83)    \tSame output weak and strong augmentation: 87.95% (73/83)\n",
            "| Epoch [ 57/100] Iter[ 16/ 32]\t\tLoss: 0.4046 Acc@1: 98.438% \n",
            "Pseudo labeled: 35.156% (1260/3584)    \tCorrect pseudo label: 81.11% (1022/1260)    \tSame output weak and strong augmentation: 86.83% (1094/1260)\n",
            "| Epoch [ 57/100] Iter[ 31/ 32]\t\tLoss: 0.5828 Acc@1: 97.984% \n",
            "Pseudo labeled: 36.766% (2553/6944)    \tCorrect pseudo label: 79.95% (2041/2553)    \tSame output weak and strong augmentation: 85.74% (2189/2553)\n",
            "\n",
            "| Validation Epoch #57\t\t\tLoss: 0.6559 Acc@1: 53.30%\n",
            "\n",
            "=> Training Epoch #58, LR=0.0300\n",
            "| Epoch [ 58/100] Iter[  1/ 32]\t\tLoss: 0.8763 Acc@1: 100.000% \n",
            "Pseudo labeled: 25.000% (56/224)    \tCorrect pseudo label: 87.50% (49/56)    \tSame output weak and strong augmentation: 66.07% (37/56)\n",
            "| Epoch [ 58/100] Iter[ 16/ 32]\t\tLoss: 0.5662 Acc@1: 98.438% \n",
            "Pseudo labeled: 33.566% (1203/3584)    \tCorrect pseudo label: 81.13% (976/1203)    \tSame output weak and strong augmentation: 83.79% (1008/1203)\n",
            "| Epoch [ 58/100] Iter[ 31/ 32]\t\tLoss: 0.4671 Acc@1: 98.286% \n",
            "Pseudo labeled: 35.858% (2490/6944)    \tCorrect pseudo label: 79.88% (1989/2490)    \tSame output weak and strong augmentation: 83.98% (2091/2490)\n",
            "\n",
            "| Validation Epoch #58\t\t\tLoss: 0.7525 Acc@1: 59.75%\n",
            "\n",
            "=> Training Epoch #59, LR=0.0300\n",
            "| Epoch [ 59/100] Iter[  1/ 32]\t\tLoss: 0.4014 Acc@1: 96.875% \n",
            "Pseudo labeled: 39.732% (89/224)    \tCorrect pseudo label: 80.90% (72/89)    \tSame output weak and strong augmentation: 86.52% (77/89)\n",
            "| Epoch [ 59/100] Iter[ 16/ 32]\t\tLoss: 0.7554 Acc@1: 97.266% \n",
            "Pseudo labeled: 37.919% (1359/3584)    \tCorrect pseudo label: 79.99% (1087/1359)    \tSame output weak and strong augmentation: 83.74% (1138/1359)\n",
            "| Epoch [ 59/100] Iter[ 31/ 32]\t\tLoss: 0.6069 Acc@1: 96.371% \n",
            "Pseudo labeled: 37.471% (2602/6944)    \tCorrect pseudo label: 79.98% (2081/2602)    \tSame output weak and strong augmentation: 84.78% (2206/2602)\n",
            "\n",
            "| Validation Epoch #59\t\t\tLoss: 0.4141 Acc@1: 52.85%\n",
            "\n",
            "=> Training Epoch #60, LR=0.0300\n",
            "| Epoch [ 60/100] Iter[  1/ 32]\t\tLoss: 0.4427 Acc@1: 96.875% \n",
            "Pseudo labeled: 37.946% (85/224)    \tCorrect pseudo label: 75.29% (64/85)    \tSame output weak and strong augmentation: 88.24% (75/85)\n",
            "| Epoch [ 60/100] Iter[ 16/ 32]\t\tLoss: 0.6821 Acc@1: 96.094% \n",
            "Pseudo labeled: 34.933% (1252/3584)    \tCorrect pseudo label: 76.68% (960/1252)    \tSame output weak and strong augmentation: 83.95% (1051/1252)\n",
            "| Epoch [ 60/100] Iter[ 31/ 32]\t\tLoss: 1.0150 Acc@1: 95.161% \n",
            "Pseudo labeled: 34.245% (2378/6944)    \tCorrect pseudo label: 79.23% (1884/2378)    \tSame output weak and strong augmentation: 83.94% (1996/2378)\n",
            "\n",
            "| Validation Epoch #60\t\t\tLoss: 0.4807 Acc@1: 55.00%\n",
            "\n",
            "=> Training Epoch #61, LR=0.0300\n",
            "| Epoch [ 61/100] Iter[  1/ 32]\t\tLoss: 0.4454 Acc@1: 96.875% \n",
            "Pseudo labeled: 36.607% (82/224)    \tCorrect pseudo label: 68.29% (56/82)    \tSame output weak and strong augmentation: 84.15% (69/82)\n",
            "| Epoch [ 61/100] Iter[ 16/ 32]\t\tLoss: 0.6395 Acc@1: 96.875% \n",
            "Pseudo labeled: 37.528% (1345/3584)    \tCorrect pseudo label: 78.96% (1062/1345)    \tSame output weak and strong augmentation: 84.31% (1134/1345)\n",
            "| Epoch [ 61/100] Iter[ 31/ 32]\t\tLoss: 0.6764 Acc@1: 96.573% \n",
            "Pseudo labeled: 36.046% (2503/6944)    \tCorrect pseudo label: 78.51% (1965/2503)    \tSame output weak and strong augmentation: 83.70% (2095/2503)\n",
            "\n",
            "| Validation Epoch #61\t\t\tLoss: 0.1190 Acc@1: 58.10%\n",
            "\n",
            "=> Training Epoch #62, LR=0.0300\n",
            "| Epoch [ 62/100] Iter[  1/ 32]\t\tLoss: 0.5826 Acc@1: 100.000% \n",
            "Pseudo labeled: 35.714% (80/224)    \tCorrect pseudo label: 82.50% (66/80)    \tSame output weak and strong augmentation: 81.25% (65/80)\n",
            "| Epoch [ 62/100] Iter[ 16/ 32]\t\tLoss: 0.3171 Acc@1: 98.438% \n",
            "Pseudo labeled: 37.974% (1361/3584)    \tCorrect pseudo label: 82.22% (1119/1361)    \tSame output weak and strong augmentation: 87.44% (1190/1361)\n",
            "| Epoch [ 62/100] Iter[ 31/ 32]\t\tLoss: 0.6800 Acc@1: 98.891% \n",
            "Pseudo labeled: 37.010% (2570/6944)    \tCorrect pseudo label: 82.14% (2111/2570)    \tSame output weak and strong augmentation: 85.99% (2210/2570)\n",
            "\n",
            "| Validation Epoch #62\t\t\tLoss: 0.1477 Acc@1: 61.40%\n",
            "| Saving Best model...\t\t\tTop1 = 61.40%\n",
            "\n",
            "=> Training Epoch #63, LR=0.0300\n",
            "| Epoch [ 63/100] Iter[  1/ 32]\t\tLoss: 0.3254 Acc@1: 100.000% \n",
            "Pseudo labeled: 40.179% (90/224)    \tCorrect pseudo label: 82.22% (74/90)    \tSame output weak and strong augmentation: 93.33% (84/90)\n",
            "| Epoch [ 63/100] Iter[ 16/ 32]\t\tLoss: 0.3863 Acc@1: 98.438% \n",
            "Pseudo labeled: 35.547% (1274/3584)    \tCorrect pseudo label: 81.95% (1044/1274)    \tSame output weak and strong augmentation: 84.69% (1079/1274)\n",
            "| Epoch [ 63/100] Iter[ 31/ 32]\t\tLoss: 0.5865 Acc@1: 97.883% \n",
            "Pseudo labeled: 36.406% (2528/6944)    \tCorrect pseudo label: 79.39% (2007/2528)    \tSame output weak and strong augmentation: 83.94% (2122/2528)\n",
            "\n",
            "| Validation Epoch #63\t\t\tLoss: 0.3426 Acc@1: 59.75%\n",
            "\n",
            "=> Training Epoch #64, LR=0.0300\n",
            "| Epoch [ 64/100] Iter[  1/ 32]\t\tLoss: 0.4057 Acc@1: 100.000% \n",
            "Pseudo labeled: 40.625% (91/224)    \tCorrect pseudo label: 89.01% (81/91)    \tSame output weak and strong augmentation: 84.62% (77/91)\n",
            "| Epoch [ 64/100] Iter[ 16/ 32]\t\tLoss: 0.4914 Acc@1: 97.461% \n",
            "Pseudo labeled: 38.504% (1380/3584)    \tCorrect pseudo label: 81.01% (1118/1380)    \tSame output weak and strong augmentation: 82.90% (1144/1380)\n",
            "| Epoch [ 64/100] Iter[ 31/ 32]\t\tLoss: 0.4659 Acc@1: 97.581% \n",
            "Pseudo labeled: 37.860% (2629/6944)    \tCorrect pseudo label: 80.11% (2106/2629)    \tSame output weak and strong augmentation: 83.45% (2194/2629)\n",
            "\n",
            "| Validation Epoch #64\t\t\tLoss: 0.7540 Acc@1: 58.95%\n",
            "\n",
            "=> Training Epoch #65, LR=0.0300\n",
            "| Epoch [ 65/100] Iter[  1/ 32]\t\tLoss: 0.5339 Acc@1: 100.000% \n",
            "Pseudo labeled: 38.839% (87/224)    \tCorrect pseudo label: 86.21% (75/87)    \tSame output weak and strong augmentation: 87.36% (76/87)\n",
            "| Epoch [ 65/100] Iter[ 16/ 32]\t\tLoss: 0.7192 Acc@1: 99.023% \n",
            "Pseudo labeled: 39.927% (1431/3584)    \tCorrect pseudo label: 82.74% (1184/1431)    \tSame output weak and strong augmentation: 86.37% (1236/1431)\n",
            "| Epoch [ 65/100] Iter[ 31/ 32]\t\tLoss: 0.6788 Acc@1: 98.690% \n",
            "Pseudo labeled: 38.004% (2639/6944)    \tCorrect pseudo label: 81.36% (2147/2639)    \tSame output weak and strong augmentation: 85.71% (2262/2639)\n",
            "\n",
            "| Validation Epoch #65\t\t\tLoss: 0.1317 Acc@1: 59.95%\n",
            "\n",
            "=> Training Epoch #66, LR=0.0300\n",
            "| Epoch [ 66/100] Iter[  1/ 32]\t\tLoss: 0.3209 Acc@1: 96.875% \n",
            "Pseudo labeled: 41.071% (92/224)    \tCorrect pseudo label: 83.70% (77/92)    \tSame output weak and strong augmentation: 91.30% (84/92)\n",
            "| Epoch [ 66/100] Iter[ 16/ 32]\t\tLoss: 0.4355 Acc@1: 98.242% \n",
            "Pseudo labeled: 39.286% (1408/3584)    \tCorrect pseudo label: 80.68% (1136/1408)    \tSame output weak and strong augmentation: 86.93% (1224/1408)\n",
            "| Epoch [ 66/100] Iter[ 31/ 32]\t\tLoss: 1.0728 Acc@1: 97.581% \n",
            "Pseudo labeled: 39.574% (2748/6944)    \tCorrect pseudo label: 80.49% (2212/2748)    \tSame output weak and strong augmentation: 86.03% (2364/2748)\n",
            "\n",
            "| Validation Epoch #66\t\t\tLoss: 0.1364 Acc@1: 61.20%\n",
            "\n",
            "=> Training Epoch #67, LR=0.0300\n",
            "| Epoch [ 67/100] Iter[  1/ 32]\t\tLoss: 0.5077 Acc@1: 100.000% \n",
            "Pseudo labeled: 39.286% (88/224)    \tCorrect pseudo label: 82.95% (73/88)    \tSame output weak and strong augmentation: 82.95% (73/88)\n",
            "| Epoch [ 67/100] Iter[ 16/ 32]\t\tLoss: 0.6785 Acc@1: 99.023% \n",
            "Pseudo labeled: 41.099% (1473/3584)    \tCorrect pseudo label: 80.72% (1189/1473)    \tSame output weak and strong augmentation: 83.91% (1236/1473)\n",
            "| Epoch [ 67/100] Iter[ 31/ 32]\t\tLoss: 0.5548 Acc@1: 97.883% \n",
            "Pseudo labeled: 39.689% (2756/6944)    \tCorrect pseudo label: 79.90% (2202/2756)    \tSame output weak and strong augmentation: 84.43% (2327/2756)\n",
            "\n",
            "| Validation Epoch #67\t\t\tLoss: 0.3083 Acc@1: 60.80%\n",
            "\n",
            "=> Training Epoch #68, LR=0.0300\n",
            "| Epoch [ 68/100] Iter[  1/ 32]\t\tLoss: 0.3862 Acc@1: 100.000% \n",
            "Pseudo labeled: 42.411% (95/224)    \tCorrect pseudo label: 80.00% (76/95)    \tSame output weak and strong augmentation: 88.42% (84/95)\n",
            "| Epoch [ 68/100] Iter[ 16/ 32]\t\tLoss: 0.5628 Acc@1: 99.023% \n",
            "Pseudo labeled: 39.927% (1431/3584)    \tCorrect pseudo label: 82.46% (1180/1431)    \tSame output weak and strong augmentation: 86.44% (1237/1431)\n",
            "| Epoch [ 68/100] Iter[ 31/ 32]\t\tLoss: 0.4657 Acc@1: 98.286% \n",
            "Pseudo labeled: 39.257% (2726/6944)    \tCorrect pseudo label: 81.77% (2229/2726)    \tSame output weak and strong augmentation: 85.51% (2331/2726)\n",
            "\n",
            "| Validation Epoch #68\t\t\tLoss: 0.1239 Acc@1: 61.60%\n",
            "| Saving Best model...\t\t\tTop1 = 61.60%\n",
            "\n",
            "=> Training Epoch #69, LR=0.0300\n",
            "| Epoch [ 69/100] Iter[  1/ 32]\t\tLoss: 0.6419 Acc@1: 100.000% \n",
            "Pseudo labeled: 38.839% (87/224)    \tCorrect pseudo label: 79.31% (69/87)    \tSame output weak and strong augmentation: 79.31% (69/87)\n",
            "| Epoch [ 69/100] Iter[ 16/ 32]\t\tLoss: 0.5403 Acc@1: 99.023% \n",
            "Pseudo labeled: 39.593% (1419/3584)    \tCorrect pseudo label: 80.83% (1147/1419)    \tSame output weak and strong augmentation: 84.99% (1206/1419)\n",
            "| Epoch [ 69/100] Iter[ 31/ 32]\t\tLoss: 0.6969 Acc@1: 97.984% \n",
            "Pseudo labeled: 39.660% (2754/6944)    \tCorrect pseudo label: 80.76% (2224/2754)    \tSame output weak and strong augmentation: 84.35% (2323/2754)\n",
            "\n",
            "| Validation Epoch #69\t\t\tLoss: 0.0452 Acc@1: 56.75%\n",
            "\n",
            "=> Training Epoch #70, LR=0.0300\n",
            "| Epoch [ 70/100] Iter[  1/ 32]\t\tLoss: 0.6153 Acc@1: 96.875% \n",
            "Pseudo labeled: 35.268% (79/224)    \tCorrect pseudo label: 82.28% (65/79)    \tSame output weak and strong augmentation: 74.68% (59/79)\n",
            "| Epoch [ 70/100] Iter[ 16/ 32]\t\tLoss: 0.3601 Acc@1: 99.219% \n",
            "Pseudo labeled: 38.811% (1391/3584)    \tCorrect pseudo label: 82.31% (1145/1391)    \tSame output weak and strong augmentation: 85.55% (1190/1391)\n",
            "| Epoch [ 70/100] Iter[ 31/ 32]\t\tLoss: 0.4953 Acc@1: 98.690% \n",
            "Pseudo labeled: 40.193% (2791/6944)    \tCorrect pseudo label: 81.66% (2279/2791)    \tSame output weak and strong augmentation: 86.21% (2406/2791)\n",
            "\n",
            "| Validation Epoch #70\t\t\tLoss: 0.0375 Acc@1: 60.25%\n",
            "\n",
            "=> Training Epoch #71, LR=0.0300\n",
            "| Epoch [ 71/100] Iter[  1/ 32]\t\tLoss: 0.4564 Acc@1: 96.875% \n",
            "Pseudo labeled: 33.929% (76/224)    \tCorrect pseudo label: 82.89% (63/76)    \tSame output weak and strong augmentation: 89.47% (68/76)\n",
            "| Epoch [ 71/100] Iter[ 16/ 32]\t\tLoss: 0.5049 Acc@1: 99.219% \n",
            "Pseudo labeled: 37.444% (1342/3584)    \tCorrect pseudo label: 82.27% (1104/1342)    \tSame output weak and strong augmentation: 86.89% (1166/1342)\n",
            "| Epoch [ 71/100] Iter[ 31/ 32]\t\tLoss: 0.5517 Acc@1: 98.387% \n",
            "Pseudo labeled: 38.868% (2699/6944)    \tCorrect pseudo label: 81.10% (2189/2699)    \tSame output weak and strong augmentation: 85.29% (2302/2699)\n",
            "\n",
            "| Validation Epoch #71\t\t\tLoss: 0.1033 Acc@1: 57.80%\n",
            "\n",
            "=> Training Epoch #72, LR=0.0300\n",
            "| Epoch [ 72/100] Iter[  1/ 32]\t\tLoss: 0.4493 Acc@1: 100.000% \n",
            "Pseudo labeled: 36.161% (81/224)    \tCorrect pseudo label: 67.90% (55/81)    \tSame output weak and strong augmentation: 86.42% (70/81)\n",
            "| Epoch [ 72/100] Iter[ 16/ 32]\t\tLoss: 0.4595 Acc@1: 98.633% \n",
            "Pseudo labeled: 36.189% (1297/3584)    \tCorrect pseudo label: 76.25% (989/1297)    \tSame output weak and strong augmentation: 83.81% (1087/1297)\n",
            "| Epoch [ 72/100] Iter[ 31/ 32]\t\tLoss: 0.6544 Acc@1: 98.690% \n",
            "Pseudo labeled: 36.953% (2566/6944)    \tCorrect pseudo label: 78.18% (2006/2566)    \tSame output weak and strong augmentation: 83.63% (2146/2566)\n",
            "\n",
            "| Validation Epoch #72\t\t\tLoss: 0.0167 Acc@1: 59.05%\n",
            "\n",
            "=> Training Epoch #73, LR=0.0300\n",
            "| Epoch [ 73/100] Iter[  1/ 32]\t\tLoss: 0.3441 Acc@1: 96.875% \n",
            "Pseudo labeled: 36.161% (81/224)    \tCorrect pseudo label: 91.36% (74/81)    \tSame output weak and strong augmentation: 86.42% (70/81)\n",
            "| Epoch [ 73/100] Iter[ 16/ 32]\t\tLoss: 0.4230 Acc@1: 98.438% \n",
            "Pseudo labeled: 38.560% (1382/3584)    \tCorrect pseudo label: 81.69% (1129/1382)    \tSame output weak and strong augmentation: 85.60% (1183/1382)\n",
            "| Epoch [ 73/100] Iter[ 31/ 32]\t\tLoss: 0.7372 Acc@1: 98.790% \n",
            "Pseudo labeled: 39.257% (2726/6944)    \tCorrect pseudo label: 81.51% (2222/2726)    \tSame output weak and strong augmentation: 85.55% (2332/2726)\n",
            "\n",
            "| Validation Epoch #73\t\t\tLoss: 0.4678 Acc@1: 57.05%\n",
            "\n",
            "=> Training Epoch #74, LR=0.0300\n",
            "| Epoch [ 74/100] Iter[  1/ 32]\t\tLoss: 0.5864 Acc@1: 96.875% \n",
            "Pseudo labeled: 41.964% (94/224)    \tCorrect pseudo label: 80.85% (76/94)    \tSame output weak and strong augmentation: 82.98% (78/94)\n",
            "| Epoch [ 74/100] Iter[ 16/ 32]\t\tLoss: 0.2569 Acc@1: 99.023% \n",
            "Pseudo labeled: 36.496% (1308/3584)    \tCorrect pseudo label: 86.31% (1129/1308)    \tSame output weak and strong augmentation: 86.47% (1131/1308)\n",
            "| Epoch [ 74/100] Iter[ 31/ 32]\t\tLoss: 0.4345 Acc@1: 98.992% \n",
            "Pseudo labeled: 38.666% (2685/6944)    \tCorrect pseudo label: 83.50% (2242/2685)    \tSame output weak and strong augmentation: 87.00% (2336/2685)\n",
            "\n",
            "| Validation Epoch #74\t\t\tLoss: 0.9206 Acc@1: 51.35%\n",
            "\n",
            "=> Training Epoch #75, LR=0.0300\n",
            "| Epoch [ 75/100] Iter[  1/ 32]\t\tLoss: 0.6490 Acc@1: 93.750% \n",
            "Pseudo labeled: 42.857% (96/224)    \tCorrect pseudo label: 85.42% (82/96)    \tSame output weak and strong augmentation: 84.38% (81/96)\n",
            "| Epoch [ 75/100] Iter[ 16/ 32]\t\tLoss: 0.4489 Acc@1: 98.828% \n",
            "Pseudo labeled: 37.640% (1349/3584)    \tCorrect pseudo label: 82.13% (1108/1349)    \tSame output weak and strong augmentation: 83.91% (1132/1349)\n",
            "| Epoch [ 75/100] Iter[ 31/ 32]\t\tLoss: 0.5033 Acc@1: 98.286% \n",
            "Pseudo labeled: 39.415% (2737/6944)    \tCorrect pseudo label: 81.55% (2232/2737)    \tSame output weak and strong augmentation: 85.86% (2350/2737)\n",
            "\n",
            "| Validation Epoch #75\t\t\tLoss: 0.6652 Acc@1: 60.25%\n",
            "\n",
            "=> Training Epoch #76, LR=0.0300\n",
            "| Epoch [ 76/100] Iter[  1/ 32]\t\tLoss: 0.5681 Acc@1: 96.875% \n",
            "Pseudo labeled: 41.071% (92/224)    \tCorrect pseudo label: 79.35% (73/92)    \tSame output weak and strong augmentation: 84.78% (78/92)\n",
            "| Epoch [ 76/100] Iter[ 16/ 32]\t\tLoss: 0.3653 Acc@1: 99.219% \n",
            "Pseudo labeled: 39.760% (1425/3584)    \tCorrect pseudo label: 82.95% (1182/1425)    \tSame output weak and strong augmentation: 85.61% (1220/1425)\n",
            "| Epoch [ 76/100] Iter[ 31/ 32]\t\tLoss: 0.4900 Acc@1: 98.286% \n",
            "Pseudo labeled: 39.401% (2736/6944)    \tCorrect pseudo label: 81.80% (2238/2736)    \tSame output weak and strong augmentation: 85.09% (2328/2736)\n",
            "\n",
            "| Validation Epoch #76\t\t\tLoss: 0.0277 Acc@1: 62.10%\n",
            "| Saving Best model...\t\t\tTop1 = 62.10%\n",
            "\n",
            "=> Training Epoch #77, LR=0.0300\n",
            "| Epoch [ 77/100] Iter[  1/ 32]\t\tLoss: 0.5288 Acc@1: 100.000% \n",
            "Pseudo labeled: 38.393% (86/224)    \tCorrect pseudo label: 83.72% (72/86)    \tSame output weak and strong augmentation: 81.40% (70/86)\n",
            "| Epoch [ 77/100] Iter[ 16/ 32]\t\tLoss: 0.6980 Acc@1: 99.219% \n",
            "Pseudo labeled: 39.704% (1423/3584)    \tCorrect pseudo label: 82.50% (1174/1423)    \tSame output weak and strong augmentation: 85.66% (1219/1423)\n",
            "| Epoch [ 77/100] Iter[ 31/ 32]\t\tLoss: 0.4149 Acc@1: 99.294% \n",
            "Pseudo labeled: 40.006% (2778/6944)    \tCorrect pseudo label: 81.75% (2271/2778)    \tSame output weak and strong augmentation: 85.85% (2385/2778)\n",
            "\n",
            "| Validation Epoch #77\t\t\tLoss: 0.0853 Acc@1: 61.40%\n",
            "\n",
            "=> Training Epoch #78, LR=0.0300\n",
            "| Epoch [ 78/100] Iter[  1/ 32]\t\tLoss: 0.4088 Acc@1: 100.000% \n",
            "Pseudo labeled: 35.714% (80/224)    \tCorrect pseudo label: 81.25% (65/80)    \tSame output weak and strong augmentation: 91.25% (73/80)\n",
            "| Epoch [ 78/100] Iter[ 16/ 32]\t\tLoss: 0.4894 Acc@1: 98.633% \n",
            "Pseudo labeled: 38.867% (1393/3584)    \tCorrect pseudo label: 81.41% (1134/1393)    \tSame output weak and strong augmentation: 86.86% (1210/1393)\n",
            "| Epoch [ 78/100] Iter[ 31/ 32]\t\tLoss: 0.4710 Acc@1: 98.589% \n",
            "Pseudo labeled: 39.559% (2747/6944)    \tCorrect pseudo label: 81.54% (2240/2747)    \tSame output weak and strong augmentation: 85.77% (2356/2747)\n",
            "\n",
            "| Validation Epoch #78\t\t\tLoss: 0.3477 Acc@1: 58.35%\n",
            "\n",
            "=> Training Epoch #79, LR=0.0300\n",
            "| Epoch [ 79/100] Iter[  1/ 32]\t\tLoss: 0.3909 Acc@1: 93.750% \n",
            "Pseudo labeled: 37.946% (85/224)    \tCorrect pseudo label: 82.35% (70/85)    \tSame output weak and strong augmentation: 90.59% (77/85)\n",
            "| Epoch [ 79/100] Iter[ 16/ 32]\t\tLoss: 0.3532 Acc@1: 99.023% \n",
            "Pseudo labeled: 41.518% (1488/3584)    \tCorrect pseudo label: 83.74% (1246/1488)    \tSame output weak and strong augmentation: 86.09% (1281/1488)\n",
            "| Epoch [ 79/100] Iter[ 31/ 32]\t\tLoss: 0.4169 Acc@1: 99.093% \n",
            "Pseudo labeled: 41.215% (2862/6944)    \tCorrect pseudo label: 82.74% (2368/2862)    \tSame output weak and strong augmentation: 86.65% (2480/2862)\n",
            "\n",
            "| Validation Epoch #79\t\t\tLoss: 0.3616 Acc@1: 62.65%\n",
            "| Saving Best model...\t\t\tTop1 = 62.65%\n",
            "\n",
            "=> Training Epoch #80, LR=0.0300\n",
            "| Epoch [ 80/100] Iter[  1/ 32]\t\tLoss: 0.3466 Acc@1: 100.000% \n",
            "Pseudo labeled: 41.071% (92/224)    \tCorrect pseudo label: 88.04% (81/92)    \tSame output weak and strong augmentation: 90.22% (83/92)\n",
            "| Epoch [ 80/100] Iter[ 16/ 32]\t\tLoss: 0.3952 Acc@1: 98.047% \n",
            "Pseudo labeled: 43.750% (1568/3584)    \tCorrect pseudo label: 83.61% (1311/1568)    \tSame output weak and strong augmentation: 89.48% (1403/1568)\n",
            "| Epoch [ 80/100] Iter[ 31/ 32]\t\tLoss: 0.4934 Acc@1: 98.185% \n",
            "Pseudo labeled: 43.275% (3005/6944)    \tCorrect pseudo label: 82.16% (2469/3005)    \tSame output weak and strong augmentation: 87.65% (2634/3005)\n",
            "\n",
            "| Validation Epoch #80\t\t\tLoss: 0.6211 Acc@1: 56.55%\n",
            "\n",
            "=> Training Epoch #81, LR=0.0030\n",
            "| Epoch [ 81/100] Iter[  1/ 32]\t\tLoss: 0.2986 Acc@1: 100.000% \n",
            "Pseudo labeled: 36.161% (81/224)    \tCorrect pseudo label: 86.42% (70/81)    \tSame output weak and strong augmentation: 90.12% (73/81)\n",
            "| Epoch [ 81/100] Iter[ 16/ 32]\t\tLoss: 0.2520 Acc@1: 99.023% \n",
            "Pseudo labeled: 39.593% (1419/3584)    \tCorrect pseudo label: 82.52% (1171/1419)    \tSame output weak and strong augmentation: 87.53% (1242/1419)\n",
            "| Epoch [ 81/100] Iter[ 31/ 32]\t\tLoss: 0.4664 Acc@1: 99.395% \n",
            "Pseudo labeled: 40.366% (2803/6944)    \tCorrect pseudo label: 82.63% (2316/2803)    \tSame output weak and strong augmentation: 87.12% (2442/2803)\n",
            "\n",
            "| Validation Epoch #81\t\t\tLoss: 0.1119 Acc@1: 63.50%\n",
            "| Saving Best model...\t\t\tTop1 = 63.50%\n",
            "\n",
            "=> Training Epoch #82, LR=0.0030\n",
            "| Epoch [ 82/100] Iter[  1/ 32]\t\tLoss: 0.3938 Acc@1: 100.000% \n",
            "Pseudo labeled: 41.964% (94/224)    \tCorrect pseudo label: 78.72% (74/94)    \tSame output weak and strong augmentation: 87.23% (82/94)\n",
            "| Epoch [ 82/100] Iter[ 16/ 32]\t\tLoss: 0.5144 Acc@1: 99.414% \n",
            "Pseudo labeled: 40.123% (1438/3584)    \tCorrect pseudo label: 81.78% (1176/1438)    \tSame output weak and strong augmentation: 86.44% (1243/1438)\n",
            "| Epoch [ 82/100] Iter[ 31/ 32]\t\tLoss: 0.2712 Acc@1: 99.597% \n",
            "Pseudo labeled: 40.524% (2814/6944)    \tCorrect pseudo label: 81.88% (2304/2814)    \tSame output weak and strong augmentation: 86.53% (2435/2814)\n",
            "\n",
            "| Validation Epoch #82\t\t\tLoss: 0.1270 Acc@1: 64.65%\n",
            "| Saving Best model...\t\t\tTop1 = 64.65%\n",
            "\n",
            "=> Training Epoch #83, LR=0.0030\n",
            "| Epoch [ 83/100] Iter[  1/ 32]\t\tLoss: 0.2061 Acc@1: 100.000% \n",
            "Pseudo labeled: 41.964% (94/224)    \tCorrect pseudo label: 76.60% (72/94)    \tSame output weak and strong augmentation: 95.74% (90/94)\n",
            "| Epoch [ 83/100] Iter[ 16/ 32]\t\tLoss: 0.4051 Acc@1: 100.000% \n",
            "Pseudo labeled: 42.941% (1539/3584)    \tCorrect pseudo label: 82.13% (1264/1539)    \tSame output weak and strong augmentation: 87.46% (1346/1539)\n",
            "| Epoch [ 83/100] Iter[ 31/ 32]\t\tLoss: 0.2258 Acc@1: 99.798% \n",
            "Pseudo labeled: 42.382% (2943/6944)    \tCorrect pseudo label: 82.50% (2428/2943)    \tSame output weak and strong augmentation: 87.94% (2588/2943)\n",
            "\n",
            "| Validation Epoch #83\t\t\tLoss: 0.0985 Acc@1: 64.45%\n",
            "\n",
            "=> Training Epoch #84, LR=0.0030\n",
            "| Epoch [ 84/100] Iter[  1/ 32]\t\tLoss: 0.2154 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.196% (99/224)    \tCorrect pseudo label: 83.84% (83/99)    \tSame output weak and strong augmentation: 92.93% (92/99)\n",
            "| Epoch [ 84/100] Iter[ 16/ 32]\t\tLoss: 0.3008 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.555% (1561/3584)    \tCorrect pseudo label: 82.58% (1289/1561)    \tSame output weak and strong augmentation: 87.83% (1371/1561)\n",
            "| Epoch [ 84/100] Iter[ 31/ 32]\t\tLoss: 0.4039 Acc@1: 100.000% \n",
            "Pseudo labeled: 42.886% (2978/6944)    \tCorrect pseudo label: 83.04% (2473/2978)    \tSame output weak and strong augmentation: 87.98% (2620/2978)\n",
            "\n",
            "| Validation Epoch #84\t\t\tLoss: 0.1174 Acc@1: 64.80%\n",
            "| Saving Best model...\t\t\tTop1 = 64.80%\n",
            "\n",
            "=> Training Epoch #85, LR=0.0030\n",
            "| Epoch [ 85/100] Iter[  1/ 32]\t\tLoss: 0.3415 Acc@1: 100.000% \n",
            "Pseudo labeled: 42.857% (96/224)    \tCorrect pseudo label: 82.29% (79/96)    \tSame output weak and strong augmentation: 88.54% (85/96)\n",
            "| Epoch [ 85/100] Iter[ 16/ 32]\t\tLoss: 0.3753 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.025% (1542/3584)    \tCorrect pseudo label: 82.94% (1279/1542)    \tSame output weak and strong augmentation: 87.94% (1356/1542)\n",
            "| Epoch [ 85/100] Iter[ 31/ 32]\t\tLoss: 0.4228 Acc@1: 99.899% \n",
            "Pseudo labeled: 43.332% (3009/6944)    \tCorrect pseudo label: 82.25% (2475/3009)    \tSame output weak and strong augmentation: 87.47% (2632/3009)\n",
            "\n",
            "| Validation Epoch #85\t\t\tLoss: 0.1305 Acc@1: 64.50%\n",
            "\n",
            "=> Training Epoch #86, LR=0.0030\n",
            "| Epoch [ 86/100] Iter[  1/ 32]\t\tLoss: 0.3221 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.196% (99/224)    \tCorrect pseudo label: 89.90% (89/99)    \tSame output weak and strong augmentation: 87.88% (87/99)\n",
            "| Epoch [ 86/100] Iter[ 16/ 32]\t\tLoss: 0.3511 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.331% (1553/3584)    \tCorrect pseudo label: 85.32% (1325/1553)    \tSame output weak and strong augmentation: 87.89% (1365/1553)\n",
            "| Epoch [ 86/100] Iter[ 31/ 32]\t\tLoss: 0.3221 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.520% (3022/6944)    \tCorrect pseudo label: 85.27% (2577/3022)    \tSame output weak and strong augmentation: 87.52% (2645/3022)\n",
            "\n",
            "| Validation Epoch #86\t\t\tLoss: 0.0955 Acc@1: 65.95%\n",
            "| Saving Best model...\t\t\tTop1 = 65.95%\n",
            "\n",
            "=> Training Epoch #87, LR=0.0030\n",
            "| Epoch [ 87/100] Iter[  1/ 32]\t\tLoss: 0.5166 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.750% (98/224)    \tCorrect pseudo label: 80.61% (79/98)    \tSame output weak and strong augmentation: 81.63% (80/98)\n",
            "| Epoch [ 87/100] Iter[ 16/ 32]\t\tLoss: 0.2840 Acc@1: 100.000% \n",
            "Pseudo labeled: 42.578% (1526/3584)    \tCorrect pseudo label: 82.83% (1264/1526)    \tSame output weak and strong augmentation: 87.75% (1339/1526)\n",
            "| Epoch [ 87/100] Iter[ 31/ 32]\t\tLoss: 0.3227 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.606% (3028/6944)    \tCorrect pseudo label: 83.85% (2539/3028)    \tSame output weak and strong augmentation: 87.71% (2656/3028)\n",
            "\n",
            "| Validation Epoch #87\t\t\tLoss: 0.0431 Acc@1: 66.20%\n",
            "| Saving Best model...\t\t\tTop1 = 66.20%\n",
            "\n",
            "=> Training Epoch #88, LR=0.0030\n",
            "| Epoch [ 88/100] Iter[  1/ 32]\t\tLoss: 0.3478 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.643% (100/224)    \tCorrect pseudo label: 84.00% (84/100)    \tSame output weak and strong augmentation: 89.00% (89/100)\n",
            "| Epoch [ 88/100] Iter[ 16/ 32]\t\tLoss: 0.4091 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.555% (1561/3584)    \tCorrect pseudo label: 82.90% (1294/1561)    \tSame output weak and strong augmentation: 87.25% (1362/1561)\n",
            "| Epoch [ 88/100] Iter[ 31/ 32]\t\tLoss: 0.4507 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.635% (3030/6944)    \tCorrect pseudo label: 83.17% (2520/3030)    \tSame output weak and strong augmentation: 86.83% (2631/3030)\n",
            "\n",
            "| Validation Epoch #88\t\t\tLoss: 0.0193 Acc@1: 65.75%\n",
            "\n",
            "=> Training Epoch #89, LR=0.0030\n",
            "| Epoch [ 89/100] Iter[  1/ 32]\t\tLoss: 0.1412 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.089% (101/224)    \tCorrect pseudo label: 87.13% (88/101)    \tSame output weak and strong augmentation: 97.03% (98/101)\n",
            "| Epoch [ 89/100] Iter[ 16/ 32]\t\tLoss: 0.3522 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.643% (1600/3584)    \tCorrect pseudo label: 83.12% (1330/1600)    \tSame output weak and strong augmentation: 87.75% (1404/1600)\n",
            "| Epoch [ 89/100] Iter[ 31/ 32]\t\tLoss: 0.2530 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.484% (3089/6944)    \tCorrect pseudo label: 83.26% (2572/3089)    \tSame output weak and strong augmentation: 87.25% (2695/3089)\n",
            "\n",
            "| Validation Epoch #89\t\t\tLoss: 0.1330 Acc@1: 65.60%\n",
            "\n",
            "=> Training Epoch #90, LR=0.0030\n",
            "| Epoch [ 90/100] Iter[  1/ 32]\t\tLoss: 0.4204 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.536% (102/224)    \tCorrect pseudo label: 81.37% (83/102)    \tSame output weak and strong augmentation: 89.22% (91/102)\n",
            "| Epoch [ 90/100] Iter[ 16/ 32]\t\tLoss: 0.3548 Acc@1: 99.805% \n",
            "Pseudo labeled: 45.145% (1618/3584)    \tCorrect pseudo label: 83.50% (1351/1618)    \tSame output weak and strong augmentation: 88.44% (1431/1618)\n",
            "| Epoch [ 90/100] Iter[ 31/ 32]\t\tLoss: 0.5579 Acc@1: 99.899% \n",
            "Pseudo labeled: 44.844% (3114/6944)    \tCorrect pseudo label: 82.56% (2571/3114)    \tSame output weak and strong augmentation: 87.60% (2728/3114)\n",
            "\n",
            "| Validation Epoch #90\t\t\tLoss: 0.1448 Acc@1: 64.35%\n",
            "\n",
            "=> Training Epoch #91, LR=0.0030\n",
            "| Epoch [ 91/100] Iter[  1/ 32]\t\tLoss: 0.3867 Acc@1: 100.000% \n",
            "Pseudo labeled: 40.179% (90/224)    \tCorrect pseudo label: 91.11% (82/90)    \tSame output weak and strong augmentation: 91.11% (82/90)\n",
            "| Epoch [ 91/100] Iter[ 16/ 32]\t\tLoss: 0.4482 Acc@1: 100.000% \n",
            "Pseudo labeled: 43.945% (1575/3584)    \tCorrect pseudo label: 83.24% (1311/1575)    \tSame output weak and strong augmentation: 87.37% (1376/1575)\n",
            "| Epoch [ 91/100] Iter[ 31/ 32]\t\tLoss: 0.3234 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.153% (3066/6944)    \tCorrect pseudo label: 82.45% (2528/3066)    \tSame output weak and strong augmentation: 87.02% (2668/3066)\n",
            "\n",
            "| Validation Epoch #91\t\t\tLoss: 0.1358 Acc@1: 63.95%\n",
            "\n",
            "=> Training Epoch #92, LR=0.0030\n",
            "| Epoch [ 92/100] Iter[  1/ 32]\t\tLoss: 0.3805 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.089% (101/224)    \tCorrect pseudo label: 91.09% (92/101)    \tSame output weak and strong augmentation: 88.12% (89/101)\n",
            "| Epoch [ 92/100] Iter[ 16/ 32]\t\tLoss: 0.2836 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.564% (1633/3584)    \tCorrect pseudo label: 83.53% (1364/1633)    \tSame output weak and strong augmentation: 88.67% (1448/1633)\n",
            "| Epoch [ 92/100] Iter[ 31/ 32]\t\tLoss: 0.4238 Acc@1: 99.899% \n",
            "Pseudo labeled: 45.190% (3138/6944)    \tCorrect pseudo label: 82.44% (2587/3138)    \tSame output weak and strong augmentation: 88.37% (2773/3138)\n",
            "\n",
            "| Validation Epoch #92\t\t\tLoss: 0.0149 Acc@1: 65.70%\n",
            "\n",
            "=> Training Epoch #93, LR=0.0030\n",
            "| Epoch [ 93/100] Iter[  1/ 32]\t\tLoss: 0.3603 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.643% (100/224)    \tCorrect pseudo label: 85.00% (85/100)    \tSame output weak and strong augmentation: 89.00% (89/100)\n",
            "| Epoch [ 93/100] Iter[ 16/ 32]\t\tLoss: 0.4162 Acc@1: 99.805% \n",
            "Pseudo labeled: 43.834% (1571/3584)    \tCorrect pseudo label: 83.39% (1310/1571)    \tSame output weak and strong augmentation: 88.80% (1395/1571)\n",
            "| Epoch [ 93/100] Iter[ 31/ 32]\t\tLoss: 0.3597 Acc@1: 99.899% \n",
            "Pseudo labeled: 44.859% (3115/6944)    \tCorrect pseudo label: 82.02% (2555/3115)    \tSame output weak and strong augmentation: 88.22% (2748/3115)\n",
            "\n",
            "| Validation Epoch #93\t\t\tLoss: 0.0632 Acc@1: 65.65%\n",
            "\n",
            "=> Training Epoch #94, LR=0.0030\n",
            "| Epoch [ 94/100] Iter[  1/ 32]\t\tLoss: 0.3316 Acc@1: 100.000% \n",
            "Pseudo labeled: 41.071% (92/224)    \tCorrect pseudo label: 88.04% (81/92)    \tSame output weak and strong augmentation: 86.96% (80/92)\n",
            "| Epoch [ 94/100] Iter[ 16/ 32]\t\tLoss: 0.3915 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.699% (1602/3584)    \tCorrect pseudo label: 83.27% (1334/1602)    \tSame output weak and strong augmentation: 88.33% (1415/1602)\n",
            "| Epoch [ 94/100] Iter[ 31/ 32]\t\tLoss: 0.3505 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.600% (3097/6944)    \tCorrect pseudo label: 82.60% (2558/3097)    \tSame output weak and strong augmentation: 88.80% (2750/3097)\n",
            "\n",
            "| Validation Epoch #94\t\t\tLoss: 0.0354 Acc@1: 66.35%\n",
            "| Saving Best model...\t\t\tTop1 = 66.35%\n",
            "\n",
            "=> Training Epoch #95, LR=0.0030\n",
            "| Epoch [ 95/100] Iter[  1/ 32]\t\tLoss: 0.3105 Acc@1: 100.000% \n",
            "Pseudo labeled: 50.000% (112/224)    \tCorrect pseudo label: 79.46% (89/112)    \tSame output weak and strong augmentation: 89.29% (100/112)\n",
            "| Epoch [ 95/100] Iter[ 16/ 32]\t\tLoss: 0.3253 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.810% (1606/3584)    \tCorrect pseudo label: 83.19% (1336/1606)    \tSame output weak and strong augmentation: 89.48% (1437/1606)\n",
            "| Epoch [ 95/100] Iter[ 31/ 32]\t\tLoss: 0.2979 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.089% (3131/6944)    \tCorrect pseudo label: 83.65% (2619/3131)    \tSame output weak and strong augmentation: 89.05% (2788/3131)\n",
            "\n",
            "| Validation Epoch #95\t\t\tLoss: 0.0416 Acc@1: 65.75%\n",
            "\n",
            "=> Training Epoch #96, LR=0.0030\n",
            "| Epoch [ 96/100] Iter[  1/ 32]\t\tLoss: 0.5293 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.536% (102/224)    \tCorrect pseudo label: 75.49% (77/102)    \tSame output weak and strong augmentation: 83.33% (85/102)\n",
            "| Epoch [ 96/100] Iter[ 16/ 32]\t\tLoss: 0.2619 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.866% (1608/3584)    \tCorrect pseudo label: 83.21% (1338/1608)    \tSame output weak and strong augmentation: 86.75% (1395/1608)\n",
            "| Epoch [ 96/100] Iter[ 31/ 32]\t\tLoss: 0.3551 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.196% (3069/6944)    \tCorrect pseudo label: 82.86% (2543/3069)    \tSame output weak and strong augmentation: 87.42% (2683/3069)\n",
            "\n",
            "| Validation Epoch #96\t\t\tLoss: 0.1690 Acc@1: 66.10%\n",
            "\n",
            "=> Training Epoch #97, LR=0.0030\n",
            "| Epoch [ 97/100] Iter[  1/ 32]\t\tLoss: 0.5211 Acc@1: 100.000% \n",
            "Pseudo labeled: 46.429% (104/224)    \tCorrect pseudo label: 83.65% (87/104)    \tSame output weak and strong augmentation: 87.50% (91/104)\n",
            "| Epoch [ 97/100] Iter[ 16/ 32]\t\tLoss: 0.2894 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.871% (1644/3584)    \tCorrect pseudo label: 84.43% (1388/1644)    \tSame output weak and strong augmentation: 87.35% (1436/1644)\n",
            "| Epoch [ 97/100] Iter[ 31/ 32]\t\tLoss: 0.3326 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.781% (3179/6944)    \tCorrect pseudo label: 84.49% (2686/3179)    \tSame output weak and strong augmentation: 87.83% (2792/3179)\n",
            "\n",
            "| Validation Epoch #97\t\t\tLoss: 0.0444 Acc@1: 67.10%\n",
            "| Saving Best model...\t\t\tTop1 = 67.10%\n",
            "\n",
            "=> Training Epoch #98, LR=0.0030\n",
            "| Epoch [ 98/100] Iter[  1/ 32]\t\tLoss: 0.3077 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.536% (102/224)    \tCorrect pseudo label: 85.29% (87/102)    \tSame output weak and strong augmentation: 90.20% (92/102)\n",
            "| Epoch [ 98/100] Iter[ 16/ 32]\t\tLoss: 0.2931 Acc@1: 100.000% \n",
            "Pseudo labeled: 44.978% (1612/3584)    \tCorrect pseudo label: 83.13% (1340/1612)    \tSame output weak and strong augmentation: 88.65% (1429/1612)\n",
            "| Epoch [ 98/100] Iter[ 31/ 32]\t\tLoss: 0.4186 Acc@1: 99.899% \n",
            "Pseudo labeled: 44.945% (3121/6944)    \tCorrect pseudo label: 83.08% (2593/3121)    \tSame output weak and strong augmentation: 88.47% (2761/3121)\n",
            "\n",
            "| Validation Epoch #98\t\t\tLoss: 0.0396 Acc@1: 65.85%\n",
            "\n",
            "=> Training Epoch #99, LR=0.0030\n",
            "| Epoch [ 99/100] Iter[  1/ 32]\t\tLoss: 0.3502 Acc@1: 100.000% \n",
            "Pseudo labeled: 46.875% (105/224)    \tCorrect pseudo label: 81.90% (86/105)    \tSame output weak and strong augmentation: 89.52% (94/105)\n",
            "| Epoch [ 99/100] Iter[ 16/ 32]\t\tLoss: 0.2629 Acc@1: 100.000% \n",
            "Pseudo labeled: 46.540% (1668/3584)    \tCorrect pseudo label: 84.29% (1406/1668)    \tSame output weak and strong augmentation: 89.27% (1489/1668)\n",
            "| Epoch [ 99/100] Iter[ 31/ 32]\t\tLoss: 0.2540 Acc@1: 100.000% \n",
            "Pseudo labeled: 45.060% (3129/6944)    \tCorrect pseudo label: 83.57% (2615/3129)    \tSame output weak and strong augmentation: 88.62% (2773/3129)\n",
            "\n",
            "| Validation Epoch #99\t\t\tLoss: 0.1021 Acc@1: 67.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_final(net, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2h7t1dAXHWm",
        "outputId": "3e83b202-61d9-4c28-d7d9-f4e1fc360e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST \t\t\tLoss: 2.3513 Acc@1: 65.94%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 1, 8, ..., 5, 1, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7 :\n",
        "Pour éviter le surapprentissage en Deep Learning on peut limiter la taille de nos architectures utilisés en fonction de la taille des données d'entraînement : pour les arbres de décisions on limite la profondeur des arbres, pour les réseaux de neurones on limite le nombre de neurones. Avec une architecture trop grande, on peut apprendre par coeur une base de données. On peut aussi utiliser un paramètre de régularisation pour contrôler le surapprentissage. Une dernière possibilité serait d'utiliser des données sans label pendant l'entraînement comme ça a été fait dans FixMatch, le réseau de neurones ne peut apprendre ces données par coeur étant donné qu'il ne sait pas quelle sortie il doit obtenir sur ces données."
      ],
      "metadata": {
        "id": "hvOnfdkMiELY"
      }
    }
  ]
}