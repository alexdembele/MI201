{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "040af7baf94e4fe9b9f8b09c1909e1c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_722eedcf70854df58bc9bb74f09e05da",
              "IPY_MODEL_22e34515505642e680807bd5ad31c9a1",
              "IPY_MODEL_c441667ff4d94d65b1d3bcfdf01f1cd3"
            ],
            "layout": "IPY_MODEL_06f84f43d8474622b4ad756ac054dbab"
          }
        },
        "722eedcf70854df58bc9bb74f09e05da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80af8b34a7104a96af1f57498ad7480c",
            "placeholder": "​",
            "style": "IPY_MODEL_0615ad4a36044af5ac35e8b68bcddefa",
            "value": "100%"
          }
        },
        "22e34515505642e680807bd5ad31c9a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85bd9f9ccec4c28abaf67230b0ee08f",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fde535be4d354ffa941fd223b3d8abad",
            "value": 170498071
          }
        },
        "c441667ff4d94d65b1d3bcfdf01f1cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f69409fe38014cc9988e6c65adac5d99",
            "placeholder": "​",
            "style": "IPY_MODEL_84e925e57489424994f342b38ee5a69a",
            "value": " 170498071/170498071 [00:03&lt;00:00, 51354606.52it/s]"
          }
        },
        "06f84f43d8474622b4ad756ac054dbab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80af8b34a7104a96af1f57498ad7480c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0615ad4a36044af5ac35e8b68bcddefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85bd9f9ccec4c28abaf67230b0ee08f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fde535be4d354ffa941fd223b3d8abad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f69409fe38014cc9988e6c65adac5d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e925e57489424994f342b38ee5a69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexdembele/MI201/blob/main/ProjetMI201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4zsDWUk5cavn"
      },
      "source": [
        "# Unsupervised Learning Tutorial of Gianni Franchi\n",
        "**PLEASE write your name and first name here:**\n",
        "\n",
        "Welcome to ML project!\n",
        "**In this notebook, you will**:\n",
        "- Learn what is SSL\n",
        "- Learn the difficulty with Overfitting\n",
        "- Learn to implement an Convolutional Neural Network.\n",
        "- Learn to train it when we don't have enough data\n",
        "\n",
        "If you have never used jupyter notebooks, nor Colab notebooks, [here](https://colab.research.google.com/notebooks/welcome.ipynb) is a short intro.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvision\n",
        "\n",
        "Resnet 18\n",
        "\n",
        "tester les modèles en faisant varier les paramètres.\n",
        "\n",
        "Pas utilisé y unabled\n",
        "\n",
        "Renvoyer notebook et rapport réponse question\n",
        "\n",
        "Q5: Fixmatch , inspiré se UDA. On a un set labelisé et un set pas labélisé, pour les labelisé on fait du supervise learning. Sur les non labelisé on fait du self learning: data augmentation (faible puis forte,) puis prédiction, on veur que les différentes dprédictions sient pareil."
      ],
      "metadata": {
        "id": "XGB8_oFGVsZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from numpy import asarray\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import random\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "#import matplotlib.pyplot as plt\n",
        "#from modules import *\n",
        "#import torchvision.models as models_pytorch\n",
        "#import h5py\n",
        "#import torch.optim as optim\n",
        "#import augmentations\n",
        "from torch.nn.functional import kl_div, softmax, log_softmax\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from os.path import exists, join, split\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from PIL import Image, ImageFilter , ImageDraw\n",
        "import PIL\n",
        "import random\n",
        "#import madgrad \n",
        "import matplotlib.pyplot as plt\n",
        "#! pip install madgrad\n",
        "#! pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "RTJ7Zz6TAaJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = 10 #number of classes\n",
        "\n",
        "seed=111 #seed for the algorithm\n",
        "batch_size = 32\n",
        "num_train =100 # number of training image by classe\n",
        "cutout=16  # parameter for the cutout\n",
        "num_epochs=50\n",
        "#Validation set size\n",
        "valid_size = 200\n",
        "lr=0.1"
      ],
      "metadata": {
        "id": "kFCoFJzWApYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First let us define a CNN"
      ],
      "metadata": {
        "id": "2Ux1VVt2AtcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bn_momentum = 0.9\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "\n",
        "def conv_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
        "        init.constant_(m.bias, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.constant_(m.weight, 1)\n",
        "        init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class WideBasic(nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
        "        super(WideBasic, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, momentum=bn_momentum)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, widen_factor, dropout_rate, num_classes):\n",
        "        super(WideResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        assert ((depth - 4) % 6 == 0), 'Wide-resnet depth should be 6n+4'\n",
        "        n = int((depth - 4) / 6)\n",
        "        k = widen_factor\n",
        "\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "\n",
        "        self.conv1 = conv3x3(3, nStages[0])\n",
        "        self.layer1 = self._wide_layer(WideBasic, nStages[1], n, dropout_rate, stride=1)\n",
        "        self.layer2 = self._wide_layer(WideBasic, nStages[2], n, dropout_rate, stride=2)\n",
        "        self.layer3 = self._wide_layer(WideBasic, nStages[3], n, dropout_rate, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=bn_momentum)\n",
        "        self.linear = nn.Linear(nStages[3], num_classes)\n",
        "\n",
        "        # self.apply(conv_init)\n",
        "\n",
        "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
        "            self.in_planes = planes\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        # out = F.avg_pool2d(out, 8)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "YDbhFAHJAx97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now let us define a dataset\n"
      ],
      "metadata": {
        "id": "9Y7RH4rWA9c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_sub_CIFAR(data.Dataset):\n",
        "\n",
        "    def __init__(self, data_feature, data_target,transform,phase='label'):\n",
        "        self.data_feature = data_feature\n",
        "        self.data_target = data_target\n",
        "        self.transform = transform\n",
        "        self.phase=phase\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_feature)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image as ndarray type (Height * Width * Channels)\n",
        "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
        "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
        "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
        "        if self.phase=='label':\n",
        "            data_feature = self.transform(Image.fromarray(np.uint8(self.data_feature[index])))\n",
        "            data_target =  self.data_target[index]\n",
        "            return data_feature, data_target\n",
        "\n",
        "        else:\n",
        "            data_feature = self.data_feature[index].float()\n",
        "            return data_feature\n",
        "\n",
        "\n",
        "class CutoutDefault(object):\n",
        "    \"\"\"\n",
        "    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py\n",
        "    \"\"\"\n",
        "    def __init__(self, length):\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if self.length <= 0:\n",
        "            return img\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "\n",
        "        y1 = np.clip(y - self.length // 2, 0, h)\n",
        "        y2 = np.clip(y + self.length // 2, 0, h)\n",
        "        x1 = np.clip(x - self.length // 2, 0, w)\n",
        "        x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "        mask[y1: y2, x1: x2] = 0.\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img *= mask\n",
        "        return img\n",
        "\n",
        "    \n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4, padding_mode = 'reflect'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "    CutoutDefault(cutout),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "#Dataset loading\n",
        "CIFAR10_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=None, download=True)\n",
        "CIFAR10_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=None, download=True)\n",
        "np.random.seed(seed=seed)\n",
        "permuation=np.random.permutation(len(CIFAR10_train_dataset.targets))\n",
        "\n",
        "Original_train_data_x = (CIFAR10_train_dataset.data)\n",
        "Original_train_data_y = np.array(CIFAR10_train_dataset.targets)\n",
        "Original_train_data_x = Original_train_data_x[permuation]\n",
        "Original_train_data_y = Original_train_data_y[permuation]\n",
        "\n",
        "Original_test_data_x = CIFAR10_test_dataset.data\n",
        "Original_test_data_y = np.array(CIFAR10_test_dataset.targets)\n",
        "\n",
        "\n",
        "\n",
        "#Selection of 250 labeled images for training and 2000 for validation\n",
        "incr_class = torch.zeros(num_class)\n",
        "train_idx_dico = {} #labeled images index dictionnary\n",
        "\n",
        "for i in range(num_class):\n",
        "    train_idx_dico[str(i)] = []\n",
        "\n",
        "valid_idx = np.zeros(num_class * valid_size, dtype=np.int32) #validation images indexes (2000)\n",
        "incr_t = 0\n",
        "incr_v = 0\n",
        "incrtotal = 0\n",
        "\n",
        "for idx in range(len(Original_train_data_y)):\n",
        "    class_y = Original_train_data_y[idx]\n",
        "    incrtotal += 1\n",
        "\n",
        "    train_idx_dico[str(class_y)].append(idx)\n",
        "    incr_class[class_y] += 1 #count the number of image per class\n",
        "    incr_t += 1\n",
        "\n",
        "\n",
        "train_idx = np.zeros(num_class * num_train, dtype=np.int32) #train labeled images indexes (1000)\n",
        "list_train_id = []\n",
        "list_unalabel_id = []\n",
        "valid_idx = []\n",
        "unlabel_idx_dico = {}\n",
        "for i in range(num_class):\n",
        "    unlabel_idx_dico[str(i)] = []\n",
        "for i in range(num_class):\n",
        "    list_train_id = list_train_id + train_idx_dico[str(i)][0:num_train]\n",
        "    valid_idx =valid_idx + train_idx_dico[str(i)][num_train:num_train+valid_size]\n",
        "    list_unalabel_id = list_unalabel_id + train_idx_dico[str(i)][num_train+valid_size::]\n",
        "    unlabel_idx_dico[str(i)] = train_idx_dico[str(i)][num_train::]\n",
        "\n",
        "#Get labeled and unlabeled data\n",
        "\n",
        "x_train = Original_train_data_x[[int(i) for i in list_train_id]]\n",
        "y_train = Original_train_data_y[[int(i) for i in list_train_id]]\n",
        "\n",
        "x_unlabeled = Original_train_data_x[[int(i) for i in list_unalabel_id]]\n",
        "y_unlabeled = Original_train_data_y[[int(i) for i in list_unalabel_id]]\n",
        "\n",
        "#Get validation set data\n",
        "x_valid = Original_train_data_x[[int(i) for i in valid_idx]]\n",
        "y_valid = Original_train_data_y[[int(i) for i in valid_idx]]\n",
        "\n",
        "# Printing the size of the training, validation and test sets\n",
        "print('Number of training examples: ' + str(x_train.shape[0]))\n",
        "print('Number of unlabeled examples: ' + str(x_unlabeled.shape[0]))\n",
        "print('Number of validation examples: ' + str(x_valid.shape[0]))\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "#Dataloader creation\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(Original_test_data_x, Original_test_data_y, transform=transform_test),\n",
        "    batch_size = batch_size,\n",
        "    shuffle=False, num_workers=2)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_train, y_train, transform=transform_train),\n",
        "    batch_size=batch_size,shuffle=True, num_workers=2) #num_workers = 2 ou 1\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    Dataset_sub_CIFAR(x_valid, y_valid, transform=transform_test),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "040af7baf94e4fe9b9f8b09c1909e1c9",
            "722eedcf70854df58bc9bb74f09e05da",
            "22e34515505642e680807bd5ad31c9a1",
            "c441667ff4d94d65b1d3bcfdf01f1cd3",
            "06f84f43d8474622b4ad756ac054dbab",
            "80af8b34a7104a96af1f57498ad7480c",
            "0615ad4a36044af5ac35e8b68bcddefa",
            "b85bd9f9ccec4c28abaf67230b0ee08f",
            "fde535be4d354ffa941fd223b3d8abad",
            "f69409fe38014cc9988e6c65adac5d99",
            "84e925e57489424994f342b38ee5a69a"
          ]
        },
        "id": "JE1_MblIA5V7",
        "outputId": "94b1db5d-e667-4461-c011-01628fe8446f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "040af7baf94e4fe9b9f8b09c1909e1c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Number of training examples: 1000\n",
            "Number of unlabeled examples: 47000\n",
            "Number of validation examples: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we build the CNN and the optimizer"
      ],
      "metadata": {
        "id": "jxIA7jzZBIjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#Networks creation\n",
        "net = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class)\n",
        "net =net.cuda()\n",
        "net_save = WideResNet(28, 2, dropout_rate=0.0, num_classes=num_class) # model where to save the results\n",
        "net_save =net_save.cuda()\n",
        "\n",
        "def learning_rate_scheduler(init, epoch):\n",
        "    optim_factor = 0\n",
        "    if(epoch > 200):\n",
        "        optim_factor = 3\n",
        "    elif(epoch > 160):\n",
        "        optim_factor = 2\n",
        "    elif(epoch > 80):\n",
        "        optim_factor = 1\n",
        "\n",
        "    return init*math.pow(0.1, optim_factor)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch,net,trainloader,log_interval=15):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate_scheduler(lr, epoch)))\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_scheduler(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        " \n",
        "        inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)               # Forward Propagation\n",
        "        loss = criterion(outputs, targets)  # Loss\n",
        "        loss.backward()  # Backward Propagation\n",
        "        optimizer.step() # Optimizer update\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%'\n",
        "                %(epoch, num_epochs, batch_idx+1,\n",
        "                    (len(trainloader.dataset)//batch_size)+1, loss.item(), 100.*correct/total))\n",
        "\n",
        "\n",
        "def test(epoch,net,testloader):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "        \n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %(epoch, loss.item(), acc))\n",
        "    return acc"
      ],
      "metadata": {
        "id": "5HSobiPQBNjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "tY_jn6VVBTgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc=0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "    train(epoch,net,train_loader)\n",
        "    acc =test(epoch,net,valid_loader)\n",
        "    # Save checkpoint when best model\n",
        "    if acc > best_acc:\n",
        "        print('| Saving Best model...\\t\\t\\tTop1 = %.2f%%' %(acc))\n",
        "        net_save.load_state_dict(net.state_dict(), strict=True)\n",
        "        best_acc=acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpnbckfWBWbB",
        "outputId": "cbde3563-7ff3-4353-e3f7-1e23fb96e105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.1000\n",
            "| Epoch [  0/ 50] Iter[  1/ 32]\t\tLoss: 2.2772 Acc@1: 18.750%\n",
            "| Epoch [  0/ 50] Iter[ 16/ 32]\t\tLoss: 2.2335 Acc@1: 15.234%\n",
            "| Epoch [  0/ 50] Iter[ 31/ 32]\t\tLoss: 2.0996 Acc@1: 16.532%\n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 4.2378 Acc@1: 20.15%\n",
            "| Saving Best model...\t\t\tTop1 = 20.15%\n",
            "\n",
            "=> Training Epoch #1, LR=0.1000\n",
            "| Epoch [  1/ 50] Iter[  1/ 32]\t\tLoss: 2.2018 Acc@1: 12.500%\n",
            "| Epoch [  1/ 50] Iter[ 16/ 32]\t\tLoss: 1.9117 Acc@1: 22.461%\n",
            "| Epoch [  1/ 50] Iter[ 31/ 32]\t\tLoss: 2.1678 Acc@1: 21.673%\n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 2.6422 Acc@1: 17.15%\n",
            "\n",
            "=> Training Epoch #2, LR=0.1000\n",
            "| Epoch [  2/ 50] Iter[  1/ 32]\t\tLoss: 2.2136 Acc@1: 21.875%\n",
            "| Epoch [  2/ 50] Iter[ 16/ 32]\t\tLoss: 1.8043 Acc@1: 21.484%\n",
            "| Epoch [  2/ 50] Iter[ 31/ 32]\t\tLoss: 2.2775 Acc@1: 21.976%\n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 1.1964 Acc@1: 27.00%\n",
            "| Saving Best model...\t\t\tTop1 = 27.00%\n",
            "\n",
            "=> Training Epoch #3, LR=0.1000\n",
            "| Epoch [  3/ 50] Iter[  1/ 32]\t\tLoss: 2.0322 Acc@1: 28.125%\n",
            "| Epoch [  3/ 50] Iter[ 16/ 32]\t\tLoss: 2.1364 Acc@1: 24.805%\n",
            "| Epoch [  3/ 50] Iter[ 31/ 32]\t\tLoss: 2.1108 Acc@1: 25.706%\n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 2.1897 Acc@1: 25.65%\n",
            "\n",
            "=> Training Epoch #4, LR=0.1000\n",
            "| Epoch [  4/ 50] Iter[  1/ 32]\t\tLoss: 2.2050 Acc@1: 15.625%\n",
            "| Epoch [  4/ 50] Iter[ 16/ 32]\t\tLoss: 2.0228 Acc@1: 24.023%\n",
            "| Epoch [  4/ 50] Iter[ 31/ 32]\t\tLoss: 1.9688 Acc@1: 23.387%\n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 0.9453 Acc@1: 27.40%\n",
            "| Saving Best model...\t\t\tTop1 = 27.40%\n",
            "\n",
            "=> Training Epoch #5, LR=0.1000\n",
            "| Epoch [  5/ 50] Iter[  1/ 32]\t\tLoss: 1.7217 Acc@1: 43.750%\n",
            "| Epoch [  5/ 50] Iter[ 16/ 32]\t\tLoss: 1.8827 Acc@1: 29.492%\n",
            "| Epoch [  5/ 50] Iter[ 31/ 32]\t\tLoss: 1.9523 Acc@1: 28.226%\n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 2.5989 Acc@1: 26.65%\n",
            "\n",
            "=> Training Epoch #6, LR=0.1000\n",
            "| Epoch [  6/ 50] Iter[  1/ 32]\t\tLoss: 1.7573 Acc@1: 28.125%\n",
            "| Epoch [  6/ 50] Iter[ 16/ 32]\t\tLoss: 1.8888 Acc@1: 26.758%\n",
            "| Epoch [  6/ 50] Iter[ 31/ 32]\t\tLoss: 2.1220 Acc@1: 26.512%\n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 1.2859 Acc@1: 31.70%\n",
            "| Saving Best model...\t\t\tTop1 = 31.70%\n",
            "\n",
            "=> Training Epoch #7, LR=0.1000\n",
            "| Epoch [  7/ 50] Iter[  1/ 32]\t\tLoss: 2.0033 Acc@1: 28.125%\n",
            "| Epoch [  7/ 50] Iter[ 16/ 32]\t\tLoss: 1.8882 Acc@1: 32.031%\n",
            "| Epoch [  7/ 50] Iter[ 31/ 32]\t\tLoss: 1.9094 Acc@1: 28.831%\n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 1.4360 Acc@1: 31.50%\n",
            "\n",
            "=> Training Epoch #8, LR=0.1000\n",
            "| Epoch [  8/ 50] Iter[  1/ 32]\t\tLoss: 2.0701 Acc@1: 31.250%\n",
            "| Epoch [  8/ 50] Iter[ 16/ 32]\t\tLoss: 1.7839 Acc@1: 30.078%\n",
            "| Epoch [  8/ 50] Iter[ 31/ 32]\t\tLoss: 1.7737 Acc@1: 29.536%\n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 0.9947 Acc@1: 28.70%\n",
            "\n",
            "=> Training Epoch #9, LR=0.1000\n",
            "| Epoch [  9/ 50] Iter[  1/ 32]\t\tLoss: 1.8684 Acc@1: 37.500%\n",
            "| Epoch [  9/ 50] Iter[ 16/ 32]\t\tLoss: 1.8734 Acc@1: 32.031%\n",
            "| Epoch [  9/ 50] Iter[ 31/ 32]\t\tLoss: 1.9143 Acc@1: 31.149%\n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 1.9318 Acc@1: 31.65%\n",
            "\n",
            "=> Training Epoch #10, LR=0.1000\n",
            "| Epoch [ 10/ 50] Iter[  1/ 32]\t\tLoss: 1.7313 Acc@1: 31.250%\n",
            "| Epoch [ 10/ 50] Iter[ 16/ 32]\t\tLoss: 1.5414 Acc@1: 33.398%\n",
            "| Epoch [ 10/ 50] Iter[ 31/ 32]\t\tLoss: 1.8507 Acc@1: 31.351%\n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 1.5410 Acc@1: 31.85%\n",
            "| Saving Best model...\t\t\tTop1 = 31.85%\n",
            "\n",
            "=> Training Epoch #11, LR=0.1000\n",
            "| Epoch [ 11/ 50] Iter[  1/ 32]\t\tLoss: 1.6851 Acc@1: 34.375%\n",
            "| Epoch [ 11/ 50] Iter[ 16/ 32]\t\tLoss: 1.9630 Acc@1: 31.641%\n",
            "| Epoch [ 11/ 50] Iter[ 31/ 32]\t\tLoss: 2.0647 Acc@1: 34.073%\n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 1.7839 Acc@1: 31.50%\n",
            "\n",
            "=> Training Epoch #12, LR=0.1000\n",
            "| Epoch [ 12/ 50] Iter[  1/ 32]\t\tLoss: 1.6983 Acc@1: 37.500%\n",
            "| Epoch [ 12/ 50] Iter[ 16/ 32]\t\tLoss: 1.9488 Acc@1: 33.984%\n",
            "| Epoch [ 12/ 50] Iter[ 31/ 32]\t\tLoss: 1.9841 Acc@1: 31.754%\n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 1.3141 Acc@1: 34.30%\n",
            "| Saving Best model...\t\t\tTop1 = 34.30%\n",
            "\n",
            "=> Training Epoch #13, LR=0.1000\n",
            "| Epoch [ 13/ 50] Iter[  1/ 32]\t\tLoss: 1.7765 Acc@1: 28.125%\n",
            "| Epoch [ 13/ 50] Iter[ 16/ 32]\t\tLoss: 1.4098 Acc@1: 35.156%\n",
            "| Epoch [ 13/ 50] Iter[ 31/ 32]\t\tLoss: 2.0417 Acc@1: 34.375%\n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 3.0333 Acc@1: 28.60%\n",
            "\n",
            "=> Training Epoch #14, LR=0.1000\n",
            "| Epoch [ 14/ 50] Iter[  1/ 32]\t\tLoss: 1.6106 Acc@1: 43.750%\n",
            "| Epoch [ 14/ 50] Iter[ 16/ 32]\t\tLoss: 1.8527 Acc@1: 37.695%\n",
            "| Epoch [ 14/ 50] Iter[ 31/ 32]\t\tLoss: 1.9319 Acc@1: 34.980%\n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 0.9210 Acc@1: 30.85%\n",
            "\n",
            "=> Training Epoch #15, LR=0.1000\n",
            "| Epoch [ 15/ 50] Iter[  1/ 32]\t\tLoss: 1.8689 Acc@1: 21.875%\n",
            "| Epoch [ 15/ 50] Iter[ 16/ 32]\t\tLoss: 1.6049 Acc@1: 31.641%\n",
            "| Epoch [ 15/ 50] Iter[ 31/ 32]\t\tLoss: 1.7197 Acc@1: 32.964%\n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 1.9271 Acc@1: 34.15%\n",
            "\n",
            "=> Training Epoch #16, LR=0.1000\n",
            "| Epoch [ 16/ 50] Iter[  1/ 32]\t\tLoss: 1.7577 Acc@1: 34.375%\n",
            "| Epoch [ 16/ 50] Iter[ 16/ 32]\t\tLoss: 1.5090 Acc@1: 35.938%\n",
            "| Epoch [ 16/ 50] Iter[ 31/ 32]\t\tLoss: 1.7196 Acc@1: 34.274%\n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 1.3931 Acc@1: 34.00%\n",
            "\n",
            "=> Training Epoch #17, LR=0.1000\n",
            "| Epoch [ 17/ 50] Iter[  1/ 32]\t\tLoss: 1.9522 Acc@1: 28.125%\n",
            "| Epoch [ 17/ 50] Iter[ 16/ 32]\t\tLoss: 1.6624 Acc@1: 40.430%\n",
            "| Epoch [ 17/ 50] Iter[ 31/ 32]\t\tLoss: 1.8843 Acc@1: 36.794%\n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 1.5482 Acc@1: 34.95%\n",
            "| Saving Best model...\t\t\tTop1 = 34.95%\n",
            "\n",
            "=> Training Epoch #18, LR=0.1000\n",
            "| Epoch [ 18/ 50] Iter[  1/ 32]\t\tLoss: 1.8154 Acc@1: 31.250%\n",
            "| Epoch [ 18/ 50] Iter[ 16/ 32]\t\tLoss: 1.4788 Acc@1: 36.719%\n",
            "| Epoch [ 18/ 50] Iter[ 31/ 32]\t\tLoss: 1.5570 Acc@1: 36.694%\n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 1.0661 Acc@1: 30.10%\n",
            "\n",
            "=> Training Epoch #19, LR=0.1000\n",
            "| Epoch [ 19/ 50] Iter[  1/ 32]\t\tLoss: 1.8990 Acc@1: 28.125%\n",
            "| Epoch [ 19/ 50] Iter[ 16/ 32]\t\tLoss: 1.6232 Acc@1: 38.477%\n",
            "| Epoch [ 19/ 50] Iter[ 31/ 32]\t\tLoss: 1.7329 Acc@1: 39.012%\n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 1.2051 Acc@1: 35.85%\n",
            "| Saving Best model...\t\t\tTop1 = 35.85%\n",
            "\n",
            "=> Training Epoch #20, LR=0.1000\n",
            "| Epoch [ 20/ 50] Iter[  1/ 32]\t\tLoss: 1.6052 Acc@1: 28.125%\n",
            "| Epoch [ 20/ 50] Iter[ 16/ 32]\t\tLoss: 1.6287 Acc@1: 35.156%\n",
            "| Epoch [ 20/ 50] Iter[ 31/ 32]\t\tLoss: 1.9589 Acc@1: 32.863%\n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 2.2741 Acc@1: 33.30%\n",
            "\n",
            "=> Training Epoch #21, LR=0.1000\n",
            "| Epoch [ 21/ 50] Iter[  1/ 32]\t\tLoss: 1.4867 Acc@1: 46.875%\n",
            "| Epoch [ 21/ 50] Iter[ 16/ 32]\t\tLoss: 1.6771 Acc@1: 46.875%\n",
            "| Epoch [ 21/ 50] Iter[ 31/ 32]\t\tLoss: 1.6733 Acc@1: 42.339%\n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 0.7645 Acc@1: 38.30%\n",
            "| Saving Best model...\t\t\tTop1 = 38.30%\n",
            "\n",
            "=> Training Epoch #22, LR=0.1000\n",
            "| Epoch [ 22/ 50] Iter[  1/ 32]\t\tLoss: 1.4172 Acc@1: 56.250%\n",
            "| Epoch [ 22/ 50] Iter[ 16/ 32]\t\tLoss: 1.6517 Acc@1: 40.039%\n",
            "| Epoch [ 22/ 50] Iter[ 31/ 32]\t\tLoss: 1.3711 Acc@1: 40.625%\n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 1.9238 Acc@1: 37.35%\n",
            "\n",
            "=> Training Epoch #23, LR=0.1000\n",
            "| Epoch [ 23/ 50] Iter[  1/ 32]\t\tLoss: 1.5441 Acc@1: 34.375%\n",
            "| Epoch [ 23/ 50] Iter[ 16/ 32]\t\tLoss: 1.9558 Acc@1: 42.383%\n",
            "| Epoch [ 23/ 50] Iter[ 31/ 32]\t\tLoss: 1.7368 Acc@1: 41.331%\n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 1.7210 Acc@1: 38.35%\n",
            "| Saving Best model...\t\t\tTop1 = 38.35%\n",
            "\n",
            "=> Training Epoch #24, LR=0.1000\n",
            "| Epoch [ 24/ 50] Iter[  1/ 32]\t\tLoss: 1.6570 Acc@1: 28.125%\n",
            "| Epoch [ 24/ 50] Iter[ 16/ 32]\t\tLoss: 1.8464 Acc@1: 37.109%\n",
            "| Epoch [ 24/ 50] Iter[ 31/ 32]\t\tLoss: 1.8022 Acc@1: 41.230%\n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 0.9604 Acc@1: 37.20%\n",
            "\n",
            "=> Training Epoch #25, LR=0.1000\n",
            "| Epoch [ 25/ 50] Iter[  1/ 32]\t\tLoss: 1.6023 Acc@1: 31.250%\n",
            "| Epoch [ 25/ 50] Iter[ 16/ 32]\t\tLoss: 1.6832 Acc@1: 44.531%\n",
            "| Epoch [ 25/ 50] Iter[ 31/ 32]\t\tLoss: 1.6788 Acc@1: 41.633%\n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 2.4719 Acc@1: 37.05%\n",
            "\n",
            "=> Training Epoch #26, LR=0.1000\n",
            "| Epoch [ 26/ 50] Iter[  1/ 32]\t\tLoss: 1.2776 Acc@1: 50.000%\n",
            "| Epoch [ 26/ 50] Iter[ 16/ 32]\t\tLoss: 1.5424 Acc@1: 38.086%\n",
            "| Epoch [ 26/ 50] Iter[ 31/ 32]\t\tLoss: 1.5833 Acc@1: 40.827%\n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 1.3060 Acc@1: 38.20%\n",
            "\n",
            "=> Training Epoch #27, LR=0.1000\n",
            "| Epoch [ 27/ 50] Iter[  1/ 32]\t\tLoss: 1.4462 Acc@1: 53.125%\n",
            "| Epoch [ 27/ 50] Iter[ 16/ 32]\t\tLoss: 1.2729 Acc@1: 44.336%\n",
            "| Epoch [ 27/ 50] Iter[ 31/ 32]\t\tLoss: 1.6518 Acc@1: 43.952%\n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 1.1659 Acc@1: 42.55%\n",
            "| Saving Best model...\t\t\tTop1 = 42.55%\n",
            "\n",
            "=> Training Epoch #28, LR=0.1000\n",
            "| Epoch [ 28/ 50] Iter[  1/ 32]\t\tLoss: 1.1900 Acc@1: 56.250%\n",
            "| Epoch [ 28/ 50] Iter[ 16/ 32]\t\tLoss: 1.4703 Acc@1: 42.773%\n",
            "| Epoch [ 28/ 50] Iter[ 31/ 32]\t\tLoss: 1.3185 Acc@1: 43.548%\n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 0.9901 Acc@1: 39.85%\n",
            "\n",
            "=> Training Epoch #29, LR=0.1000\n",
            "| Epoch [ 29/ 50] Iter[  1/ 32]\t\tLoss: 1.7250 Acc@1: 34.375%\n",
            "| Epoch [ 29/ 50] Iter[ 16/ 32]\t\tLoss: 1.4359 Acc@1: 44.336%\n",
            "| Epoch [ 29/ 50] Iter[ 31/ 32]\t\tLoss: 1.4992 Acc@1: 42.036%\n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 2.2831 Acc@1: 38.15%\n",
            "\n",
            "=> Training Epoch #30, LR=0.1000\n",
            "| Epoch [ 30/ 50] Iter[  1/ 32]\t\tLoss: 1.7198 Acc@1: 40.625%\n",
            "| Epoch [ 30/ 50] Iter[ 16/ 32]\t\tLoss: 1.4872 Acc@1: 43.555%\n",
            "| Epoch [ 30/ 50] Iter[ 31/ 32]\t\tLoss: 1.6860 Acc@1: 41.835%\n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 1.1178 Acc@1: 40.50%\n",
            "\n",
            "=> Training Epoch #31, LR=0.1000\n",
            "| Epoch [ 31/ 50] Iter[  1/ 32]\t\tLoss: 1.5862 Acc@1: 34.375%\n",
            "| Epoch [ 31/ 50] Iter[ 16/ 32]\t\tLoss: 1.8023 Acc@1: 42.188%\n",
            "| Epoch [ 31/ 50] Iter[ 31/ 32]\t\tLoss: 1.6353 Acc@1: 42.238%\n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 0.3351 Acc@1: 36.40%\n",
            "\n",
            "=> Training Epoch #32, LR=0.1000\n",
            "| Epoch [ 32/ 50] Iter[  1/ 32]\t\tLoss: 1.6060 Acc@1: 34.375%\n",
            "| Epoch [ 32/ 50] Iter[ 16/ 32]\t\tLoss: 1.3650 Acc@1: 46.680%\n",
            "| Epoch [ 32/ 50] Iter[ 31/ 32]\t\tLoss: 1.4907 Acc@1: 45.766%\n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 1.2948 Acc@1: 42.30%\n",
            "\n",
            "=> Training Epoch #33, LR=0.1000\n",
            "| Epoch [ 33/ 50] Iter[  1/ 32]\t\tLoss: 1.8135 Acc@1: 34.375%\n",
            "| Epoch [ 33/ 50] Iter[ 16/ 32]\t\tLoss: 1.4847 Acc@1: 46.484%\n",
            "| Epoch [ 33/ 50] Iter[ 31/ 32]\t\tLoss: 1.6913 Acc@1: 45.665%\n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 1.5944 Acc@1: 41.60%\n",
            "\n",
            "=> Training Epoch #34, LR=0.1000\n",
            "| Epoch [ 34/ 50] Iter[  1/ 32]\t\tLoss: 1.7903 Acc@1: 37.500%\n",
            "| Epoch [ 34/ 50] Iter[ 16/ 32]\t\tLoss: 1.3050 Acc@1: 47.461%\n",
            "| Epoch [ 34/ 50] Iter[ 31/ 32]\t\tLoss: 1.3737 Acc@1: 46.875%\n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 0.7164 Acc@1: 47.40%\n",
            "| Saving Best model...\t\t\tTop1 = 47.40%\n",
            "\n",
            "=> Training Epoch #35, LR=0.1000\n",
            "| Epoch [ 35/ 50] Iter[  1/ 32]\t\tLoss: 1.2448 Acc@1: 50.000%\n",
            "| Epoch [ 35/ 50] Iter[ 16/ 32]\t\tLoss: 1.4219 Acc@1: 49.219%\n",
            "| Epoch [ 35/ 50] Iter[ 31/ 32]\t\tLoss: 1.7174 Acc@1: 47.681%\n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 0.9916 Acc@1: 46.45%\n",
            "\n",
            "=> Training Epoch #36, LR=0.1000\n",
            "| Epoch [ 36/ 50] Iter[  1/ 32]\t\tLoss: 1.8510 Acc@1: 31.250%\n",
            "| Epoch [ 36/ 50] Iter[ 16/ 32]\t\tLoss: 1.2532 Acc@1: 46.875%\n",
            "| Epoch [ 36/ 50] Iter[ 31/ 32]\t\tLoss: 1.4845 Acc@1: 45.363%\n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.2097 Acc@1: 37.85%\n",
            "\n",
            "=> Training Epoch #37, LR=0.1000\n",
            "| Epoch [ 37/ 50] Iter[  1/ 32]\t\tLoss: 1.4474 Acc@1: 46.875%\n",
            "| Epoch [ 37/ 50] Iter[ 16/ 32]\t\tLoss: 1.2901 Acc@1: 48.633%\n",
            "| Epoch [ 37/ 50] Iter[ 31/ 32]\t\tLoss: 1.5649 Acc@1: 46.472%\n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 0.7540 Acc@1: 40.25%\n",
            "\n",
            "=> Training Epoch #38, LR=0.1000\n",
            "| Epoch [ 38/ 50] Iter[  1/ 32]\t\tLoss: 1.0832 Acc@1: 59.375%\n",
            "| Epoch [ 38/ 50] Iter[ 16/ 32]\t\tLoss: 1.6198 Acc@1: 46.680%\n",
            "| Epoch [ 38/ 50] Iter[ 31/ 32]\t\tLoss: 1.3856 Acc@1: 46.169%\n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 0.6570 Acc@1: 43.65%\n",
            "\n",
            "=> Training Epoch #39, LR=0.1000\n",
            "| Epoch [ 39/ 50] Iter[  1/ 32]\t\tLoss: 1.5714 Acc@1: 37.500%\n",
            "| Epoch [ 39/ 50] Iter[ 16/ 32]\t\tLoss: 1.2761 Acc@1: 51.367%\n",
            "| Epoch [ 39/ 50] Iter[ 31/ 32]\t\tLoss: 1.3816 Acc@1: 49.294%\n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 4.5835 Acc@1: 37.60%\n",
            "\n",
            "=> Training Epoch #40, LR=0.1000\n",
            "| Epoch [ 40/ 50] Iter[  1/ 32]\t\tLoss: 1.3278 Acc@1: 53.125%\n",
            "| Epoch [ 40/ 50] Iter[ 16/ 32]\t\tLoss: 1.4848 Acc@1: 43.945%\n",
            "| Epoch [ 40/ 50] Iter[ 31/ 32]\t\tLoss: 1.5114 Acc@1: 43.548%\n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 1.1519 Acc@1: 43.75%\n",
            "\n",
            "=> Training Epoch #41, LR=0.1000\n",
            "| Epoch [ 41/ 50] Iter[  1/ 32]\t\tLoss: 1.5018 Acc@1: 43.750%\n",
            "| Epoch [ 41/ 50] Iter[ 16/ 32]\t\tLoss: 1.0617 Acc@1: 50.977%\n",
            "| Epoch [ 41/ 50] Iter[ 31/ 32]\t\tLoss: 1.4913 Acc@1: 47.883%\n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 0.8847 Acc@1: 41.35%\n",
            "\n",
            "=> Training Epoch #42, LR=0.1000\n",
            "| Epoch [ 42/ 50] Iter[  1/ 32]\t\tLoss: 1.5154 Acc@1: 43.750%\n",
            "| Epoch [ 42/ 50] Iter[ 16/ 32]\t\tLoss: 1.3167 Acc@1: 45.117%\n",
            "| Epoch [ 42/ 50] Iter[ 31/ 32]\t\tLoss: 1.3853 Acc@1: 46.472%\n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 1.5624 Acc@1: 42.85%\n",
            "\n",
            "=> Training Epoch #43, LR=0.1000\n",
            "| Epoch [ 43/ 50] Iter[  1/ 32]\t\tLoss: 1.4233 Acc@1: 46.875%\n",
            "| Epoch [ 43/ 50] Iter[ 16/ 32]\t\tLoss: 1.2698 Acc@1: 50.977%\n",
            "| Epoch [ 43/ 50] Iter[ 31/ 32]\t\tLoss: 1.4126 Acc@1: 52.319%\n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 1.1446 Acc@1: 47.15%\n",
            "\n",
            "=> Training Epoch #44, LR=0.1000\n",
            "| Epoch [ 44/ 50] Iter[  1/ 32]\t\tLoss: 1.5720 Acc@1: 46.875%\n",
            "| Epoch [ 44/ 50] Iter[ 16/ 32]\t\tLoss: 1.2386 Acc@1: 49.414%\n",
            "| Epoch [ 44/ 50] Iter[ 31/ 32]\t\tLoss: 1.5839 Acc@1: 49.395%\n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 1.3755 Acc@1: 41.60%\n",
            "\n",
            "=> Training Epoch #45, LR=0.1000\n",
            "| Epoch [ 45/ 50] Iter[  1/ 32]\t\tLoss: 1.5095 Acc@1: 56.250%\n",
            "| Epoch [ 45/ 50] Iter[ 16/ 32]\t\tLoss: 1.4707 Acc@1: 50.781%\n",
            "| Epoch [ 45/ 50] Iter[ 31/ 32]\t\tLoss: 1.0932 Acc@1: 49.899%\n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 1.3439 Acc@1: 46.70%\n",
            "\n",
            "=> Training Epoch #46, LR=0.1000\n",
            "| Epoch [ 46/ 50] Iter[  1/ 32]\t\tLoss: 1.3496 Acc@1: 50.000%\n",
            "| Epoch [ 46/ 50] Iter[ 16/ 32]\t\tLoss: 1.0147 Acc@1: 54.102%\n",
            "| Epoch [ 46/ 50] Iter[ 31/ 32]\t\tLoss: 1.4498 Acc@1: 53.427%\n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 1.4377 Acc@1: 43.75%\n",
            "\n",
            "=> Training Epoch #47, LR=0.1000\n",
            "| Epoch [ 47/ 50] Iter[  1/ 32]\t\tLoss: 1.0542 Acc@1: 62.500%\n",
            "| Epoch [ 47/ 50] Iter[ 16/ 32]\t\tLoss: 1.6665 Acc@1: 50.391%\n",
            "| Epoch [ 47/ 50] Iter[ 31/ 32]\t\tLoss: 1.5034 Acc@1: 49.798%\n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 0.5514 Acc@1: 45.80%\n",
            "\n",
            "=> Training Epoch #48, LR=0.1000\n",
            "| Epoch [ 48/ 50] Iter[  1/ 32]\t\tLoss: 1.1744 Acc@1: 50.000%\n",
            "| Epoch [ 48/ 50] Iter[ 16/ 32]\t\tLoss: 1.5787 Acc@1: 51.367%\n",
            "| Epoch [ 48/ 50] Iter[ 31/ 32]\t\tLoss: 1.4483 Acc@1: 50.101%\n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 1.1309 Acc@1: 46.65%\n",
            "\n",
            "=> Training Epoch #49, LR=0.1000\n",
            "| Epoch [ 49/ 50] Iter[  1/ 32]\t\tLoss: 1.3159 Acc@1: 50.000%\n",
            "| Epoch [ 49/ 50] Iter[ 16/ 32]\t\tLoss: 1.1668 Acc@1: 58.594%\n",
            "| Epoch [ 49/ 50] Iter[ 31/ 32]\t\tLoss: 1.8332 Acc@1: 54.738%\n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 1.2793 Acc@1: 42.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "weX9mgbZBiNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def test_final(net,testloader):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "            if batch_idx == 0:\n",
        "                predicted_concat = predicted.clone()\n",
        "            else:\n",
        "                predicted_concat = torch.cat((predicted_concat, predicted), 0)\n",
        "\n",
        "        # Save checkpoint when best model\n",
        "    acc = 100.*correct/total\n",
        "    print(\"\\n| TEST \\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %( loss.item(), acc))\n",
        "    return predicted_concat.cpu().numpy()\n",
        "    \n",
        "\n",
        "predicted_concat = test_final(net,test_loader)\n",
        "\n",
        "\n",
        "id_concat =range(len(predicted_concat))\n",
        "my_submission = pd.DataFrame({'Id': id_concat,'Expected': predicted_concat})\n",
        "\n",
        "# you could use any filename. We choose submission here\n",
        "my_submission.to_csv('submission2.csv', index=False)\n",
        "print('we have saved the submission !! ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDqFxbtmBkAL",
        "outputId": "abdac3f8-4529-43c6-c39d-062bb5eca22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "| TEST \t\t\tLoss: 2.0615 Acc@1: 41.86%\n",
            "we have saved the submission !! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question for the report\n",
        "I want that you send me a small report with the answer to this question and your notebook.\n",
        "- Q0: Please train wideresnet, and please understand a bit wideresnet.\n",
        "- Q1: Please change DNN with a Resnet 18. Try with one that is pre-trained and one that is not pre-trained. \n",
        "- Q2: Please change DNN with an AlexNet. Try with one that is pre-trained and one that is not pre-trained. (Be careful, you need a bit to play with the learning rate, for questions two, one and zeros I want to see the training loss and training accuracy. What other curb is interesting? Plot it and analyse it.)\n",
        "- Q3: Please try to train an SVM and a random forest.\n",
        "- Q4 After you have trained several models please draw a table and make some conclusions.\n",
        "-  Q5 Read the paper fixmatch (https://amitness.com/2020/03/fixmatch-semi-supervised/) and explain it.\n",
        "- Q6 please try to implement it and try to make it work.\n",
        "- Q7 What can we do to avoid overfitting in Deep learning?\n",
        "\n",
        "\n",
        "*Q0-Q5 = 14 pts*\n",
        "\n",
        "*Q6 = 6 pts*\n",
        "\n",
        "*Q7 = 1 pts*"
      ],
      "metadata": {
        "id": "UrJS4AqABprt"
      }
    }
  ]
}